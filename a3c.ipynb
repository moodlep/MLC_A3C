{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moodlep/MLC_A3C/blob/main/a3c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oq0AxEo1W9o"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deq0in1i1iM7"
      },
      "source": [
        "import os\n",
        "import Box2D\n",
        "import pyglet\n",
        "import imageio\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "954_wRxQtHoe"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3r5xRwwZGFa"
      },
      "source": [
        "## Gym Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjrzTEmz1Q3L"
      },
      "source": [
        "# The env - quick test: \n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "s = env.reset()\n",
        "\n",
        "for _ in range(5): \n",
        "  a = env.action_space.sample()\n",
        "  next_state, reward, done, info = env.step(a)\n",
        "  print(next_state, reward, a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVEU5PUOG7aU"
      },
      "source": [
        "class SharedAdam(torch.optim.Adam):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
        "                 weight_decay=0):\n",
        "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        # State initialization\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                state['step'] = 0\n",
        "                state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                # share in memory\n",
        "                state['exp_avg'].share_memory_()\n",
        "                state['exp_avg_sq'].share_memory_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7EURhaU18Rl"
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkudVJbo_q6f"
      },
      "source": [
        "## Actor - policy NN and value NN \n",
        "### data collection -> batch\n",
        "### train: calculate loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTIuEegY2JYl"
      },
      "source": [
        "\n",
        "class Policy(nn.Module):\n",
        "\tdef __init__(self, state_dim,action_dim,hidden=100):\n",
        "\t\tsuper(Policy, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, hidden)\n",
        "\t\tself.l2 = nn.Linear(hidden, hidden)\n",
        "\t\tself.l3 = nn.Linear(hidden,action_dim)\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\tprint(state)\n",
        "\t\tq = F.leaky_relu(self.l1(state))\n",
        "\t\tq = F.leaky_relu(self.l2(q))\n",
        "\t\treturn F.softmax(self.l3(q), dim = 1)\n",
        "\t\n",
        "\tdef get_action(self,state):\n",
        "\t\tstate = torch.tensor(state, dtype = torch.double)\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tpol = self.forward(state)\n",
        "\t\t\tdist = torch.distributions.Categorical(pol)\n",
        "\t\treturn dist.sample() #returns a batch of values\n",
        "\t\n",
        "\tdef log_prob(self, state, actions):\n",
        "\t\t  # Part of the loss term\n",
        "\t\t\tpol = self.forward(state)\n",
        "\t\t\tlog_prob = torch.distributions.Categorical(pol).log_prob(actions)\n",
        "\t\t\treturn log_prob\n",
        "\t\n",
        "\tdef entropy(self, state):\n",
        "\t\t\tpol = self.forward(state)\n",
        "\t\t\treturn torch.distributions.Categorical(pol).entropy()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en5eOtrYe0dC"
      },
      "source": [
        "# create batch of states \n",
        "batch_states = torch.rand(5, env.observation_space.shape[0])\n",
        "\n",
        "policy = Policy(env.observation_space.shape[0], env.action_space.n)\n",
        "policy(batch_states).data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaOYNnx3hBnM"
      },
      "source": [
        "#batch_actions = policy.get_action(batch_states)\n",
        "#batch_actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkd5Oabji-el"
      },
      "source": [
        "policy.log_prob(batch_states, batch_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AYm6eCD53O2"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim,hidden=100):\n",
        "      super(Critic, self).__init__()\n",
        "    \n",
        "      self.l1 = nn.Linear(state_dim, hidden)\n",
        "      self.l2 = nn.Linear(hidden, hidden)\n",
        "      self.l3 = nn.Linear(hidden,1)\n",
        "\n",
        "    def forward(self, state):\n",
        "      q = F.leaky_relu(self.l1(state))\n",
        "      q = F.leaky_relu(self.l2(q))\n",
        "      return self.l3(q)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NJlftFuaPxE"
      },
      "source": [
        "#testing the critic output\n",
        "critic = Critic(env.observation_space.shape[0])\n",
        "critic(batch_states)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXp6nb66ZSC0"
      },
      "source": [
        "## Worker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBBJ9hbfA7Hs"
      },
      "source": [
        "class ActorCriticWorker(mp.Process):\n",
        "\tdef __init__(self,env_name,glb_critic,global_actor,opt,T,lock,gamma = 0.99,max_step=100):\n",
        "\t\tsuper(ActorCriticWorker, self).__init__()\n",
        "\t\tself.env = gym.make(env_name)\n",
        "\t\tself.t = 0\n",
        "\t\tself.max_step = max_step\n",
        "\t\tself.T = T\n",
        "\t\tself.lock = lock\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.opt = opt\n",
        "\n",
        "\t\tself.actor = Policy(self.env.observation_space.shape[0], self.env.action_space.n)\n",
        "\t\tself.critic = Critic(self.env.observation_space.shape[0])\n",
        "\t\tself.global_critic = global_critic\n",
        "\t\tself.global_actor = global_actor\n",
        "\t\n",
        "\tdef run(self):\n",
        "\n",
        "\t\t# 1. Sync local from global - we need this for the actor: get_action()\n",
        "\t\tself.actor.load_state_dict(self.global_actor.state_dict())\n",
        "\t\tself.critic.load_state_dict(self.global_critic.state_dict())\n",
        "\t\n",
        "\t\t# 2. Create a rollout\n",
        "\t\tt_start = self.t\t\t\n",
        "\t\tstate   = self.env.reset() #giving us a state from the gym env.\n",
        "\t\tdone    = False\n",
        "\t\tstates  = []\n",
        "\t\tactions = []\n",
        "\t\trewards = []\n",
        "\t\treturns = []\n",
        "\t\twhile not done and (self.t - t_start+1)%self.max_step !=0:\n",
        "\t\t\t\taction = self.actor.get_action(state)\n",
        "\t\t\t\tnext_state, reward,done, _info = self.env.step(action)\n",
        "\t\t\t\trewards.append(reward)\n",
        "\t\t\t\tactions.append(action)\n",
        "\t\t\t\tstates.append(state)\n",
        "\t\t\t\tstate = next_state\n",
        "\t\t\t\tself.t  += 1\t\t\t\t\t\n",
        "\t\t\t\t# lock memory\n",
        "\t\t\t\twith self.lock:\n",
        "\t\t\t\t\tself.T.value +=1\n",
        "\n",
        "\t\t# Calculate reward\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tif not done:\t\t\t\n",
        "\t\t\t\tR = self.critic(torch.tensor(state,dtype = torch.double)).item() #calculating the value function\n",
        "\t\t\telse:\n",
        "\t\t\t\tR = 0.0\n",
        "\t\t\n",
        "\t\tfor i in range(len(states)-1,-1,-1):  #Reverse because this is a bellman-type of calculation (you know all your rewards from t to the end)\n",
        "\t\t\tR = rewards[i] + self.gamma*R\n",
        "\t\t\treturns.append(R) \n",
        "\t\treturns.reverse() # list of returns\n",
        "\t\t\n",
        "\t\t# 3. Calculating Loss\n",
        "\t\tstates_t = torch.tensor(states, dtype = torch.double)\n",
        "\t\tactions_t = torch.tensor(actions, dtype = torch.int64)\n",
        "\t\treturns_t = torch.tensor(returns, dtype = torch.double)\t\n",
        "\n",
        "\t\ttd_error = returns_t - self.critic(states_t)\t# n_batch x 1\n",
        "\t\tcritic_loss = F.mse_loss(td_error) # 1 x 1\n",
        "\t\tactor_loss = -1.0*td_error.detach()*self.actor.log_prob(states_t, actions_t) # n_batch x 1\n",
        "\t\t# Take mean of the actor and critic loss \n",
        "\t\ttotal_loss = (critic_loss + actor_loss.mean())\n",
        "\n",
        "\t\t# 4. Calculate grad and update optimiser\n",
        "\t\tself.opt.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\t# align global grads to local grads\n",
        "\t\tfor gp, lp in zip(self.global_critic.parameters(), self.critic.parameters()):\n",
        "\t\t\tgp._grad = lp.grad\n",
        "\t\tfor gp, lp in zip(self.global_actor.parameters(), self.actor.parameters()):\n",
        "\t\t\tgp._grad = lp.grad\n",
        "\n",
        "\t\t# take a step!\n",
        "\t\tself.opt.step()\n",
        "\n",
        "\n",
        "\n",
        "# T is a global counter\n",
        "# Tmax is total steps overall\n",
        "# t is the local counter per process\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YquuAOX-9Vrf"
      },
      "source": [
        "x = np.zeros((1,3))\n",
        "list_x = [x,x,x]\n",
        "\n",
        "x_t=torch.tensor(list_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GoAtCL-9xjQ"
      },
      "source": [
        "x, list_x, x_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWowuEK249kp"
      },
      "source": [
        "# worker process\n",
        "# Input: A2C network, env, no of steps, \n",
        "\n",
        "# 1. \n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "global_critic = Critic(state_dim)\n",
        "global_actor = Policy(state_dim,action_dim)\n",
        "global_critic.share_memory()\n",
        "global_actor.share_memory()\n",
        "n_workers = 10\n",
        "\n",
        "\n",
        "env_name = \"LunarLander-v2\"\n",
        "\n",
        "global_opt = SharedAdam(list(list(global_critic.parameters()) + list(global_actor.parameters())))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "global_ctr = mp.Value('i',0)\n",
        "lock = mp.Lock()\n",
        "\n",
        "#pr = [ ActorCriticWorker(env_name,global_critic,global_actor,global_opt,global_ctr,lock) for _ in range(n_workers)]\n",
        "pr = []\n",
        "for i in range(mp.cpu_count()):\n",
        "  pr.append(ActorCriticWorker(env_name,global_critic,global_actor,global_opt,global_ctr,lock))\n",
        "\n",
        "for p in pr:\n",
        "    print(type(p))\n",
        "    p.start()\n",
        "\n",
        "    \n",
        "for p in pr:\n",
        "    p.join()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}