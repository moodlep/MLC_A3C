{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3-Ray-RLLIb.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZvjU5Dr6g_p3",
        "qd_Yy_nm_vs5",
        "k94AfEtXi_QT"
      ],
      "authorship_tag": "ABX9TyM9D4SqdAggXPeE6Zt/JdZL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcipolina/MLC_A3C/blob/main/Copy_of_AC3_Ray_RLLIb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZMwliNct0Iq"
      },
      "source": [
        "# Simple Demo of Ray's RLLib\n",
        "\n",
        "We show how to train a reinforcement learning environment that has been built on top of OpenAI Gym using Ray and RLlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXg2LQOd6h5"
      },
      "source": [
        "# Importing the usuals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import sklearn\n",
        "import json  #to convert result files into Json\n",
        "import sys, os\n",
        "\n",
        "# just to display my images\n",
        "import cv2  \n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He6yN8c8TwhE"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NHtaXgYN5dD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888657fe-7db6-43b7-99d7-e8f3846782bd"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufih9PL6N-GM"
      },
      "source": [
        "import os\n",
        "import Box2D\n",
        "import pyglet\n",
        "import imageio\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKFAFr32ijke",
        "outputId": "b3234133-0258-4e0a-b551-d1ec8f8e4bb4"
      },
      "source": [
        "# Gym RL algos will be running under the hood\n",
        "!pip install gym\n",
        "import gym"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nOrz6-02Rri"
      },
      "source": [
        "### RLLib in Ray\n",
        "Ray comes with many popular DRL models already coded (they are wrapped from Gym), so we are using a “packaged” A3C from RLLIB, we don’t have to code anything ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-uaV3hPyaKi"
      },
      "source": [
        "!pip install ray[rllib]\n",
        "!pip install 'ray[default]'\n",
        "\n",
        "#Warning: Given that we are executing our examples in Colab we need to restart the runtime after installing ray package"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwK0Vn391Vsm"
      },
      "source": [
        "Define directory for checkpoints\n",
        " \n",
        " Checkpoints are used for the Rollouts of the policy after training or to resume training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrXS5GjS1UtO"
      },
      "source": [
        "import shutil\n",
        "\n",
        "#Main saving directory\n",
        "CHECKPOINT_ROOT = \"tmp/a3c/lunar\"\n",
        "\n",
        "# Where checkpoints are written:\n",
        "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
        "\n",
        "# Where some data will be written and used by Tensorboard below:\n",
        "ray_results = os.getenv(\"HOME\") + \"/ray_results/\"\n",
        "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN_Z2aCqzjBe"
      },
      "source": [
        "### Initializing Ray for laptop use (without cluster)\n",
        "\n",
        "In the Ray initialization command is where we define the parallelization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brLrcsmyziAc",
        "outputId": "b14d2d3d-0e01-4900-8ce2-f5bd4d6e4b29"
      },
      "source": [
        "import ray\n",
        "#import ray.rllib.agents.ppo as ppo #Import RL model to use (pre-built on RLLIB)\n",
        "\n",
        "import ray.rllib.agents.a3c as a3c\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "#NOTE: It prints the dashboard running on a local port"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-30 09:52:56,223\tINFO services.py:1252 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metrics_export_port': 65455,\n",
              " 'node_id': '84c155fcd5ed074e8bb44b747f47800e62892b62d15e28d030d81fa3',\n",
              " 'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2021-10-30_09-52-53_834944_1026/sockets/plasma_store',\n",
              " 'raylet_ip_address': '172.28.0.2',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2021-10-30_09-52-53_834944_1026/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:6379',\n",
              " 'session_dir': '/tmp/ray/session_2021-10-30_09-52-53_834944_1026',\n",
              " 'webui_url': '127.0.0.1:8265'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw_DvFOO7Fpx"
      },
      "source": [
        "### Gym Environment Configuration in RLLib\n",
        "\n",
        "RLLib supports several environments: https://docs.ray.io/en/master/rllib-env.html\n",
        "\n",
        "Here we will train a policy with PPO using Gym's Carpole environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-aYRuO7JEO"
      },
      "source": [
        "# Env Configuration\n",
        "SELECT_ENV = \"LunarLander-v2\"                      # Specifies the OpenAI Gym environment for Cart Pole\n",
        "\n",
        "# PPO parameters are passed in a Config dict\n",
        "config = a3c.DEFAULT_CONFIG.copy()              # PPO's default configuration. See the next code cell.\n",
        "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2esb8tps5hl"
      },
      "source": [
        "The general synthax for RLLib envs is to instantiate the class with parameters into a Congfig Dict\n",
        "\n",
        "```\n",
        "ray.init()\n",
        "trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
        "    \"env_config\": {},  # config to pass to env class\n",
        "      })\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMIe0GeNrWv6",
        "outputId": "66b6d58b-a259-4481-f53d-725b765a7ced"
      },
      "source": [
        "# Initializes the training class and object\n",
        "agent = a3c.A3CTrainer(config, env=SELECT_ENV)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-30 09:52:58,429\tINFO trainer.py:741 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "2021-10-30 09:52:58,432\tINFO trainer.py:760 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2021-10-30 09:53:09,576\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
            "2021-10-30 09:53:09,587\tINFO trainable.py:112 -- Trainable.setup took 11.160 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2021-10-30 09:53:09,590\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEDyuUZa8KDf"
      },
      "source": [
        "### Training\n",
        "Results are saved at: root/ray_results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83EuranD75JK",
        "outputId": "b547ca19-a0fd-4857-a684-f38108011951"
      },
      "source": [
        "# Training\n",
        "N_ITER = 200 #only 3 iterations to show the idea   (By default, training runs for 10 iterations).\n",
        "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
        "\n",
        "for n in range(N_ITER):\n",
        "  result = agent.train()                   # each call to agent.train() returns a object containing information that we will inspect below\n",
        "  file_name = agent.save(CHECKPOINT_ROOT) \n",
        "\n",
        "  #Print training stats\n",
        "  print(s.format(\n",
        "    n + 1,\n",
        "    result[\"episode_reward_min\"],\n",
        "    result[\"episode_reward_mean\"],\n",
        "    result[\"episode_reward_max\"],\n",
        "    result[\"episode_len_mean\"],\n",
        "    file_name\n",
        "   ))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1 reward    nan/   nan/   nan len    nan saved tmp/a3c/lunar/checkpoint_000001/checkpoint-1\n",
            "  2 reward -445.91/-269.83/-81.73 len  96.63 saved tmp/a3c/lunar/checkpoint_000002/checkpoint-2\n",
            "  3 reward -661.16/-364.29/-81.73 len  94.69 saved tmp/a3c/lunar/checkpoint_000003/checkpoint-3\n",
            "  4 reward -743.96/-392.93/-81.73 len  92.10 saved tmp/a3c/lunar/checkpoint_000004/checkpoint-4\n",
            "  5 reward -766.74/-387.27/-13.26 len  95.17 saved tmp/a3c/lunar/checkpoint_000005/checkpoint-5\n",
            "  6 reward -766.74/-394.93/-13.26 len  99.41 saved tmp/a3c/lunar/checkpoint_000006/checkpoint-6\n",
            "  7 reward -914.42/-422.20/-13.26 len 104.42 saved tmp/a3c/lunar/checkpoint_000007/checkpoint-7\n",
            "  8 reward -914.42/-467.56/-13.26 len 107.19 saved tmp/a3c/lunar/checkpoint_000008/checkpoint-8\n",
            "  9 reward -914.42/-481.65/-13.26 len 110.61 saved tmp/a3c/lunar/checkpoint_000009/checkpoint-9\n",
            " 10 reward -914.42/-488.12/-13.26 len 116.04 saved tmp/a3c/lunar/checkpoint_000010/checkpoint-10\n",
            " 11 reward -914.42/-483.62/-13.26 len 122.14 saved tmp/a3c/lunar/checkpoint_000011/checkpoint-11\n",
            " 12 reward -914.42/-476.51/-67.77 len 125.07 saved tmp/a3c/lunar/checkpoint_000012/checkpoint-12\n",
            " 13 reward -914.42/-465.60/-67.77 len 118.45 saved tmp/a3c/lunar/checkpoint_000013/checkpoint-13\n",
            " 14 reward -907.10/-433.68/-67.77 len 110.23 saved tmp/a3c/lunar/checkpoint_000014/checkpoint-14\n",
            " 15 reward -900.43/-396.89/ 25.37 len 107.38 saved tmp/a3c/lunar/checkpoint_000015/checkpoint-15\n",
            " 16 reward -900.43/-367.68/ 25.37 len 103.24 saved tmp/a3c/lunar/checkpoint_000016/checkpoint-16\n",
            " 17 reward -752.95/-370.65/ 25.37 len  99.02 saved tmp/a3c/lunar/checkpoint_000017/checkpoint-17\n",
            " 18 reward -752.95/-379.62/ 25.37 len  98.10 saved tmp/a3c/lunar/checkpoint_000018/checkpoint-18\n",
            " 19 reward -752.95/-387.01/ 25.37 len 105.08 saved tmp/a3c/lunar/checkpoint_000019/checkpoint-19\n",
            " 20 reward -752.95/-412.25/ 25.37 len 107.98 saved tmp/a3c/lunar/checkpoint_000020/checkpoint-20\n",
            " 21 reward -752.95/-424.10/ 25.37 len 110.39 saved tmp/a3c/lunar/checkpoint_000021/checkpoint-21\n",
            " 22 reward -752.95/-438.57/-80.91 len 111.71 saved tmp/a3c/lunar/checkpoint_000022/checkpoint-22\n",
            " 23 reward -829.74/-436.47/ 10.63 len 114.15 saved tmp/a3c/lunar/checkpoint_000023/checkpoint-23\n",
            " 24 reward -829.74/-427.09/ 10.63 len 115.31 saved tmp/a3c/lunar/checkpoint_000024/checkpoint-24\n",
            " 25 reward -829.74/-426.69/ 10.63 len 117.65 saved tmp/a3c/lunar/checkpoint_000025/checkpoint-25\n",
            " 26 reward -855.09/-415.71/ 10.63 len 115.44 saved tmp/a3c/lunar/checkpoint_000026/checkpoint-26\n",
            " 27 reward -855.09/-384.23/ 10.63 len 114.41 saved tmp/a3c/lunar/checkpoint_000027/checkpoint-27\n",
            " 28 reward -855.09/-367.98/ 10.63 len 113.04 saved tmp/a3c/lunar/checkpoint_000028/checkpoint-28\n",
            " 29 reward -855.09/-378.92/-53.50 len 109.26 saved tmp/a3c/lunar/checkpoint_000029/checkpoint-29\n",
            " 30 reward -855.09/-388.26/-32.79 len 110.60 saved tmp/a3c/lunar/checkpoint_000030/checkpoint-30\n",
            " 31 reward -855.09/-397.74/-32.79 len 107.96 saved tmp/a3c/lunar/checkpoint_000031/checkpoint-31\n",
            " 32 reward -854.20/-381.53/-32.79 len 107.08 saved tmp/a3c/lunar/checkpoint_000032/checkpoint-32\n",
            " 33 reward -854.20/-405.36/-32.79 len 108.75 saved tmp/a3c/lunar/checkpoint_000033/checkpoint-33\n",
            " 34 reward -854.20/-423.60/-32.79 len 108.67 saved tmp/a3c/lunar/checkpoint_000034/checkpoint-34\n",
            " 35 reward -854.20/-429.16/-32.79 len 110.56 saved tmp/a3c/lunar/checkpoint_000035/checkpoint-35\n",
            " 36 reward -854.20/-414.59/-17.51 len 111.92 saved tmp/a3c/lunar/checkpoint_000036/checkpoint-36\n",
            " 37 reward -787.77/-393.80/-17.51 len 109.74 saved tmp/a3c/lunar/checkpoint_000037/checkpoint-37\n",
            " 38 reward -787.77/-406.26/-17.51 len 111.96 saved tmp/a3c/lunar/checkpoint_000038/checkpoint-38\n",
            " 39 reward -787.77/-404.07/-17.51 len 108.12 saved tmp/a3c/lunar/checkpoint_000039/checkpoint-39\n",
            " 40 reward -787.77/-399.46/-17.51 len 107.36 saved tmp/a3c/lunar/checkpoint_000040/checkpoint-40\n",
            " 41 reward -813.84/-387.81/-17.51 len 107.01 saved tmp/a3c/lunar/checkpoint_000041/checkpoint-41\n",
            " 42 reward -813.84/-391.03/-27.23 len 109.43 saved tmp/a3c/lunar/checkpoint_000042/checkpoint-42\n",
            " 43 reward -813.84/-422.45/-55.56 len 111.45 saved tmp/a3c/lunar/checkpoint_000043/checkpoint-43\n",
            " 44 reward -813.84/-436.64/-55.56 len 103.86 saved tmp/a3c/lunar/checkpoint_000044/checkpoint-44\n",
            " 45 reward -813.84/-437.29/-40.93 len 105.66 saved tmp/a3c/lunar/checkpoint_000045/checkpoint-45\n",
            " 46 reward -813.84/-415.20/-40.93 len 105.60 saved tmp/a3c/lunar/checkpoint_000046/checkpoint-46\n",
            " 47 reward -759.98/-403.32/ -1.29 len 107.86 saved tmp/a3c/lunar/checkpoint_000047/checkpoint-47\n",
            " 48 reward -759.98/-401.00/ 11.65 len 104.45 saved tmp/a3c/lunar/checkpoint_000048/checkpoint-48\n",
            " 49 reward -759.98/-367.55/ 11.65 len 105.97 saved tmp/a3c/lunar/checkpoint_000049/checkpoint-49\n",
            " 50 reward -759.98/-338.74/ 11.65 len 110.26 saved tmp/a3c/lunar/checkpoint_000050/checkpoint-50\n",
            " 51 reward -771.47/-341.68/ 11.65 len 110.80 saved tmp/a3c/lunar/checkpoint_000051/checkpoint-51\n",
            " 52 reward -771.47/-343.67/ 11.65 len 113.49 saved tmp/a3c/lunar/checkpoint_000052/checkpoint-52\n",
            " 53 reward -771.47/-362.10/ 11.65 len 110.15 saved tmp/a3c/lunar/checkpoint_000053/checkpoint-53\n",
            " 54 reward -771.47/-356.22/ 11.65 len 110.03 saved tmp/a3c/lunar/checkpoint_000054/checkpoint-54\n",
            " 55 reward -771.47/-359.13/  0.71 len 112.86 saved tmp/a3c/lunar/checkpoint_000055/checkpoint-55\n",
            " 56 reward -771.47/-335.79/  0.71 len 110.17 saved tmp/a3c/lunar/checkpoint_000056/checkpoint-56\n",
            " 57 reward -771.47/-322.29/  0.71 len 110.93 saved tmp/a3c/lunar/checkpoint_000057/checkpoint-57\n",
            " 58 reward -755.47/-342.70/-33.65 len 107.55 saved tmp/a3c/lunar/checkpoint_000058/checkpoint-58\n",
            " 59 reward -730.70/-331.93/-33.65 len 106.01 saved tmp/a3c/lunar/checkpoint_000059/checkpoint-59\n",
            " 60 reward -730.70/-341.15/-33.65 len 106.00 saved tmp/a3c/lunar/checkpoint_000060/checkpoint-60\n",
            " 61 reward -730.70/-330.60/-33.65 len 105.43 saved tmp/a3c/lunar/checkpoint_000061/checkpoint-61\n",
            " 62 reward -730.70/-363.45/-44.08 len 108.16 saved tmp/a3c/lunar/checkpoint_000062/checkpoint-62\n",
            " 63 reward -730.70/-380.10/-58.57 len 111.17 saved tmp/a3c/lunar/checkpoint_000063/checkpoint-63\n",
            " 64 reward -730.70/-378.98/-58.57 len 109.79 saved tmp/a3c/lunar/checkpoint_000064/checkpoint-64\n",
            " 65 reward -730.70/-392.95/-58.57 len 110.60 saved tmp/a3c/lunar/checkpoint_000065/checkpoint-65\n",
            " 66 reward -715.19/-389.83/-58.57 len 113.09 saved tmp/a3c/lunar/checkpoint_000066/checkpoint-66\n",
            " 67 reward -715.19/-392.80/-66.43 len 110.00 saved tmp/a3c/lunar/checkpoint_000067/checkpoint-67\n",
            " 68 reward -715.19/-376.78/-66.43 len 108.79 saved tmp/a3c/lunar/checkpoint_000068/checkpoint-68\n",
            " 69 reward -715.19/-373.39/-66.43 len 110.26 saved tmp/a3c/lunar/checkpoint_000069/checkpoint-69\n",
            " 70 reward -715.19/-365.46/-35.92 len 110.32 saved tmp/a3c/lunar/checkpoint_000070/checkpoint-70\n",
            " 71 reward -715.19/-363.78/-35.92 len 111.10 saved tmp/a3c/lunar/checkpoint_000071/checkpoint-71\n",
            " 72 reward -715.19/-345.09/-35.92 len 118.87 saved tmp/a3c/lunar/checkpoint_000072/checkpoint-72\n",
            " 73 reward -699.31/-345.25/-35.92 len 117.32 saved tmp/a3c/lunar/checkpoint_000073/checkpoint-73\n",
            " 74 reward -699.31/-363.68/  4.18 len 118.33 saved tmp/a3c/lunar/checkpoint_000074/checkpoint-74\n",
            " 75 reward -699.31/-382.89/  4.18 len 115.99 saved tmp/a3c/lunar/checkpoint_000075/checkpoint-75\n",
            " 76 reward -717.71/-397.81/  4.18 len 114.95 saved tmp/a3c/lunar/checkpoint_000076/checkpoint-76\n",
            " 77 reward -717.71/-387.66/  4.18 len 114.76 saved tmp/a3c/lunar/checkpoint_000077/checkpoint-77\n",
            " 78 reward -717.71/-395.71/  4.18 len 113.10 saved tmp/a3c/lunar/checkpoint_000078/checkpoint-78\n",
            " 79 reward -717.71/-407.30/  4.18 len 114.72 saved tmp/a3c/lunar/checkpoint_000079/checkpoint-79\n",
            " 80 reward -800.53/-403.07/  7.53 len 125.55 saved tmp/a3c/lunar/checkpoint_000080/checkpoint-80\n",
            " 81 reward -800.53/-396.16/  7.53 len 126.05 saved tmp/a3c/lunar/checkpoint_000081/checkpoint-81\n",
            " 82 reward -800.53/-367.49/  7.53 len 124.71 saved tmp/a3c/lunar/checkpoint_000082/checkpoint-82\n",
            " 83 reward -800.53/-340.37/  7.53 len 126.98 saved tmp/a3c/lunar/checkpoint_000083/checkpoint-83\n",
            " 84 reward -800.53/-330.56/  7.53 len 128.29 saved tmp/a3c/lunar/checkpoint_000084/checkpoint-84\n",
            " 85 reward -800.53/-325.30/  7.53 len 125.42 saved tmp/a3c/lunar/checkpoint_000085/checkpoint-85\n",
            " 86 reward -926.43/-343.20/  7.53 len 121.12 saved tmp/a3c/lunar/checkpoint_000086/checkpoint-86\n",
            " 87 reward -926.43/-348.78/ 44.34 len 110.23 saved tmp/a3c/lunar/checkpoint_000087/checkpoint-87\n",
            " 88 reward -926.43/-362.66/ 44.34 len 110.38 saved tmp/a3c/lunar/checkpoint_000088/checkpoint-88\n",
            " 89 reward -926.43/-382.48/ 44.34 len 111.24 saved tmp/a3c/lunar/checkpoint_000089/checkpoint-89\n",
            " 90 reward -926.43/-376.74/ 44.34 len 112.25 saved tmp/a3c/lunar/checkpoint_000090/checkpoint-90\n",
            " 91 reward -926.43/-381.85/ 44.34 len 111.26 saved tmp/a3c/lunar/checkpoint_000091/checkpoint-91\n",
            " 92 reward -836.78/-377.41/ 44.34 len 109.61 saved tmp/a3c/lunar/checkpoint_000092/checkpoint-92\n",
            " 93 reward -836.78/-361.42/-35.93 len 107.45 saved tmp/a3c/lunar/checkpoint_000093/checkpoint-93\n",
            " 94 reward -836.78/-366.76/-11.09 len 108.67 saved tmp/a3c/lunar/checkpoint_000094/checkpoint-94\n",
            " 95 reward -836.78/-386.81/-11.09 len 105.22 saved tmp/a3c/lunar/checkpoint_000095/checkpoint-95\n",
            " 96 reward -836.78/-378.68/-11.09 len 100.97 saved tmp/a3c/lunar/checkpoint_000096/checkpoint-96\n",
            " 97 reward -836.78/-372.30/-11.09 len  96.13 saved tmp/a3c/lunar/checkpoint_000097/checkpoint-97\n",
            " 98 reward -718.28/-363.04/-11.09 len  98.07 saved tmp/a3c/lunar/checkpoint_000098/checkpoint-98\n",
            " 99 reward -718.28/-343.29/-14.95 len  97.70 saved tmp/a3c/lunar/checkpoint_000099/checkpoint-99\n",
            "100 reward -718.28/-312.36/-14.95 len  99.25 saved tmp/a3c/lunar/checkpoint_000100/checkpoint-100\n",
            "101 reward -718.28/-315.77/ 38.47 len 101.74 saved tmp/a3c/lunar/checkpoint_000101/checkpoint-101\n",
            "102 reward -718.28/-321.95/ 38.47 len 104.22 saved tmp/a3c/lunar/checkpoint_000102/checkpoint-102\n",
            "103 reward -718.28/-328.38/ 38.47 len 107.29 saved tmp/a3c/lunar/checkpoint_000103/checkpoint-103\n",
            "104 reward -726.32/-356.73/ 38.47 len 105.45 saved tmp/a3c/lunar/checkpoint_000104/checkpoint-104\n",
            "105 reward -889.05/-400.23/ 38.47 len 105.09 saved tmp/a3c/lunar/checkpoint_000105/checkpoint-105\n",
            "106 reward -889.05/-404.62/ 38.47 len 104.94 saved tmp/a3c/lunar/checkpoint_000106/checkpoint-106\n",
            "107 reward -889.05/-381.71/-32.37 len 101.63 saved tmp/a3c/lunar/checkpoint_000107/checkpoint-107\n",
            "108 reward -889.05/-379.30/-32.37 len 100.67 saved tmp/a3c/lunar/checkpoint_000108/checkpoint-108\n",
            "109 reward -889.05/-348.08/-32.37 len  97.99 saved tmp/a3c/lunar/checkpoint_000109/checkpoint-109\n",
            "110 reward -729.28/-301.76/-32.37 len  96.54 saved tmp/a3c/lunar/checkpoint_000110/checkpoint-110\n",
            "111 reward -711.97/-292.90/-32.37 len  98.79 saved tmp/a3c/lunar/checkpoint_000111/checkpoint-111\n",
            "112 reward -697.37/-299.77/-32.37 len  98.88 saved tmp/a3c/lunar/checkpoint_000112/checkpoint-112\n",
            "113 reward -697.37/-295.67/-44.72 len 101.91 saved tmp/a3c/lunar/checkpoint_000113/checkpoint-113\n",
            "114 reward -697.37/-300.60/-37.07 len 102.06 saved tmp/a3c/lunar/checkpoint_000114/checkpoint-114\n",
            "115 reward -697.37/-311.47/-37.07 len 101.65 saved tmp/a3c/lunar/checkpoint_000115/checkpoint-115\n",
            "116 reward -697.37/-310.35/-37.07 len 105.22 saved tmp/a3c/lunar/checkpoint_000116/checkpoint-116\n",
            "117 reward -628.45/-294.47/ 33.38 len 101.85 saved tmp/a3c/lunar/checkpoint_000117/checkpoint-117\n",
            "118 reward -628.45/-277.90/ 33.38 len 100.26 saved tmp/a3c/lunar/checkpoint_000118/checkpoint-118\n",
            "119 reward -628.45/-288.22/ 33.38 len  98.10 saved tmp/a3c/lunar/checkpoint_000119/checkpoint-119\n",
            "120 reward -685.09/-284.93/ 33.38 len  99.64 saved tmp/a3c/lunar/checkpoint_000120/checkpoint-120\n",
            "121 reward -685.09/-270.26/ 33.38 len 101.66 saved tmp/a3c/lunar/checkpoint_000121/checkpoint-121\n",
            "122 reward -685.09/-284.60/ 33.38 len 101.47 saved tmp/a3c/lunar/checkpoint_000122/checkpoint-122\n",
            "123 reward -685.09/-290.98/ -6.05 len 102.03 saved tmp/a3c/lunar/checkpoint_000123/checkpoint-123\n",
            "124 reward -685.09/-299.82/  4.85 len 103.76 saved tmp/a3c/lunar/checkpoint_000124/checkpoint-124\n",
            "125 reward -685.09/-303.72/  4.85 len 104.07 saved tmp/a3c/lunar/checkpoint_000125/checkpoint-125\n",
            "126 reward -618.82/-294.33/  4.85 len 103.51 saved tmp/a3c/lunar/checkpoint_000126/checkpoint-126\n",
            "127 reward -618.82/-298.45/  4.85 len 105.98 saved tmp/a3c/lunar/checkpoint_000127/checkpoint-127\n",
            "128 reward -630.70/-297.77/  4.85 len 106.92 saved tmp/a3c/lunar/checkpoint_000128/checkpoint-128\n",
            "129 reward -630.70/-298.04/  4.85 len 109.63 saved tmp/a3c/lunar/checkpoint_000129/checkpoint-129\n",
            "130 reward -630.70/-293.06/  4.85 len 110.63 saved tmp/a3c/lunar/checkpoint_000130/checkpoint-130\n",
            "131 reward -630.70/-291.20/ -6.07 len 115.08 saved tmp/a3c/lunar/checkpoint_000131/checkpoint-131\n",
            "132 reward -630.70/-279.69/-29.49 len 116.70 saved tmp/a3c/lunar/checkpoint_000132/checkpoint-132\n",
            "133 reward -760.35/-292.34/-29.49 len 122.92 saved tmp/a3c/lunar/checkpoint_000133/checkpoint-133\n",
            "134 reward -760.35/-295.27/-29.49 len 123.79 saved tmp/a3c/lunar/checkpoint_000134/checkpoint-134\n",
            "135 reward -760.35/-303.64/-29.49 len 123.41 saved tmp/a3c/lunar/checkpoint_000135/checkpoint-135\n",
            "136 reward -760.35/-305.25/ 34.02 len 121.27 saved tmp/a3c/lunar/checkpoint_000136/checkpoint-136\n",
            "137 reward -760.35/-298.34/ 34.02 len 121.36 saved tmp/a3c/lunar/checkpoint_000137/checkpoint-137\n",
            "138 reward -760.35/-295.20/ 34.02 len 125.27 saved tmp/a3c/lunar/checkpoint_000138/checkpoint-138\n",
            "139 reward -760.35/-284.23/ 34.02 len 128.08 saved tmp/a3c/lunar/checkpoint_000139/checkpoint-139\n",
            "140 reward -760.35/-269.10/ 34.02 len 126.90 saved tmp/a3c/lunar/checkpoint_000140/checkpoint-140\n",
            "141 reward -676.97/-255.47/ 34.02 len 130.18 saved tmp/a3c/lunar/checkpoint_000141/checkpoint-141\n",
            "142 reward -676.97/-242.35/ 34.02 len 133.24 saved tmp/a3c/lunar/checkpoint_000142/checkpoint-142\n",
            "143 reward -676.97/-223.10/ 34.02 len 138.61 saved tmp/a3c/lunar/checkpoint_000143/checkpoint-143\n",
            "144 reward -676.97/-213.56/  2.07 len 143.28 saved tmp/a3c/lunar/checkpoint_000144/checkpoint-144\n",
            "145 reward -676.97/-208.60/  2.07 len 146.06 saved tmp/a3c/lunar/checkpoint_000145/checkpoint-145\n",
            "146 reward -561.25/-194.46/ 47.22 len 154.71 saved tmp/a3c/lunar/checkpoint_000146/checkpoint-146\n",
            "147 reward -496.65/-189.19/ 47.22 len 151.44 saved tmp/a3c/lunar/checkpoint_000147/checkpoint-147\n",
            "148 reward -496.65/-195.99/ 47.22 len 149.58 saved tmp/a3c/lunar/checkpoint_000148/checkpoint-148\n",
            "149 reward -496.65/-188.68/ 47.22 len 148.78 saved tmp/a3c/lunar/checkpoint_000149/checkpoint-149\n",
            "150 reward -358.86/-179.61/ 47.22 len 150.91 saved tmp/a3c/lunar/checkpoint_000150/checkpoint-150\n",
            "151 reward -344.29/-174.31/ 47.22 len 151.17 saved tmp/a3c/lunar/checkpoint_000151/checkpoint-151\n",
            "152 reward -342.57/-171.11/ 47.22 len 151.17 saved tmp/a3c/lunar/checkpoint_000152/checkpoint-152\n",
            "153 reward -342.57/-176.34/ 47.22 len 153.79 saved tmp/a3c/lunar/checkpoint_000153/checkpoint-153\n",
            "154 reward -342.57/-172.80/ 47.22 len 154.75 saved tmp/a3c/lunar/checkpoint_000154/checkpoint-154\n",
            "155 reward -342.57/-170.75/ 47.22 len 167.05 saved tmp/a3c/lunar/checkpoint_000155/checkpoint-155\n",
            "156 reward -342.57/-169.44/ -4.10 len 163.24 saved tmp/a3c/lunar/checkpoint_000156/checkpoint-156\n",
            "157 reward -342.57/-165.57/ -4.10 len 164.08 saved tmp/a3c/lunar/checkpoint_000157/checkpoint-157\n",
            "158 reward -342.57/-159.09/ 19.10 len 175.75 saved tmp/a3c/lunar/checkpoint_000158/checkpoint-158\n",
            "159 reward -342.57/-154.14/ 19.10 len 177.15 saved tmp/a3c/lunar/checkpoint_000159/checkpoint-159\n",
            "160 reward -342.57/-153.22/ 19.10 len 182.08 saved tmp/a3c/lunar/checkpoint_000160/checkpoint-160\n",
            "161 reward -342.57/-151.90/ 19.10 len 194.53 saved tmp/a3c/lunar/checkpoint_000161/checkpoint-161\n",
            "162 reward -342.57/-149.77/ 19.10 len 193.84 saved tmp/a3c/lunar/checkpoint_000162/checkpoint-162\n",
            "163 reward -342.57/-148.84/ 19.10 len 196.34 saved tmp/a3c/lunar/checkpoint_000163/checkpoint-163\n",
            "164 reward -407.59/-153.16/ 19.10 len 191.00 saved tmp/a3c/lunar/checkpoint_000164/checkpoint-164\n",
            "165 reward -407.59/-151.13/ 19.10 len 200.60 saved tmp/a3c/lunar/checkpoint_000165/checkpoint-165\n",
            "166 reward -407.59/-145.28/ 19.10 len 195.07 saved tmp/a3c/lunar/checkpoint_000166/checkpoint-166\n",
            "167 reward -407.59/-145.09/ 20.63 len 198.08 saved tmp/a3c/lunar/checkpoint_000167/checkpoint-167\n",
            "168 reward -407.59/-147.54/ 20.63 len 200.72 saved tmp/a3c/lunar/checkpoint_000168/checkpoint-168\n",
            "169 reward -407.59/-142.85/ 20.63 len 195.39 saved tmp/a3c/lunar/checkpoint_000169/checkpoint-169\n",
            "170 reward -407.59/-147.61/ 20.63 len 187.50 saved tmp/a3c/lunar/checkpoint_000170/checkpoint-170\n",
            "171 reward -407.59/-144.55/ 20.63 len 190.40 saved tmp/a3c/lunar/checkpoint_000171/checkpoint-171\n",
            "172 reward -407.59/-143.76/ 20.63 len 197.75 saved tmp/a3c/lunar/checkpoint_000172/checkpoint-172\n",
            "173 reward -407.59/-142.15/ 20.63 len 199.79 saved tmp/a3c/lunar/checkpoint_000173/checkpoint-173\n",
            "174 reward -407.59/-140.31/ 20.63 len 192.96 saved tmp/a3c/lunar/checkpoint_000174/checkpoint-174\n",
            "175 reward -407.59/-138.76/ 20.63 len 206.00 saved tmp/a3c/lunar/checkpoint_000175/checkpoint-175\n",
            "176 reward -407.59/-136.38/ 20.63 len 204.07 saved tmp/a3c/lunar/checkpoint_000176/checkpoint-176\n",
            "177 reward -407.59/-132.37/ 20.63 len 208.22 saved tmp/a3c/lunar/checkpoint_000177/checkpoint-177\n",
            "178 reward -407.59/-131.44/ 20.63 len 223.14 saved tmp/a3c/lunar/checkpoint_000178/checkpoint-178\n",
            "179 reward -277.35/-125.36/ 20.63 len 234.57 saved tmp/a3c/lunar/checkpoint_000179/checkpoint-179\n",
            "180 reward -277.35/-129.29/ 20.63 len 225.46 saved tmp/a3c/lunar/checkpoint_000180/checkpoint-180\n",
            "181 reward -277.35/-128.11/ 20.63 len 233.96 saved tmp/a3c/lunar/checkpoint_000181/checkpoint-181\n",
            "182 reward -277.35/-124.13/ 20.63 len 248.71 saved tmp/a3c/lunar/checkpoint_000182/checkpoint-182\n",
            "183 reward -277.35/-124.11/ 18.81 len 240.63 saved tmp/a3c/lunar/checkpoint_000183/checkpoint-183\n",
            "184 reward -277.35/-123.26/ 18.81 len 244.15 saved tmp/a3c/lunar/checkpoint_000184/checkpoint-184\n",
            "185 reward -277.35/-122.76/ 18.81 len 245.36 saved tmp/a3c/lunar/checkpoint_000185/checkpoint-185\n",
            "186 reward -277.35/-118.77/ 18.81 len 255.87 saved tmp/a3c/lunar/checkpoint_000186/checkpoint-186\n",
            "187 reward -277.35/-118.84/ 18.81 len 269.34 saved tmp/a3c/lunar/checkpoint_000187/checkpoint-187\n",
            "188 reward -277.35/-115.60/138.58 len 276.82 saved tmp/a3c/lunar/checkpoint_000188/checkpoint-188\n",
            "189 reward -277.35/-115.46/138.58 len 279.46 saved tmp/a3c/lunar/checkpoint_000189/checkpoint-189\n",
            "190 reward -268.97/-111.72/138.58 len 287.18 saved tmp/a3c/lunar/checkpoint_000190/checkpoint-190\n",
            "191 reward -268.97/-110.25/138.58 len 291.70 saved tmp/a3c/lunar/checkpoint_000191/checkpoint-191\n",
            "192 reward -268.97/-113.76/138.58 len 291.83 saved tmp/a3c/lunar/checkpoint_000192/checkpoint-192\n",
            "193 reward -268.97/-112.78/138.58 len 294.45 saved tmp/a3c/lunar/checkpoint_000193/checkpoint-193\n",
            "194 reward -268.97/-111.61/138.58 len 298.55 saved tmp/a3c/lunar/checkpoint_000194/checkpoint-194\n",
            "195 reward -268.97/-109.59/138.58 len 300.80 saved tmp/a3c/lunar/checkpoint_000195/checkpoint-195\n",
            "196 reward -268.97/-107.71/138.58 len 306.19 saved tmp/a3c/lunar/checkpoint_000196/checkpoint-196\n",
            "197 reward -268.97/-105.94/138.58 len 310.52 saved tmp/a3c/lunar/checkpoint_000197/checkpoint-197\n",
            "198 reward -268.97/-106.35/138.58 len 307.50 saved tmp/a3c/lunar/checkpoint_000198/checkpoint-198\n",
            "199 reward -268.97/-106.35/138.58 len 307.50 saved tmp/a3c/lunar/checkpoint_000199/checkpoint-199\n",
            "200 reward -268.97/-102.69/138.58 len 322.79 saved tmp/a3c/lunar/checkpoint_000200/checkpoint-200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqbHCUAL2mTS",
        "outputId": "b405198d-229f-4f47-ad7b-679263dc1dea"
      },
      "source": [
        "#https://github.com/anyscale/academy/blob/main/ray-rllib/explore-rllib/01-Application-Cart-Pole.ipynb\n",
        "\n",
        "# Similarly, we can save the training stats on list to inspect\n",
        "results = []\n",
        "episode_data = []\n",
        "episode_json = []\n",
        "\n",
        "for n in range(N_ITER):\n",
        "    result = agent.train()\n",
        "    results.append(result)\n",
        "    \n",
        "    episode = {'n': n, \n",
        "               'episode_reward_min': result['episode_reward_min'], \n",
        "               'episode_reward_mean':result['episode_reward_mean'], \n",
        "               'episode_reward_max': result['episode_reward_max'],  \n",
        "               'episode_len_mean':   result['episode_len_mean']}\n",
        "    \n",
        "    episode_data.append(episode)\n",
        "    episode_json.append(json.dumps(episode))\n",
        "    file_name = agent.save(CHECKPOINT_ROOT)\n",
        "    \n",
        "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0: Min/Mean/Max reward: -268.9696/-101.7669/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000201/checkpoint-201\n",
            "  1: Min/Mean/Max reward: -268.9696/-101.7669/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000202/checkpoint-202\n",
            "  2: Min/Mean/Max reward: -268.9696/-98.5094/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000203/checkpoint-203\n",
            "  3: Min/Mean/Max reward: -268.9696/-96.3411/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000204/checkpoint-204\n",
            "  4: Min/Mean/Max reward: -268.9696/-95.0554/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000205/checkpoint-205\n",
            "  5: Min/Mean/Max reward: -268.9696/-95.1753/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000206/checkpoint-206\n",
            "  6: Min/Mean/Max reward: -268.9696/-94.1470/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000207/checkpoint-207\n",
            "  7: Min/Mean/Max reward: -268.9696/-94.5182/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000208/checkpoint-208\n",
            "  8: Min/Mean/Max reward: -226.1665/-90.7522/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000209/checkpoint-209\n",
            "  9: Min/Mean/Max reward: -226.1665/-89.6393/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000210/checkpoint-210\n",
            " 10: Min/Mean/Max reward: -226.1665/-89.6875/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000211/checkpoint-211\n",
            " 11: Min/Mean/Max reward: -226.1665/-89.6875/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000212/checkpoint-212\n",
            " 12: Min/Mean/Max reward: -226.1665/-87.7630/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000213/checkpoint-213\n",
            " 13: Min/Mean/Max reward: -226.1665/-88.2660/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000214/checkpoint-214\n",
            " 14: Min/Mean/Max reward: -226.1665/-87.9440/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000215/checkpoint-215\n",
            " 15: Min/Mean/Max reward: -226.1665/-87.4719/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000216/checkpoint-216\n",
            " 16: Min/Mean/Max reward: -226.1665/-87.4719/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000217/checkpoint-217\n",
            " 17: Min/Mean/Max reward: -226.1665/-84.3624/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000218/checkpoint-218\n",
            " 18: Min/Mean/Max reward: -226.1665/-85.2836/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000219/checkpoint-219\n",
            " 19: Min/Mean/Max reward: -226.1665/-83.2456/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000220/checkpoint-220\n",
            " 20: Min/Mean/Max reward: -226.1665/-81.7388/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000221/checkpoint-221\n",
            " 21: Min/Mean/Max reward: -226.1665/-81.4026/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000222/checkpoint-222\n",
            " 22: Min/Mean/Max reward: -226.1665/-80.1482/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000223/checkpoint-223\n",
            " 23: Min/Mean/Max reward: -226.1665/-77.3316/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000224/checkpoint-224\n",
            " 24: Min/Mean/Max reward: -226.1665/-76.0086/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000225/checkpoint-225\n",
            " 25: Min/Mean/Max reward: -226.1665/-74.8685/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000226/checkpoint-226\n",
            " 26: Min/Mean/Max reward: -226.1665/-74.8685/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000227/checkpoint-227\n",
            " 27: Min/Mean/Max reward: -226.1665/-73.0599/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000228/checkpoint-228\n",
            " 28: Min/Mean/Max reward: -226.1665/-72.2364/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000229/checkpoint-229\n",
            " 29: Min/Mean/Max reward: -226.1665/-71.2043/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000230/checkpoint-230\n",
            " 30: Min/Mean/Max reward: -226.1665/-71.2043/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000231/checkpoint-231\n",
            " 31: Min/Mean/Max reward: -226.1665/-69.5023/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000232/checkpoint-232\n",
            " 32: Min/Mean/Max reward: -226.1665/-70.4265/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000233/checkpoint-233\n",
            " 33: Min/Mean/Max reward: -226.1665/-69.9634/138.5827. Checkpoint saved to tmp/a3c/lunar/checkpoint_000234/checkpoint-234\n",
            " 34: Min/Mean/Max reward: -226.1665/-71.6038/111.9317. Checkpoint saved to tmp/a3c/lunar/checkpoint_000235/checkpoint-235\n",
            " 35: Min/Mean/Max reward: -226.1665/-68.6729/111.9317. Checkpoint saved to tmp/a3c/lunar/checkpoint_000236/checkpoint-236\n",
            " 36: Min/Mean/Max reward: -226.1665/-67.9824/111.9317. Checkpoint saved to tmp/a3c/lunar/checkpoint_000237/checkpoint-237\n",
            " 37: Min/Mean/Max reward: -226.1665/-67.7608/111.9317. Checkpoint saved to tmp/a3c/lunar/checkpoint_000238/checkpoint-238\n",
            " 38: Min/Mean/Max reward: -226.1665/-67.8296/111.9317. Checkpoint saved to tmp/a3c/lunar/checkpoint_000239/checkpoint-239\n",
            " 39: Min/Mean/Max reward: -199.8671/-64.3748/119.3103. Checkpoint saved to tmp/a3c/lunar/checkpoint_000240/checkpoint-240\n",
            " 40: Min/Mean/Max reward: -199.8671/-62.0672/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000241/checkpoint-241\n",
            " 41: Min/Mean/Max reward: -199.8671/-60.5602/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000242/checkpoint-242\n",
            " 42: Min/Mean/Max reward: -199.8671/-57.9980/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000243/checkpoint-243\n",
            " 43: Min/Mean/Max reward: -199.8671/-59.3564/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000244/checkpoint-244\n",
            " 44: Min/Mean/Max reward: -199.8671/-58.2628/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000245/checkpoint-245\n",
            " 45: Min/Mean/Max reward: -199.8671/-56.8990/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000246/checkpoint-246\n",
            " 46: Min/Mean/Max reward: -199.8671/-56.8990/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000247/checkpoint-247\n",
            " 47: Min/Mean/Max reward: -199.8671/-54.3442/198.5520. Checkpoint saved to tmp/a3c/lunar/checkpoint_000248/checkpoint-248\n",
            " 48: Min/Mean/Max reward: -199.8671/-49.5868/215.8549. Checkpoint saved to tmp/a3c/lunar/checkpoint_000249/checkpoint-249\n",
            " 49: Min/Mean/Max reward: -199.8671/-50.0714/215.8549. Checkpoint saved to tmp/a3c/lunar/checkpoint_000250/checkpoint-250\n",
            " 50: Min/Mean/Max reward: -199.8671/-50.0714/215.8549. Checkpoint saved to tmp/a3c/lunar/checkpoint_000251/checkpoint-251\n",
            " 51: Min/Mean/Max reward: -199.8671/-47.3307/215.8549. Checkpoint saved to tmp/a3c/lunar/checkpoint_000252/checkpoint-252\n",
            " 52: Min/Mean/Max reward: -199.8671/-47.1937/215.8549. Checkpoint saved to tmp/a3c/lunar/checkpoint_000253/checkpoint-253\n",
            " 53: Min/Mean/Max reward: -199.8671/-47.1937/215.8549. Checkpoint saved to tmp/a3c/lunar/checkpoint_000254/checkpoint-254\n",
            " 54: Min/Mean/Max reward: -199.8671/-40.9039/215.8549. Checkpoint saved to tmp/a3c/lunar/checkpoint_000255/checkpoint-255\n",
            " 55: Min/Mean/Max reward: -199.8671/-36.9419/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000256/checkpoint-256\n",
            " 56: Min/Mean/Max reward: -199.8671/-35.7315/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000257/checkpoint-257\n",
            " 57: Min/Mean/Max reward: -199.8671/-34.3784/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000258/checkpoint-258\n",
            " 58: Min/Mean/Max reward: -199.8671/-35.2811/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000259/checkpoint-259\n",
            " 59: Min/Mean/Max reward: -199.8671/-35.5493/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000260/checkpoint-260\n",
            " 60: Min/Mean/Max reward: -199.8671/-35.4290/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000261/checkpoint-261\n",
            " 61: Min/Mean/Max reward: -199.8671/-35.4759/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000262/checkpoint-262\n",
            " 62: Min/Mean/Max reward: -199.8671/-35.4759/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000263/checkpoint-263\n",
            " 63: Min/Mean/Max reward: -199.8671/-34.4358/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000264/checkpoint-264\n",
            " 64: Min/Mean/Max reward: -199.8671/-31.4913/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000265/checkpoint-265\n",
            " 65: Min/Mean/Max reward: -199.8671/-31.0810/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000266/checkpoint-266\n",
            " 66: Min/Mean/Max reward: -199.8671/-28.5697/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000267/checkpoint-267\n",
            " 67: Min/Mean/Max reward: -199.8671/-25.5604/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000268/checkpoint-268\n",
            " 68: Min/Mean/Max reward: -199.8671/-22.5709/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000269/checkpoint-269\n",
            " 69: Min/Mean/Max reward: -199.8671/-19.3007/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000270/checkpoint-270\n",
            " 70: Min/Mean/Max reward: -199.8671/-20.2041/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000271/checkpoint-271\n",
            " 71: Min/Mean/Max reward: -199.8671/-15.4562/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000272/checkpoint-272\n",
            " 72: Min/Mean/Max reward: -199.8671/-16.8670/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000273/checkpoint-273\n",
            " 73: Min/Mean/Max reward: -199.8671/-16.8670/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000274/checkpoint-274\n",
            " 74: Min/Mean/Max reward: -199.8671/-13.9985/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000275/checkpoint-275\n",
            " 75: Min/Mean/Max reward: -199.8671/-13.9985/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000276/checkpoint-276\n",
            " 76: Min/Mean/Max reward: -199.8671/-13.3893/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000277/checkpoint-277\n",
            " 77: Min/Mean/Max reward: -199.8671/-10.8171/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000278/checkpoint-278\n",
            " 78: Min/Mean/Max reward: -199.8671/ -9.7764/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000279/checkpoint-279\n",
            " 79: Min/Mean/Max reward: -199.8671/ -9.7639/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000280/checkpoint-280\n",
            " 80: Min/Mean/Max reward: -199.8671/ -9.7639/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000281/checkpoint-281\n",
            " 81: Min/Mean/Max reward: -199.8671/ -8.1864/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000282/checkpoint-282\n",
            " 82: Min/Mean/Max reward: -199.8671/ -7.0477/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000283/checkpoint-283\n",
            " 83: Min/Mean/Max reward: -199.8671/ -6.4615/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000284/checkpoint-284\n",
            " 84: Min/Mean/Max reward: -199.8671/ -7.5566/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000285/checkpoint-285\n",
            " 85: Min/Mean/Max reward: -199.8671/ -4.8173/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000286/checkpoint-286\n",
            " 86: Min/Mean/Max reward: -199.8671/ -2.6801/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000287/checkpoint-287\n",
            " 87: Min/Mean/Max reward: -151.1565/ -0.9373/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000288/checkpoint-288\n",
            " 88: Min/Mean/Max reward: -151.1565/ -1.1826/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000289/checkpoint-289\n",
            " 89: Min/Mean/Max reward: -151.1565/  0.7740/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000290/checkpoint-290\n",
            " 90: Min/Mean/Max reward: -151.1565/  1.6393/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000291/checkpoint-291\n",
            " 91: Min/Mean/Max reward: -151.1565/  1.1380/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000292/checkpoint-292\n",
            " 92: Min/Mean/Max reward: -151.1565/  2.4021/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000293/checkpoint-293\n",
            " 93: Min/Mean/Max reward: -151.1565/  1.9189/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000294/checkpoint-294\n",
            " 94: Min/Mean/Max reward: -151.1565/  1.9189/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000295/checkpoint-295\n",
            " 95: Min/Mean/Max reward: -151.1565/  5.0859/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000296/checkpoint-296\n",
            " 96: Min/Mean/Max reward: -151.1565/  4.2757/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000297/checkpoint-297\n",
            " 97: Min/Mean/Max reward: -151.1565/  2.8727/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000298/checkpoint-298\n",
            " 98: Min/Mean/Max reward: -151.1565/  3.1209/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000299/checkpoint-299\n",
            " 99: Min/Mean/Max reward: -151.1565/  5.2053/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000300/checkpoint-300\n",
            "100: Min/Mean/Max reward: -151.1565/  6.1524/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000301/checkpoint-301\n",
            "101: Min/Mean/Max reward: -151.1565/  7.3274/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000302/checkpoint-302\n",
            "102: Min/Mean/Max reward: -151.1565/  6.5089/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000303/checkpoint-303\n",
            "103: Min/Mean/Max reward: -151.1565/  4.9120/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000304/checkpoint-304\n",
            "104: Min/Mean/Max reward: -151.1565/  4.9120/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000305/checkpoint-305\n",
            "105: Min/Mean/Max reward: -151.1565/  4.7943/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000306/checkpoint-306\n",
            "106: Min/Mean/Max reward: -151.1565/  6.7578/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000307/checkpoint-307\n",
            "107: Min/Mean/Max reward: -151.1565/  8.0382/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000308/checkpoint-308\n",
            "108: Min/Mean/Max reward: -151.1565/  8.3639/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000309/checkpoint-309\n",
            "109: Min/Mean/Max reward: -151.1565/  5.3028/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000310/checkpoint-310\n",
            "110: Min/Mean/Max reward: -151.1565/  6.9972/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000311/checkpoint-311\n",
            "111: Min/Mean/Max reward: -151.1565/  6.9972/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000312/checkpoint-312\n",
            "112: Min/Mean/Max reward: -151.1565/ 10.9131/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000313/checkpoint-313\n",
            "113: Min/Mean/Max reward: -151.1565/ 11.0726/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000314/checkpoint-314\n",
            "114: Min/Mean/Max reward: -151.1565/ 10.2512/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000315/checkpoint-315\n",
            "115: Min/Mean/Max reward: -244.7429/ 10.6318/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000316/checkpoint-316\n",
            "116: Min/Mean/Max reward: -244.7429/ 12.4853/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000317/checkpoint-317\n",
            "117: Min/Mean/Max reward: -244.7429/ 13.8926/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000318/checkpoint-318\n",
            "118: Min/Mean/Max reward: -244.7429/ 16.1063/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000319/checkpoint-319\n",
            "119: Min/Mean/Max reward: -244.7429/ 16.1063/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000320/checkpoint-320\n",
            "120: Min/Mean/Max reward: -244.7429/ 16.3727/235.5930. Checkpoint saved to tmp/a3c/lunar/checkpoint_000321/checkpoint-321\n",
            "121: Min/Mean/Max reward: -244.7429/ 16.4657/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000322/checkpoint-322\n",
            "122: Min/Mean/Max reward: -244.7429/ 19.4286/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000323/checkpoint-323\n",
            "123: Min/Mean/Max reward: -244.7429/ 20.6886/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000324/checkpoint-324\n",
            "124: Min/Mean/Max reward: -244.7429/ 18.0356/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000325/checkpoint-325\n",
            "125: Min/Mean/Max reward: -244.7429/ 18.8619/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000326/checkpoint-326\n",
            "126: Min/Mean/Max reward: -244.7429/ 15.7907/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000327/checkpoint-327\n",
            "127: Min/Mean/Max reward: -244.7429/ 17.7326/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000328/checkpoint-328\n",
            "128: Min/Mean/Max reward: -244.7429/ 16.5840/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000329/checkpoint-329\n",
            "129: Min/Mean/Max reward: -244.7429/ 19.0097/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000330/checkpoint-330\n",
            "130: Min/Mean/Max reward: -244.7429/ 19.2744/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000331/checkpoint-331\n",
            "131: Min/Mean/Max reward: -244.7429/ 19.5855/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000332/checkpoint-332\n",
            "132: Min/Mean/Max reward: -244.7429/ 16.3473/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000333/checkpoint-333\n",
            "133: Min/Mean/Max reward: -244.7429/ 13.3256/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000334/checkpoint-334\n",
            "134: Min/Mean/Max reward: -244.7429/ 10.6277/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000335/checkpoint-335\n",
            "135: Min/Mean/Max reward: -244.7429/ 10.2007/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000336/checkpoint-336\n",
            "136: Min/Mean/Max reward: -244.7429/ 13.0571/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000337/checkpoint-337\n",
            "137: Min/Mean/Max reward: -244.7429/ 12.1530/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000338/checkpoint-338\n",
            "138: Min/Mean/Max reward: -244.7429/ 11.1882/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000339/checkpoint-339\n",
            "139: Min/Mean/Max reward: -244.7429/ 13.1407/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000340/checkpoint-340\n",
            "140: Min/Mean/Max reward: -244.7429/ 13.1407/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000341/checkpoint-341\n",
            "141: Min/Mean/Max reward: -244.7429/ 15.9086/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000342/checkpoint-342\n",
            "142: Min/Mean/Max reward: -244.7429/ 12.5444/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000343/checkpoint-343\n",
            "143: Min/Mean/Max reward: -244.7429/ 12.3670/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000344/checkpoint-344\n",
            "144: Min/Mean/Max reward: -244.7429/ 12.0394/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000345/checkpoint-345\n",
            "145: Min/Mean/Max reward: -244.7429/ 12.0394/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000346/checkpoint-346\n",
            "146: Min/Mean/Max reward: -244.7429/  9.3927/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000347/checkpoint-347\n",
            "147: Min/Mean/Max reward: -244.7429/ 11.5899/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000348/checkpoint-348\n",
            "148: Min/Mean/Max reward: -244.7429/ 11.5899/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000349/checkpoint-349\n",
            "149: Min/Mean/Max reward: -244.7429/  9.3790/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000350/checkpoint-350\n",
            "150: Min/Mean/Max reward: -244.7429/ 13.3309/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000351/checkpoint-351\n",
            "151: Min/Mean/Max reward: -244.7429/  8.2397/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000352/checkpoint-352\n",
            "152: Min/Mean/Max reward: -244.7429/  9.4212/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000353/checkpoint-353\n",
            "153: Min/Mean/Max reward: -244.7429/ 10.6233/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000354/checkpoint-354\n",
            "154: Min/Mean/Max reward: -244.7429/ 12.6883/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000355/checkpoint-355\n",
            "155: Min/Mean/Max reward: -244.7429/ 14.6925/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000356/checkpoint-356\n",
            "156: Min/Mean/Max reward: -244.7429/ 14.6348/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000357/checkpoint-357\n",
            "157: Min/Mean/Max reward: -244.7429/ 12.1000/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000358/checkpoint-358\n",
            "158: Min/Mean/Max reward: -244.7429/ 11.8437/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000359/checkpoint-359\n",
            "159: Min/Mean/Max reward: -244.7429/ 11.0534/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000360/checkpoint-360\n",
            "160: Min/Mean/Max reward: -244.7429/  9.3627/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000361/checkpoint-361\n",
            "161: Min/Mean/Max reward: -244.7429/  9.3765/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000362/checkpoint-362\n",
            "162: Min/Mean/Max reward: -244.7429/ 10.0506/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000363/checkpoint-363\n",
            "163: Min/Mean/Max reward: -244.7429/  8.0016/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000364/checkpoint-364\n",
            "164: Min/Mean/Max reward: -244.7429/  9.6367/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000365/checkpoint-365\n",
            "165: Min/Mean/Max reward: -244.7429/  9.4335/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000366/checkpoint-366\n",
            "166: Min/Mean/Max reward: -244.7429/ 10.5477/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000367/checkpoint-367\n",
            "167: Min/Mean/Max reward: -244.7429/  8.8528/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000368/checkpoint-368\n",
            "168: Min/Mean/Max reward: -244.7429/  8.8528/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000369/checkpoint-369\n",
            "169: Min/Mean/Max reward: -244.7429/ 10.5337/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000370/checkpoint-370\n",
            "170: Min/Mean/Max reward: -244.7429/ 13.9817/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000371/checkpoint-371\n",
            "171: Min/Mean/Max reward: -244.7429/ 15.2547/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000372/checkpoint-372\n",
            "172: Min/Mean/Max reward: -244.7429/ 13.2288/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000373/checkpoint-373\n",
            "173: Min/Mean/Max reward: -244.7429/ 13.2288/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000374/checkpoint-374\n",
            "174: Min/Mean/Max reward: -244.7429/ 12.4076/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000375/checkpoint-375\n",
            "175: Min/Mean/Max reward: -244.7429/ 11.8977/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000376/checkpoint-376\n",
            "176: Min/Mean/Max reward: -244.7429/ 13.9869/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000377/checkpoint-377\n",
            "177: Min/Mean/Max reward: -244.7429/ 13.8107/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000378/checkpoint-378\n",
            "178: Min/Mean/Max reward: -244.7429/ 10.6820/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000379/checkpoint-379\n",
            "179: Min/Mean/Max reward: -244.7429/ 10.7554/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000380/checkpoint-380\n",
            "180: Min/Mean/Max reward: -244.7429/ 10.3172/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000381/checkpoint-381\n",
            "181: Min/Mean/Max reward: -244.7429/ 12.7486/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000382/checkpoint-382\n",
            "182: Min/Mean/Max reward: -228.6080/ 14.1748/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000383/checkpoint-383\n",
            "183: Min/Mean/Max reward: -228.6080/ 12.9935/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000384/checkpoint-384\n",
            "184: Min/Mean/Max reward: -228.6080/ 10.6315/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000385/checkpoint-385\n",
            "185: Min/Mean/Max reward: -228.6080/  7.7572/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000386/checkpoint-386\n",
            "186: Min/Mean/Max reward: -228.6080/  6.2158/242.7248. Checkpoint saved to tmp/a3c/lunar/checkpoint_000387/checkpoint-387\n",
            "187: Min/Mean/Max reward: -228.6080/  3.3740/228.5455. Checkpoint saved to tmp/a3c/lunar/checkpoint_000388/checkpoint-388\n",
            "188: Min/Mean/Max reward: -228.6080/  2.3387/228.5455. Checkpoint saved to tmp/a3c/lunar/checkpoint_000389/checkpoint-389\n",
            "189: Min/Mean/Max reward: -228.6080/  9.1095/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000390/checkpoint-390\n",
            "190: Min/Mean/Max reward: -228.6080/  3.9677/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000391/checkpoint-391\n",
            "191: Min/Mean/Max reward: -228.6080/  5.5462/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000392/checkpoint-392\n",
            "192: Min/Mean/Max reward: -228.6080/  4.8973/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000393/checkpoint-393\n",
            "193: Min/Mean/Max reward: -228.6080/  3.5790/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000394/checkpoint-394\n",
            "194: Min/Mean/Max reward: -228.6080/  1.4917/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000395/checkpoint-395\n",
            "195: Min/Mean/Max reward: -228.6080/  4.1631/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000396/checkpoint-396\n",
            "196: Min/Mean/Max reward: -228.6080/  5.5245/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000397/checkpoint-397\n",
            "197: Min/Mean/Max reward: -228.6080/ 10.6734/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000398/checkpoint-398\n",
            "198: Min/Mean/Max reward: -228.6080/  6.5549/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000399/checkpoint-399\n",
            "199: Min/Mean/Max reward: -228.6080/  9.0883/231.1330. Checkpoint saved to tmp/a3c/lunar/checkpoint_000400/checkpoint-400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plYcowdcmsyB"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "id": "KTdpLisvUCsJ",
        "outputId": "27923053-29d4-4b2e-b8e1-ab502c3d12d3"
      },
      "source": [
        "%tensorboard --logdir /root/ray_results"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 349), started 1:37:16 ago. (Use '!kill 349' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdrJyuKTedVL"
      },
      "source": [
        "### Inspect Training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLlL-Fx37Ipj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ea2054-37c2-4791-c542-772407add26b"
      },
      "source": [
        "# Inspect the results object\n",
        "results"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-09-53',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 323.94,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -101.76692828669167,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2622,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [197,\n",
              "    179,\n",
              "    156,\n",
              "    386,\n",
              "    307,\n",
              "    190,\n",
              "    334,\n",
              "    465,\n",
              "    1000,\n",
              "    301,\n",
              "    243,\n",
              "    116,\n",
              "    213,\n",
              "    1000,\n",
              "    107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199],\n",
              "   'episode_reward': [-96.02617020439425,\n",
              "    -180.14752276482025,\n",
              "    -8.873865814764656,\n",
              "    -71.8158313282949,\n",
              "    -150.85688111916554,\n",
              "    -82.78908495433895,\n",
              "    -154.8645565801819,\n",
              "    -208.5236862124434,\n",
              "    18.80786639818237,\n",
              "    -166.917597270089,\n",
              "    -107.24215609717936,\n",
              "    -183.16535457332463,\n",
              "    -160.6566368903026,\n",
              "    -40.50382427100075,\n",
              "    -168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.823572,\n",
              "    'policy_loss': -32.17304,\n",
              "    'var_gnorm': 27.85028,\n",
              "    'vf_explained_var': -0.010289788,\n",
              "    'vf_loss': 37.879738},\n",
              "   'num_steps_sampled': 332780,\n",
              "   'num_steps_trained': 332780},\n",
              "  'iterations_since_restore': 201,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1898543015800519,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0657001483336535,\n",
              "   'mean_inference_ms': 4.1747088117930895,\n",
              "   'mean_raw_obs_processing_ms': 0.4666965287741674},\n",
              "  'time_since_restore': 989.4311282634735,\n",
              "  'time_this_iter_s': 4.887818098068237,\n",
              "  'time_total_s': 989.4311282634735,\n",
              "  'timers': {'apply_grad_throughput': 1380.068,\n",
              "   'apply_grad_time_ms': 7.246,\n",
              "   'grad_wait_time_ms': 34.101,\n",
              "   'update_time_ms': 10.131},\n",
              "  'timestamp': 1635588593,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 332780,\n",
              "  'training_iteration': 201},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-09-58',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 323.94,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -101.76692828669167,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2622,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [197,\n",
              "    179,\n",
              "    156,\n",
              "    386,\n",
              "    307,\n",
              "    190,\n",
              "    334,\n",
              "    465,\n",
              "    1000,\n",
              "    301,\n",
              "    243,\n",
              "    116,\n",
              "    213,\n",
              "    1000,\n",
              "    107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199],\n",
              "   'episode_reward': [-96.02617020439425,\n",
              "    -180.14752276482025,\n",
              "    -8.873865814764656,\n",
              "    -71.8158313282949,\n",
              "    -150.85688111916554,\n",
              "    -82.78908495433895,\n",
              "    -154.8645565801819,\n",
              "    -208.5236862124434,\n",
              "    18.80786639818237,\n",
              "    -166.917597270089,\n",
              "    -107.24215609717936,\n",
              "    -183.16535457332463,\n",
              "    -160.6566368903026,\n",
              "    -40.50382427100075,\n",
              "    -168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.141695,\n",
              "    'policy_loss': 37.94887,\n",
              "    'var_gnorm': 27.858532,\n",
              "    'vf_explained_var': 0.61381036,\n",
              "    'vf_loss': 61.34048},\n",
              "   'num_steps_sampled': 333630,\n",
              "   'num_steps_trained': 333630},\n",
              "  'iterations_since_restore': 202,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6875, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1898543015800519,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0657001483336535,\n",
              "   'mean_inference_ms': 4.1747088117930895,\n",
              "   'mean_raw_obs_processing_ms': 0.4666965287741674},\n",
              "  'time_since_restore': 994.3864150047302,\n",
              "  'time_this_iter_s': 4.955286741256714,\n",
              "  'time_total_s': 994.3864150047302,\n",
              "  'timers': {'apply_grad_throughput': 1924.522,\n",
              "   'apply_grad_time_ms': 5.196,\n",
              "   'grad_wait_time_ms': 46.366,\n",
              "   'update_time_ms': 6.678},\n",
              "  'timestamp': 1635588598,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 333630,\n",
              "  'training_iteration': 202},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-03',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 337.27,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -98.50939252877602,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 5,\n",
              "  'episodes_total': 2627,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [190,\n",
              "    334,\n",
              "    465,\n",
              "    1000,\n",
              "    301,\n",
              "    243,\n",
              "    116,\n",
              "    213,\n",
              "    1000,\n",
              "    107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208],\n",
              "   'episode_reward': [-82.78908495433895,\n",
              "    -154.8645565801819,\n",
              "    -208.5236862124434,\n",
              "    18.80786639818237,\n",
              "    -166.917597270089,\n",
              "    -107.24215609717936,\n",
              "    -183.16535457332463,\n",
              "    -160.6566368903026,\n",
              "    -40.50382427100075,\n",
              "    -168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.178621,\n",
              "    'policy_loss': -13.265122,\n",
              "    'var_gnorm': 27.87173,\n",
              "    'vf_explained_var': 0.29435146,\n",
              "    'vf_loss': 13.598415},\n",
              "   'num_steps_sampled': 334690,\n",
              "   'num_steps_trained': 334690},\n",
              "  'iterations_since_restore': 203,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19006731453551262,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0743160490492043,\n",
              "   'mean_inference_ms': 4.17650568061296,\n",
              "   'mean_raw_obs_processing_ms': 0.4670853262788269},\n",
              "  'time_since_restore': 999.2594821453094,\n",
              "  'time_this_iter_s': 4.873067140579224,\n",
              "  'time_total_s': 999.2594821453094,\n",
              "  'timers': {'apply_grad_throughput': 1620.812,\n",
              "   'apply_grad_time_ms': 6.17,\n",
              "   'grad_wait_time_ms': 23.584,\n",
              "   'update_time_ms': 8.869},\n",
              "  'timestamp': 1635588603,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 334690,\n",
              "  'training_iteration': 203},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-08',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 329.08,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -96.34114767524008,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 4,\n",
              "  'episodes_total': 2631,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [301,\n",
              "    243,\n",
              "    116,\n",
              "    213,\n",
              "    1000,\n",
              "    107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161],\n",
              "   'episode_reward': [-166.917597270089,\n",
              "    -107.24215609717936,\n",
              "    -183.16535457332463,\n",
              "    -160.6566368903026,\n",
              "    -40.50382427100075,\n",
              "    -168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.972033,\n",
              "    'policy_loss': -76.31173,\n",
              "    'var_gnorm': 27.88456,\n",
              "    'vf_explained_var': 0.43926865,\n",
              "    'vf_loss': 262.06537},\n",
              "   'num_steps_sampled': 335850,\n",
              "   'num_steps_trained': 335850},\n",
              "  'iterations_since_restore': 204,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.99999999999999, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1902966448693639,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0817398654440793,\n",
              "   'mean_inference_ms': 4.177772994939981,\n",
              "   'mean_raw_obs_processing_ms': 0.46743897857269556},\n",
              "  'time_since_restore': 1004.1977741718292,\n",
              "  'time_this_iter_s': 4.938292026519775,\n",
              "  'time_total_s': 1004.1977741718292,\n",
              "  'timers': {'apply_grad_throughput': 1600.177,\n",
              "   'apply_grad_time_ms': 6.249,\n",
              "   'grad_wait_time_ms': 31.479,\n",
              "   'update_time_ms': 9.456},\n",
              "  'timestamp': 1635588608,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 335850,\n",
              "  'training_iteration': 204},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-13',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 330.53,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -95.05536139391278,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2632,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [243,\n",
              "    116,\n",
              "    213,\n",
              "    1000,\n",
              "    107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446],\n",
              "   'episode_reward': [-107.24215609717936,\n",
              "    -183.16535457332463,\n",
              "    -160.6566368903026,\n",
              "    -40.50382427100075,\n",
              "    -168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.042313,\n",
              "    'policy_loss': -10.757744,\n",
              "    'var_gnorm': 27.894926,\n",
              "    'vf_explained_var': 0.42359978,\n",
              "    'vf_loss': 8.537699},\n",
              "   'num_steps_sampled': 336870,\n",
              "   'num_steps_trained': 336870},\n",
              "  'iterations_since_restore': 205,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.25714285714285, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19037994339281009,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0839585682367865,\n",
              "   'mean_inference_ms': 4.177946777222851,\n",
              "   'mean_raw_obs_processing_ms': 0.4675394862874896},\n",
              "  'time_since_restore': 1009.1468510627747,\n",
              "  'time_this_iter_s': 4.949076890945435,\n",
              "  'time_total_s': 1009.1468510627747,\n",
              "  'timers': {'apply_grad_throughput': 1837.769,\n",
              "   'apply_grad_time_ms': 5.441,\n",
              "   'grad_wait_time_ms': 28.689,\n",
              "   'update_time_ms': 7.664},\n",
              "  'timestamp': 1635588613,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 336870,\n",
              "  'training_iteration': 205},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-18',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 336.94,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -95.17525799798867,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2633,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [116,\n",
              "    213,\n",
              "    1000,\n",
              "    107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884],\n",
              "   'episode_reward': [-183.16535457332463,\n",
              "    -160.6566368903026,\n",
              "    -40.50382427100075,\n",
              "    -168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.347379,\n",
              "    'policy_loss': -5.757421,\n",
              "    'var_gnorm': 27.897964,\n",
              "    'vf_explained_var': 0.6283711,\n",
              "    'vf_loss': 4.0541744},\n",
              "   'num_steps_sampled': 337830,\n",
              "   'num_steps_trained': 337830},\n",
              "  'iterations_since_restore': 206,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571427, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19041943241297368,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0857356462655685,\n",
              "   'mean_inference_ms': 4.1784904360661805,\n",
              "   'mean_raw_obs_processing_ms': 0.46764200642840087},\n",
              "  'time_since_restore': 1014.054116487503,\n",
              "  'time_this_iter_s': 4.9072654247283936,\n",
              "  'time_total_s': 1014.054116487503,\n",
              "  'timers': {'apply_grad_throughput': 1777.888,\n",
              "   'apply_grad_time_ms': 5.625,\n",
              "   'grad_wait_time_ms': 35.665,\n",
              "   'update_time_ms': 6.473},\n",
              "  'timestamp': 1635588618,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 337830,\n",
              "  'training_iteration': 206},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-23',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 345.06,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -94.14703432844543,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2635,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445],\n",
              "   'episode_reward': [-40.50382427100075,\n",
              "    -168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.892045,\n",
              "    'policy_loss': -10.264137,\n",
              "    'var_gnorm': 27.91881,\n",
              "    'vf_explained_var': 0.3431543,\n",
              "    'vf_loss': 27.910454},\n",
              "   'num_steps_sampled': 338840,\n",
              "   'num_steps_trained': 338840},\n",
              "  'iterations_since_restore': 207,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.60000000000001, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1905934944838505,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0904204048386075,\n",
              "   'mean_inference_ms': 4.17895667242642,\n",
              "   'mean_raw_obs_processing_ms': 0.46786274901975294},\n",
              "  'time_since_restore': 1018.9697868824005,\n",
              "  'time_this_iter_s': 4.915670394897461,\n",
              "  'time_total_s': 1018.9697868824005,\n",
              "  'timers': {'apply_grad_throughput': 2297.582,\n",
              "   'apply_grad_time_ms': 4.352,\n",
              "   'grad_wait_time_ms': 39.362,\n",
              "   'update_time_ms': 6.864},\n",
              "  'timestamp': 1635588623,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 338840,\n",
              "  'training_iteration': 207},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-28',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 340.06,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -94.51818231551749,\n",
              "  'episode_reward_min': -268.96955955713435,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2636,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [107,\n",
              "    118,\n",
              "    123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500],\n",
              "   'episode_reward': [-168.44723022757714,\n",
              "    -268.96955955713435,\n",
              "    -164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.003532,\n",
              "    'policy_loss': 4.4590273,\n",
              "    'var_gnorm': 27.931597,\n",
              "    'vf_explained_var': 0.21928173,\n",
              "    'vf_loss': 39.69438},\n",
              "   'num_steps_sampled': 339820,\n",
              "   'num_steps_trained': 339820},\n",
              "  'iterations_since_restore': 208,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.85, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19063272094242464,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0919042585542318,\n",
              "   'mean_inference_ms': 4.179589510170984,\n",
              "   'mean_raw_obs_processing_ms': 0.46796254314917624},\n",
              "  'time_since_restore': 1023.9117465019226,\n",
              "  'time_this_iter_s': 4.941959619522095,\n",
              "  'time_total_s': 1023.9117465019226,\n",
              "  'timers': {'apply_grad_throughput': 1860.579,\n",
              "   'apply_grad_time_ms': 5.375,\n",
              "   'grad_wait_time_ms': 36.498,\n",
              "   'update_time_ms': 7.424},\n",
              "  'timestamp': 1635588628,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 339820,\n",
              "  'training_iteration': 208},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-33',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 352.18,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -90.75219064592652,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2638,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [123,\n",
              "    131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437],\n",
              "   'episode_reward': [-164.08371710862946,\n",
              "    -45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 19.997683,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.021148,\n",
              "    'policy_loss': -1.5241661,\n",
              "    'var_gnorm': 27.938713,\n",
              "    'vf_explained_var': 0.4379288,\n",
              "    'vf_loss': 2.601805},\n",
              "   'num_steps_sampled': 340900,\n",
              "   'num_steps_trained': 340900},\n",
              "  'iterations_since_restore': 209,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571427, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1907250326937764,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0960020136210797,\n",
              "   'mean_inference_ms': 4.180877460532774,\n",
              "   'mean_raw_obs_processing_ms': 0.4682037455853284},\n",
              "  'time_since_restore': 1028.8377358913422,\n",
              "  'time_this_iter_s': 4.925989389419556,\n",
              "  'time_total_s': 1028.8377358913422,\n",
              "  'timers': {'apply_grad_throughput': 2436.622,\n",
              "   'apply_grad_time_ms': 4.104,\n",
              "   'grad_wait_time_ms': 27.488,\n",
              "   'update_time_ms': 6.561},\n",
              "  'timestamp': 1635588633,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 340900,\n",
              "  'training_iteration': 209},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-38',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 360.95,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -89.6392941574456,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2639,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [131,\n",
              "    128,\n",
              "    1000,\n",
              "    275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000],\n",
              "   'episode_reward': [-45.39005606277111,\n",
              "    -213.66285643210108,\n",
              "    11.124915503903473,\n",
              "    -122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.521322,\n",
              "    'policy_loss': 41.17157,\n",
              "    'var_gnorm': 27.943415,\n",
              "    'vf_explained_var': 0.33936566,\n",
              "    'vf_loss': 124.94958},\n",
              "   'num_steps_sampled': 341930,\n",
              "   'num_steps_trained': 341930},\n",
              "  'iterations_since_restore': 210,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.57142857142856, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.190817776424468,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.0984661752534601,\n",
              "   'mean_inference_ms': 4.181226211100449,\n",
              "   'mean_raw_obs_processing_ms': 0.4683304813518243},\n",
              "  'time_since_restore': 1033.7695920467377,\n",
              "  'time_this_iter_s': 4.931856155395508,\n",
              "  'time_total_s': 1033.7695920467377,\n",
              "  'timers': {'apply_grad_throughput': 2073.073,\n",
              "   'apply_grad_time_ms': 4.824,\n",
              "   'grad_wait_time_ms': 34.623,\n",
              "   'update_time_ms': 5.816},\n",
              "  'timestamp': 1635588638,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 341930,\n",
              "  'training_iteration': 210},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-43',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 362.3,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -89.68749484086474,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2642,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502],\n",
              "   'episode_reward': [-122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 15.6107855,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.366762,\n",
              "    'policy_loss': -1.3410172,\n",
              "    'var_gnorm': 27.970224,\n",
              "    'vf_explained_var': 0.73574954,\n",
              "    'vf_loss': 13.1129265},\n",
              "   'num_steps_sampled': 343040,\n",
              "   'num_steps_trained': 343040},\n",
              "  'iterations_since_restore': 211,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.4, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19090872905797882,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1034339874099215,\n",
              "   'mean_inference_ms': 4.183733614701308,\n",
              "   'mean_raw_obs_processing_ms': 0.46868707031728607},\n",
              "  'time_since_restore': 1038.6999735832214,\n",
              "  'time_this_iter_s': 4.930381536483765,\n",
              "  'time_total_s': 1038.6999735832214,\n",
              "  'timers': {'apply_grad_throughput': 2128.408,\n",
              "   'apply_grad_time_ms': 4.698,\n",
              "   'grad_wait_time_ms': 32.153,\n",
              "   'update_time_ms': 5.744},\n",
              "  'timestamp': 1635588643,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 343040,\n",
              "  'training_iteration': 211},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-48',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 362.3,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -89.68749484086474,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2642,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [275,\n",
              "    196,\n",
              "    398,\n",
              "    1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502],\n",
              "   'episode_reward': [-122.36702459073548,\n",
              "    -46.52171402849139,\n",
              "    -132.7689008866438,\n",
              "    -27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.42482,\n",
              "    'policy_loss': -22.161459,\n",
              "    'var_gnorm': 27.978275,\n",
              "    'vf_explained_var': 0.17106438,\n",
              "    'vf_loss': 33.988632},\n",
              "   'num_steps_sampled': 344000,\n",
              "   'num_steps_trained': 344000},\n",
              "  'iterations_since_restore': 212,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19090872905797882,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1034339874099215,\n",
              "   'mean_inference_ms': 4.183733614701308,\n",
              "   'mean_raw_obs_processing_ms': 0.46868707031728607},\n",
              "  'time_since_restore': 1043.671828508377,\n",
              "  'time_this_iter_s': 4.97185492515564,\n",
              "  'time_total_s': 1043.671828508377,\n",
              "  'timers': {'apply_grad_throughput': 2403.007,\n",
              "   'apply_grad_time_ms': 4.161,\n",
              "   'grad_wait_time_ms': 47.084,\n",
              "   'update_time_ms': 5.973},\n",
              "  'timestamp': 1635588648,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 344000,\n",
              "  'training_iteration': 212},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-53',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 375.35,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -87.7630422506146,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2645,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174],\n",
              "   'episode_reward': [-27.01305952227152,\n",
              "    -54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.351747,\n",
              "    'policy_loss': 30.086567,\n",
              "    'var_gnorm': 27.975655,\n",
              "    'vf_explained_var': 0.41656053,\n",
              "    'vf_loss': 211.1537},\n",
              "   'num_steps_sampled': 345020,\n",
              "   'num_steps_trained': 345020},\n",
              "  'iterations_since_restore': 213,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.17142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1911516240105707,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1107533851959692,\n",
              "   'mean_inference_ms': 4.185362220809356,\n",
              "   'mean_raw_obs_processing_ms': 0.4691092465798266},\n",
              "  'time_since_restore': 1048.582837343216,\n",
              "  'time_this_iter_s': 4.911008834838867,\n",
              "  'time_total_s': 1048.582837343216,\n",
              "  'timers': {'apply_grad_throughput': 2520.025,\n",
              "   'apply_grad_time_ms': 3.968,\n",
              "   'grad_wait_time_ms': 24.305,\n",
              "   'update_time_ms': 7.942},\n",
              "  'timestamp': 1635588653,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 345020,\n",
              "  'training_iteration': 213},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-10-59',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 367.76,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -88.2659578885888,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2646,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [117,\n",
              "    149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241],\n",
              "   'episode_reward': [-54.678315569750424,\n",
              "    -56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.054185,\n",
              "    'policy_loss': 18.174164,\n",
              "    'var_gnorm': 27.980644,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 113.50086},\n",
              "   'num_steps_sampled': 346000,\n",
              "   'num_steps_trained': 346000},\n",
              "  'iterations_since_restore': 214,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58749999999999, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19120242582360197,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1124151874468826,\n",
              "   'mean_inference_ms': 4.186175398605757,\n",
              "   'mean_raw_obs_processing_ms': 0.4692347761062479},\n",
              "  'time_since_restore': 1053.5532631874084,\n",
              "  'time_this_iter_s': 4.970425844192505,\n",
              "  'time_total_s': 1053.5532631874084,\n",
              "  'timers': {'apply_grad_throughput': 1379.805,\n",
              "   'apply_grad_time_ms': 7.247,\n",
              "   'grad_wait_time_ms': 40.732,\n",
              "   'update_time_ms': 9.101},\n",
              "  'timestamp': 1635588659,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 346000,\n",
              "  'training_iteration': 214},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-04',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 376.59,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -87.94395413762766,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2647,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [149,\n",
              "    264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000],\n",
              "   'episode_reward': [-56.084988542680236,\n",
              "    -95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.884371,\n",
              "    'policy_loss': 1.8889701,\n",
              "    'var_gnorm': 27.984413,\n",
              "    'vf_explained_var': 0.58974636,\n",
              "    'vf_loss': 30.363194},\n",
              "   'num_steps_sampled': 346770,\n",
              "   'num_steps_trained': 346770},\n",
              "  'iterations_since_restore': 215,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.62857142857142, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19120938963059542,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1138635377786503,\n",
              "   'mean_inference_ms': 4.187351179364578,\n",
              "   'mean_raw_obs_processing_ms': 0.46936143878088693},\n",
              "  'time_since_restore': 1058.4783916473389,\n",
              "  'time_this_iter_s': 4.92512845993042,\n",
              "  'time_total_s': 1058.4783916473389,\n",
              "  'timers': {'apply_grad_throughput': 1543.294,\n",
              "   'apply_grad_time_ms': 6.48,\n",
              "   'grad_wait_time_ms': 43.165,\n",
              "   'update_time_ms': 12.653},\n",
              "  'timestamp': 1635588664,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 346770,\n",
              "  'training_iteration': 215},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-09',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 385.1,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -87.47185090462898,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2648,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000],\n",
              "   'episode_reward': [-95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999992,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.772289,\n",
              "    'policy_loss': -11.779152,\n",
              "    'var_gnorm': 27.994127,\n",
              "    'vf_explained_var': 0.82576406,\n",
              "    'vf_loss': 6.0472403},\n",
              "   'num_steps_sampled': 347770,\n",
              "   'num_steps_trained': 347770},\n",
              "  'iterations_since_restore': 216,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19126593680717424,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.11569371122104,\n",
              "   'mean_inference_ms': 4.188231023218067,\n",
              "   'mean_raw_obs_processing_ms': 0.4694986255104477},\n",
              "  'time_since_restore': 1063.4062659740448,\n",
              "  'time_this_iter_s': 4.927874326705933,\n",
              "  'time_total_s': 1063.4062659740448,\n",
              "  'timers': {'apply_grad_throughput': 1932.182,\n",
              "   'apply_grad_time_ms': 5.175,\n",
              "   'grad_wait_time_ms': 40.782,\n",
              "   'update_time_ms': 8.116},\n",
              "  'timestamp': 1635588669,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 347770,\n",
              "  'training_iteration': 216},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-14',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 385.1,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -87.47185090462898,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2648,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [264,\n",
              "    176,\n",
              "    179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000],\n",
              "   'episode_reward': [-95.73361798429568,\n",
              "    -162.60496337464792,\n",
              "    -51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 9.486091,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.070278,\n",
              "    'policy_loss': -0.04974603,\n",
              "    'var_gnorm': 28.00712,\n",
              "    'vf_explained_var': 0.28740966,\n",
              "    'vf_loss': 0.11896149},\n",
              "   'num_steps_sampled': 348560,\n",
              "   'num_steps_trained': 348560},\n",
              "  'iterations_since_restore': 217,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.88571428571427, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19126593680717424,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.11569371122104,\n",
              "   'mean_inference_ms': 4.188231023218067,\n",
              "   'mean_raw_obs_processing_ms': 0.4694986255104477},\n",
              "  'time_since_restore': 1068.4141445159912,\n",
              "  'time_this_iter_s': 5.007878541946411,\n",
              "  'time_total_s': 1068.4141445159912,\n",
              "  'timers': {'apply_grad_throughput': 1895.859,\n",
              "   'apply_grad_time_ms': 5.275,\n",
              "   'grad_wait_time_ms': 46.836,\n",
              "   'update_time_ms': 7.827},\n",
              "  'timestamp': 1635588674,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 348560,\n",
              "  'training_iteration': 217},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-19',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 400.7,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -84.3623944363922,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2650,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [179,\n",
              "    160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000],\n",
              "   'episode_reward': [-51.131064103911,\n",
              "    -172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.879887,\n",
              "    'policy_loss': 9.735903,\n",
              "    'var_gnorm': 28.022017,\n",
              "    'vf_explained_var': -0.96317637,\n",
              "    'vf_loss': 17.560646},\n",
              "   'num_steps_sampled': 349590,\n",
              "   'num_steps_trained': 349590},\n",
              "  'iterations_since_restore': 218,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.97142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19138350598670695,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.120351735597994,\n",
              "   'mean_inference_ms': 4.190095878904461,\n",
              "   'mean_raw_obs_processing_ms': 0.4698091285181784},\n",
              "  'time_since_restore': 1073.3142306804657,\n",
              "  'time_this_iter_s': 4.900086164474487,\n",
              "  'time_total_s': 1073.3142306804657,\n",
              "  'timers': {'apply_grad_throughput': 1507.474,\n",
              "   'apply_grad_time_ms': 6.634,\n",
              "   'grad_wait_time_ms': 27.242,\n",
              "   'update_time_ms': 10.389},\n",
              "  'timestamp': 1635588679,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 349590,\n",
              "  'training_iteration': 218},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-24',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 405.4,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -85.28355529995102,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2651,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [160,\n",
              "    313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649],\n",
              "   'episode_reward': [-172.73141531845448,\n",
              "    -177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 25.762829,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.270292,\n",
              "    'policy_loss': -1.4292564,\n",
              "    'var_gnorm': 28.035473,\n",
              "    'vf_explained_var': 0.56133235,\n",
              "    'vf_loss': 7.4719024},\n",
              "   'num_steps_sampled': 350630,\n",
              "   'num_steps_trained': 350630},\n",
              "  'iterations_since_restore': 219,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1914445319734319,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1230339585775098,\n",
              "   'mean_inference_ms': 4.191051089629975,\n",
              "   'mean_raw_obs_processing_ms': 0.4699763190359155},\n",
              "  'time_since_restore': 1078.2631332874298,\n",
              "  'time_this_iter_s': 4.948902606964111,\n",
              "  'time_total_s': 1078.2631332874298,\n",
              "  'timers': {'apply_grad_throughput': 2416.102,\n",
              "   'apply_grad_time_ms': 4.139,\n",
              "   'grad_wait_time_ms': 36.157,\n",
              "   'update_time_ms': 6.558},\n",
              "  'timestamp': 1635588684,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 350630,\n",
              "  'training_iteration': 219},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-29',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 413.8,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -83.24555657162286,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2652,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [313,\n",
              "    178,\n",
              "    169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000],\n",
              "   'episode_reward': [-177.16294119997502,\n",
              "    -62.81649889602736,\n",
              "    -140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.041192,\n",
              "    'policy_loss': -80.08549,\n",
              "    'var_gnorm': 28.038109,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 458.29968},\n",
              "   'num_steps_sampled': 351460,\n",
              "   'num_steps_trained': 351460},\n",
              "  'iterations_since_restore': 220,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.0375, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19155672611319685,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1261471989070146,\n",
              "   'mean_inference_ms': 4.191723075346911,\n",
              "   'mean_raw_obs_processing_ms': 0.47016010945276826},\n",
              "  'time_since_restore': 1083.2016611099243,\n",
              "  'time_this_iter_s': 4.938527822494507,\n",
              "  'time_total_s': 1083.2016611099243,\n",
              "  'timers': {'apply_grad_throughput': 1703.893,\n",
              "   'apply_grad_time_ms': 5.869,\n",
              "   'grad_wait_time_ms': 30.742,\n",
              "   'update_time_ms': 8.778},\n",
              "  'timestamp': 1635588689,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 351460,\n",
              "  'training_iteration': 220},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-34',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 420.76,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -81.73882265709683,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2654,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [169,\n",
              "    123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187],\n",
              "   'episode_reward': [-140.3143347162282,\n",
              "    -132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.89905,\n",
              "    'policy_loss': 23.657082,\n",
              "    'var_gnorm': 28.043615,\n",
              "    'vf_explained_var': 0.073875785,\n",
              "    'vf_loss': 28.297905},\n",
              "   'num_steps_sampled': 352500,\n",
              "   'num_steps_trained': 352500},\n",
              "  'iterations_since_restore': 221,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19169159545239656,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1312503225511668,\n",
              "   'mean_inference_ms': 4.193789426006346,\n",
              "   'mean_raw_obs_processing_ms': 0.4705085897744982},\n",
              "  'time_since_restore': 1088.1289386749268,\n",
              "  'time_this_iter_s': 4.927277565002441,\n",
              "  'time_total_s': 1088.1289386749268,\n",
              "  'timers': {'apply_grad_throughput': 1238.551,\n",
              "   'apply_grad_time_ms': 8.074,\n",
              "   'grad_wait_time_ms': 32.485,\n",
              "   'update_time_ms': 7.987},\n",
              "  'timestamp': 1635588694,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 352500,\n",
              "  'training_iteration': 221},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-39',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 426.16,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -81.40262599016253,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2655,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [123,\n",
              "    131,\n",
              "    160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709],\n",
              "   'episode_reward': [-132.56826246991716,\n",
              "    -41.48053356504502,\n",
              "    -138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 5.7684803,\n",
              "    'policy_loss': -249.14062,\n",
              "    'var_gnorm': 28.038767,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 5842.6514},\n",
              "   'num_steps_sampled': 353400,\n",
              "   'num_steps_trained': 353400},\n",
              "  'iterations_since_restore': 222,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.7, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19171433883858766,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1332759083736483,\n",
              "   'mean_inference_ms': 4.195182189334008,\n",
              "   'mean_raw_obs_processing_ms': 0.47067674660230696},\n",
              "  'time_since_restore': 1093.0505156517029,\n",
              "  'time_this_iter_s': 4.921576976776123,\n",
              "  'time_total_s': 1093.0505156517029,\n",
              "  'timers': {'apply_grad_throughput': 1419.906,\n",
              "   'apply_grad_time_ms': 7.043,\n",
              "   'grad_wait_time_ms': 46.753,\n",
              "   'update_time_ms': 9.516},\n",
              "  'timestamp': 1635588699,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 353400,\n",
              "  'training_iteration': 222},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-44',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 435.98,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -80.14818813101473,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2657,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [160,\n",
              "    249,\n",
              "    394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000],\n",
              "   'episode_reward': [-138.51089723846366,\n",
              "    -151.35944710574194,\n",
              "    -63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.063541,\n",
              "    'policy_loss': -9.943369,\n",
              "    'var_gnorm': 28.063437,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 100.990486},\n",
              "   'num_steps_sampled': 354450,\n",
              "   'num_steps_trained': 354450},\n",
              "  'iterations_since_restore': 223,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571427, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19181647260380205,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.137685977999701,\n",
              "   'mean_inference_ms': 4.197761258143467,\n",
              "   'mean_raw_obs_processing_ms': 0.4710335329041734},\n",
              "  'time_since_restore': 1098.0151386260986,\n",
              "  'time_this_iter_s': 4.964622974395752,\n",
              "  'time_total_s': 1098.0151386260986,\n",
              "  'timers': {'apply_grad_throughput': 1516.461,\n",
              "   'apply_grad_time_ms': 6.594,\n",
              "   'grad_wait_time_ms': 37.337,\n",
              "   'update_time_ms': 10.237},\n",
              "  'timestamp': 1635588704,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 354450,\n",
              "  'training_iteration': 223},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-49',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 447.46,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -77.33163485840022,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2659,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [394,\n",
              "    139,\n",
              "    200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805],\n",
              "   'episode_reward': [-63.93604124061773,\n",
              "    -195.90661503595487,\n",
              "    -108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.036919,\n",
              "    'policy_loss': -58.000614,\n",
              "    'var_gnorm': 28.0608,\n",
              "    'vf_explained_var': 0.48044908,\n",
              "    'vf_loss': 80.413666},\n",
              "   'num_steps_sampled': 355390,\n",
              "   'num_steps_trained': 355390},\n",
              "  'iterations_since_restore': 224,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.28571428571429, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1919231546801708,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.142234790684907,\n",
              "   'mean_inference_ms': 4.200418542748326,\n",
              "   'mean_raw_obs_processing_ms': 0.47140018603971323},\n",
              "  'time_since_restore': 1102.91339635849,\n",
              "  'time_this_iter_s': 4.898257732391357,\n",
              "  'time_total_s': 1102.91339635849,\n",
              "  'timers': {'apply_grad_throughput': 1803.847,\n",
              "   'apply_grad_time_ms': 5.544,\n",
              "   'grad_wait_time_ms': 37.906,\n",
              "   'update_time_ms': 6.869},\n",
              "  'timestamp': 1635588709,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 355390,\n",
              "  'training_iteration': 224},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-54',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 446.66,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -76.00856518110372,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2661,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [200,\n",
              "    457,\n",
              "    161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301],\n",
              "   'episode_reward': [-108.36031469098756,\n",
              "    -99.46843996018565,\n",
              "    -193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.810672,\n",
              "    'policy_loss': 30.023731,\n",
              "    'var_gnorm': 28.065338,\n",
              "    'vf_explained_var': -0.25481582,\n",
              "    'vf_loss': 60.998066},\n",
              "   'num_steps_sampled': 356420,\n",
              "   'num_steps_trained': 356420},\n",
              "  'iterations_since_restore': 225,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.67142857142856, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19203326172025384,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1473371466082234,\n",
              "   'mean_inference_ms': 4.20315882778058,\n",
              "   'mean_raw_obs_processing_ms': 0.47179531818297493},\n",
              "  'time_since_restore': 1107.8733382225037,\n",
              "  'time_this_iter_s': 4.959941864013672,\n",
              "  'time_total_s': 1107.8733382225037,\n",
              "  'timers': {'apply_grad_throughput': 1738.946,\n",
              "   'apply_grad_time_ms': 5.751,\n",
              "   'grad_wait_time_ms': 34.227,\n",
              "   'update_time_ms': 9.041},\n",
              "  'timestamp': 1635588714,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 356420,\n",
              "  'training_iteration': 225},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-11-59',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 452.35,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -74.86848958943635,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2663,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000],\n",
              "   'episode_reward': [-193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.8981123,\n",
              "    'policy_loss': 50.810883,\n",
              "    'var_gnorm': 28.069887,\n",
              "    'vf_explained_var': -0.77928865,\n",
              "    'vf_loss': 129.297},\n",
              "   'num_steps_sampled': 357410,\n",
              "   'num_steps_trained': 357410},\n",
              "  'iterations_since_restore': 226,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.68571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1921934338318566,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1529192567693618,\n",
              "   'mean_inference_ms': 4.205643026292668,\n",
              "   'mean_raw_obs_processing_ms': 0.47220939177621757},\n",
              "  'time_since_restore': 1112.7674040794373,\n",
              "  'time_this_iter_s': 4.894065856933594,\n",
              "  'time_total_s': 1112.7674040794373,\n",
              "  'timers': {'apply_grad_throughput': 1507.799,\n",
              "   'apply_grad_time_ms': 6.632,\n",
              "   'grad_wait_time_ms': 37.232,\n",
              "   'update_time_ms': 10.067},\n",
              "  'timestamp': 1635588719,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 357410,\n",
              "  'training_iteration': 226},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-04',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 452.35,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -74.86848958943635,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2663,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [161,\n",
              "    125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000],\n",
              "   'episode_reward': [-193.07805673763033,\n",
              "    -186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.8869095,\n",
              "    'policy_loss': -12.4381,\n",
              "    'var_gnorm': 28.081657,\n",
              "    'vf_explained_var': 0.73794484,\n",
              "    'vf_loss': 9.857107},\n",
              "   'num_steps_sampled': 358350,\n",
              "   'num_steps_trained': 358350},\n",
              "  'iterations_since_restore': 227,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.225, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1921934338318566,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1529192567693618,\n",
              "   'mean_inference_ms': 4.205643026292668,\n",
              "   'mean_raw_obs_processing_ms': 0.47220939177621757},\n",
              "  'time_since_restore': 1117.7268810272217,\n",
              "  'time_this_iter_s': 4.959476947784424,\n",
              "  'time_total_s': 1117.7268810272217,\n",
              "  'timers': {'apply_grad_throughput': 1622.147,\n",
              "   'apply_grad_time_ms': 6.165,\n",
              "   'grad_wait_time_ms': 46.347,\n",
              "   'update_time_ms': 9.218},\n",
              "  'timestamp': 1635588724,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 358350,\n",
              "  'training_iteration': 227},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-09',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 460.74,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -73.05987371941464,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2664,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [125,\n",
              "    184,\n",
              "    371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000],\n",
              "   'episode_reward': [-186.58972728465915,\n",
              "    -41.31338574775802,\n",
              "    -61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.207045,\n",
              "    'policy_loss': 11.298743,\n",
              "    'var_gnorm': 28.084375,\n",
              "    'vf_explained_var': 0.42693806,\n",
              "    'vf_loss': 22.058594},\n",
              "   'num_steps_sampled': 359260,\n",
              "   'num_steps_trained': 359260},\n",
              "  'iterations_since_restore': 228,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.07142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19227811793000518,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1561240935296548,\n",
              "   'mean_inference_ms': 4.206927178995991,\n",
              "   'mean_raw_obs_processing_ms': 0.47243414155944213},\n",
              "  'time_since_restore': 1122.7269051074982,\n",
              "  'time_this_iter_s': 5.000024080276489,\n",
              "  'time_total_s': 1122.7269051074982,\n",
              "  'timers': {'apply_grad_throughput': 1524.137,\n",
              "   'apply_grad_time_ms': 6.561,\n",
              "   'grad_wait_time_ms': 42.705,\n",
              "   'update_time_ms': 8.796},\n",
              "  'timestamp': 1635588729,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 359260,\n",
              "  'training_iteration': 228},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-14',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 470.94,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -72.23635718046705,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2666,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [371,\n",
              "    159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329],\n",
              "   'episode_reward': [-61.17612252378707,\n",
              "    -2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.465791,\n",
              "    'policy_loss': 8.382143,\n",
              "    'var_gnorm': 28.082043,\n",
              "    'vf_explained_var': 0.54886615,\n",
              "    'vf_loss': 6.7522044},\n",
              "   'num_steps_sampled': 360280,\n",
              "   'num_steps_trained': 360280},\n",
              "  'iterations_since_restore': 229,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.35714285714285, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19254006780106586,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1634452123736545,\n",
              "   'mean_inference_ms': 4.208881799282334,\n",
              "   'mean_raw_obs_processing_ms': 0.47291022676349614},\n",
              "  'time_since_restore': 1127.615263223648,\n",
              "  'time_this_iter_s': 4.888358116149902,\n",
              "  'time_total_s': 1127.615263223648,\n",
              "  'timers': {'apply_grad_throughput': 1766.186,\n",
              "   'apply_grad_time_ms': 5.662,\n",
              "   'grad_wait_time_ms': 33.223,\n",
              "   'update_time_ms': 7.111},\n",
              "  'timestamp': 1635588734,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 360280,\n",
              "  'training_iteration': 229},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-19',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 477.23,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -71.20425504761137,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2667,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000],\n",
              "   'episode_reward': [-2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.003636,\n",
              "    'policy_loss': 22.440664,\n",
              "    'var_gnorm': 28.082623,\n",
              "    'vf_explained_var': 0.5428821,\n",
              "    'vf_loss': 235.24843},\n",
              "   'num_steps_sampled': 361200,\n",
              "   'num_steps_trained': 361200},\n",
              "  'iterations_since_restore': 230,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571427, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1926307591732094,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1667691469131747,\n",
              "   'mean_inference_ms': 4.210206919119956,\n",
              "   'mean_raw_obs_processing_ms': 0.47314598711971684},\n",
              "  'time_since_restore': 1132.557422876358,\n",
              "  'time_this_iter_s': 4.942159652709961,\n",
              "  'time_total_s': 1132.557422876358,\n",
              "  'timers': {'apply_grad_throughput': 1773.918,\n",
              "   'apply_grad_time_ms': 5.637,\n",
              "   'grad_wait_time_ms': 39.961,\n",
              "   'update_time_ms': 11.792},\n",
              "  'timestamp': 1635588739,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 361200,\n",
              "  'training_iteration': 230},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-24',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 477.23,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -71.20425504761137,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2667,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [159,\n",
              "    1000,\n",
              "    131,\n",
              "    125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000],\n",
              "   'episode_reward': [-2.7897019897524586,\n",
              "    -133.33001187390903,\n",
              "    -97.61814095943143,\n",
              "    -44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.989732,\n",
              "    'policy_loss': -15.73859,\n",
              "    'var_gnorm': 28.074324,\n",
              "    'vf_explained_var': -0.2972319,\n",
              "    'vf_loss': 155.9519},\n",
              "   'num_steps_sampled': 361950,\n",
              "   'num_steps_trained': 361950},\n",
              "  'iterations_since_restore': 231,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.7, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1926307591732094,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1667691469131747,\n",
              "   'mean_inference_ms': 4.210206919119956,\n",
              "   'mean_raw_obs_processing_ms': 0.47314598711971684},\n",
              "  'time_since_restore': 1137.511122226715,\n",
              "  'time_this_iter_s': 4.953699350357056,\n",
              "  'time_total_s': 1137.511122226715,\n",
              "  'timers': {'apply_grad_throughput': 1867.838,\n",
              "   'apply_grad_time_ms': 5.354,\n",
              "   'grad_wait_time_ms': 57.817,\n",
              "   'update_time_ms': 4.894},\n",
              "  'timestamp': 1635588744,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 361950,\n",
              "  'training_iteration': 231},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-29',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 489.74,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -69.50226616693678,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2670,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [125,\n",
              "    720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541],\n",
              "   'episode_reward': [-44.33173677157758,\n",
              "    -192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.848427,\n",
              "    'policy_loss': 6.3970623,\n",
              "    'var_gnorm': 28.071022,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 22.585056},\n",
              "   'num_steps_sampled': 362870,\n",
              "   'num_steps_trained': 362870},\n",
              "  'iterations_since_restore': 232,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.25714285714285, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1929075510496855,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1760869662788054,\n",
              "   'mean_inference_ms': 4.214347182950239,\n",
              "   'mean_raw_obs_processing_ms': 0.4738325683517608},\n",
              "  'time_since_restore': 1142.37779712677,\n",
              "  'time_this_iter_s': 4.866674900054932,\n",
              "  'time_total_s': 1142.37779712677,\n",
              "  'timers': {'apply_grad_throughput': 1985.883,\n",
              "   'apply_grad_time_ms': 5.036,\n",
              "   'grad_wait_time_ms': 33.741,\n",
              "   'update_time_ms': 7.29},\n",
              "  'timestamp': 1635588749,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 362870,\n",
              "  'training_iteration': 232},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-34',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 490.04,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -70.42648005002698,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2671,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [720,\n",
              "    1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155],\n",
              "   'episode_reward': [-192.5086053532168,\n",
              "    -83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.130541,\n",
              "    'policy_loss': 39.799854,\n",
              "    'var_gnorm': 28.07392,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 77.66568},\n",
              "   'num_steps_sampled': 363970,\n",
              "   'num_steps_trained': 363970},\n",
              "  'iterations_since_restore': 233,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.39999999999999, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1929600955317386,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1786697679945977,\n",
              "   'mean_inference_ms': 4.216097250964174,\n",
              "   'mean_raw_obs_processing_ms': 0.4740588334646113},\n",
              "  'time_since_restore': 1147.2965908050537,\n",
              "  'time_this_iter_s': 4.918793678283691,\n",
              "  'time_total_s': 1147.2965908050537,\n",
              "  'timers': {'apply_grad_throughput': 1703.755,\n",
              "   'apply_grad_time_ms': 5.869,\n",
              "   'grad_wait_time_ms': 31.827,\n",
              "   'update_time_ms': 8.313},\n",
              "  'timestamp': 1635588754,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 363970,\n",
              "  'training_iteration': 233},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-39',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 489.5,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 138.58271933703588,\n",
              "  'episode_reward_mean': -69.96338727659055,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2672,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    781,\n",
              "    150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666],\n",
              "   'episode_reward': [-83.01457806318655,\n",
              "    138.58271933703588,\n",
              "    -199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.427707,\n",
              "    'policy_loss': -16.230547,\n",
              "    'var_gnorm': 28.075329,\n",
              "    'vf_explained_var': 0.7479416,\n",
              "    'vf_loss': 45.27149},\n",
              "   'num_steps_sampled': 364930,\n",
              "   'num_steps_trained': 364930},\n",
              "  'iterations_since_restore': 234,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.32499999999999, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19306199891411197,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1822103317871966,\n",
              "   'mean_inference_ms': 4.217561824531029,\n",
              "   'mean_raw_obs_processing_ms': 0.47431270459332076},\n",
              "  'time_since_restore': 1152.2958765029907,\n",
              "  'time_this_iter_s': 4.999285697937012,\n",
              "  'time_total_s': 1152.2958765029907,\n",
              "  'timers': {'apply_grad_throughput': 1909.253,\n",
              "   'apply_grad_time_ms': 5.238,\n",
              "   'grad_wait_time_ms': 44.726,\n",
              "   'update_time_ms': 6.564},\n",
              "  'timestamp': 1635588759,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 364930,\n",
              "  'training_iteration': 234},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-44',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 484.72,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 111.93170741347932,\n",
              "  'episode_reward_mean': -71.6037907910559,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2674,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [150,\n",
              "    143,\n",
              "    96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303],\n",
              "   'episode_reward': [-199.33410830205412,\n",
              "    -201.80220683542979,\n",
              "    -105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.069174,\n",
              "    'policy_loss': 16.825193,\n",
              "    'var_gnorm': 28.081669,\n",
              "    'vf_explained_var': 0.6674876,\n",
              "    'vf_loss': 17.123777},\n",
              "   'num_steps_sampled': 365950,\n",
              "   'num_steps_trained': 365950},\n",
              "  'iterations_since_restore': 235,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19330006590147047,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.1889639704737849,\n",
              "   'mean_inference_ms': 4.220196943012314,\n",
              "   'mean_raw_obs_processing_ms': 0.4748091279110339},\n",
              "  'time_since_restore': 1157.1118335723877,\n",
              "  'time_this_iter_s': 4.815957069396973,\n",
              "  'time_total_s': 1157.1118335723877,\n",
              "  'timers': {'apply_grad_throughput': 1512.175,\n",
              "   'apply_grad_time_ms': 6.613,\n",
              "   'grad_wait_time_ms': 41.463,\n",
              "   'update_time_ms': 10.133},\n",
              "  'timestamp': 1635588764,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 365950,\n",
              "  'training_iteration': 235},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-49',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 499.05,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 111.93170741347932,\n",
              "  'episode_reward_mean': -68.67289491687946,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2676,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [96,\n",
              "    135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726],\n",
              "   'episode_reward': [-105.58737421341795,\n",
              "    -128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.935269,\n",
              "    'policy_loss': 20.537548,\n",
              "    'var_gnorm': 28.082092,\n",
              "    'vf_explained_var': 0.6525836,\n",
              "    'vf_loss': 33.865757},\n",
              "   'num_steps_sampled': 366800,\n",
              "   'num_steps_trained': 366800},\n",
              "  'iterations_since_restore': 236,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.67142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19345913393049685,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.19467664191381,\n",
              "   'mean_inference_ms': 4.22356852974703,\n",
              "   'mean_raw_obs_processing_ms': 0.4752935316871029},\n",
              "  'time_since_restore': 1162.0326805114746,\n",
              "  'time_this_iter_s': 4.920846939086914,\n",
              "  'time_total_s': 1162.0326805114746,\n",
              "  'timers': {'apply_grad_throughput': 1781.634,\n",
              "   'apply_grad_time_ms': 5.613,\n",
              "   'grad_wait_time_ms': 28.601,\n",
              "   'update_time_ms': 9.881},\n",
              "  'timestamp': 1635588769,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 366800,\n",
              "  'training_iteration': 236},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-54',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 502.87,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 111.93170741347932,\n",
              "  'episode_reward_mean': -67.98243090512156,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2677,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [135,\n",
              "    148,\n",
              "    147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478],\n",
              "   'episode_reward': [-128.36758990837046,\n",
              "    -60.42692388606082,\n",
              "    -113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 12.177877,\n",
              "    'policy_loss': -13.136,\n",
              "    'var_gnorm': 28.092333,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 20.291298},\n",
              "   'num_steps_sampled': 367920,\n",
              "   'num_steps_trained': 367920},\n",
              "  'iterations_since_restore': 237,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.25714285714285, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19352172420336422,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.197416820648091,\n",
              "   'mean_inference_ms': 4.225450058181095,\n",
              "   'mean_raw_obs_processing_ms': 0.47553728113850857},\n",
              "  'time_since_restore': 1166.9570035934448,\n",
              "  'time_this_iter_s': 4.924323081970215,\n",
              "  'time_total_s': 1166.9570035934448,\n",
              "  'timers': {'apply_grad_throughput': 1523.584,\n",
              "   'apply_grad_time_ms': 6.563,\n",
              "   'grad_wait_time_ms': 35.123,\n",
              "   'update_time_ms': 8.954},\n",
              "  'timestamp': 1635588774,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 367920,\n",
              "  'training_iteration': 237},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-12-59',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 512.01,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 111.93170741347932,\n",
              "  'episode_reward_mean': -67.76078492045846,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2679,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [147,\n",
              "    357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711],\n",
              "   'episode_reward': [-113.25549253825345,\n",
              "    -226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.303224,\n",
              "    'policy_loss': -47.562904,\n",
              "    'var_gnorm': 28.114588,\n",
              "    'vf_explained_var': 0.2764691,\n",
              "    'vf_loss': 71.71749},\n",
              "   'num_steps_sampled': 368850,\n",
              "   'num_steps_trained': 368850},\n",
              "  'iterations_since_restore': 238,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 99.01428571428572, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19368963064248906,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2032684697853644,\n",
              "   'mean_inference_ms': 4.229031761958556,\n",
              "   'mean_raw_obs_processing_ms': 0.4760411360294896},\n",
              "  'time_since_restore': 1171.9197599887848,\n",
              "  'time_this_iter_s': 4.962756395339966,\n",
              "  'time_total_s': 1171.9197599887848,\n",
              "  'timers': {'apply_grad_throughput': 1645.128,\n",
              "   'apply_grad_time_ms': 6.079,\n",
              "   'grad_wait_time_ms': 30.417,\n",
              "   'update_time_ms': 8.628},\n",
              "  'timestamp': 1635588779,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 368850,\n",
              "  'training_iteration': 238},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-04',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 517.05,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 111.93170741347932,\n",
              "  'episode_reward_mean': -67.829611153791,\n",
              "  'episode_reward_min': -226.16647211854104,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2680,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [357,\n",
              "    211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651],\n",
              "   'episode_reward': [-226.16647211854104,\n",
              "    -32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.079797,\n",
              "    'policy_loss': 26.534613,\n",
              "    'var_gnorm': 28.123207,\n",
              "    'vf_explained_var': 0.20099437,\n",
              "    'vf_loss': 169.6662},\n",
              "   'num_steps_sampled': 369700,\n",
              "   'num_steps_trained': 369700},\n",
              "  'iterations_since_restore': 239,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.97142857142856, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19375710376620014,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2061124842146937,\n",
              "   'mean_inference_ms': 4.231022091478179,\n",
              "   'mean_raw_obs_processing_ms': 0.47629947635197745},\n",
              "  'time_since_restore': 1176.9050102233887,\n",
              "  'time_this_iter_s': 4.985250234603882,\n",
              "  'time_total_s': 1176.9050102233887,\n",
              "  'timers': {'apply_grad_throughput': 1641.144,\n",
              "   'apply_grad_time_ms': 6.093,\n",
              "   'grad_wait_time_ms': 40.794,\n",
              "   'update_time_ms': 7.444},\n",
              "  'timestamp': 1635588784,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 369700,\n",
              "  'training_iteration': 239},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-09',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 523.48,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 119.3102795286251,\n",
              "  'episode_reward_mean': -64.37484363731933,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2681,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [211,\n",
              "    117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000],\n",
              "   'episode_reward': [-32.21102250349951,\n",
              "    -144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.1214485,\n",
              "    'policy_loss': 4.707145,\n",
              "    'var_gnorm': 28.127045,\n",
              "    'vf_explained_var': 0.47473025,\n",
              "    'vf_loss': 2.9135156},\n",
              "   'num_steps_sampled': 370700,\n",
              "   'num_steps_trained': 370700},\n",
              "  'iterations_since_restore': 240,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19386751549294645,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2093136106313986,\n",
              "   'mean_inference_ms': 4.232733327004235,\n",
              "   'mean_raw_obs_processing_ms': 0.47656859959658715},\n",
              "  'time_since_restore': 1181.851812362671,\n",
              "  'time_this_iter_s': 4.946802139282227,\n",
              "  'time_total_s': 1181.851812362671,\n",
              "  'timers': {'apply_grad_throughput': 1420.349,\n",
              "   'apply_grad_time_ms': 7.041,\n",
              "   'grad_wait_time_ms': 34.804,\n",
              "   'update_time_ms': 10.221},\n",
              "  'timestamp': 1635588789,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 370700,\n",
              "  'training_iteration': 240},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-14',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 529.68,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -62.06721357668368,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2682,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [117,\n",
              "    161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831],\n",
              "   'episode_reward': [-144.3479240903039,\n",
              "    -158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.39443,\n",
              "    'policy_loss': 8.612927,\n",
              "    'var_gnorm': 28.11765,\n",
              "    'vf_explained_var': 0.53247947,\n",
              "    'vf_loss': 16.187584},\n",
              "   'num_steps_sampled': 371660,\n",
              "   'num_steps_trained': 371660},\n",
              "  'iterations_since_restore': 241,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6875, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1939421548237837,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2122497972642279,\n",
              "   'mean_inference_ms': 4.234800554757005,\n",
              "   'mean_raw_obs_processing_ms': 0.4768395567953909},\n",
              "  'time_since_restore': 1186.7702457904816,\n",
              "  'time_this_iter_s': 4.918433427810669,\n",
              "  'time_total_s': 1186.7702457904816,\n",
              "  'timers': {'apply_grad_throughput': 1685.407,\n",
              "   'apply_grad_time_ms': 5.933,\n",
              "   'grad_wait_time_ms': 34.262,\n",
              "   'update_time_ms': 9.26},\n",
              "  'timestamp': 1635588794,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 371660,\n",
              "  'training_iteration': 241},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-19',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 538.51,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -60.56022157645266,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2683,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [161,\n",
              "    1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000],\n",
              "   'episode_reward': [-158.60733853719432,\n",
              "    28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.834445,\n",
              "    'policy_loss': -36.986145,\n",
              "    'var_gnorm': 28.119719,\n",
              "    'vf_explained_var': 0.0219751,\n",
              "    'vf_loss': 35.709953},\n",
              "   'num_steps_sampled': 372610,\n",
              "   'num_steps_trained': 372610},\n",
              "  'iterations_since_restore': 242,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1940568168419728,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.215584577409834,\n",
              "   'mean_inference_ms': 4.2365747038186345,\n",
              "   'mean_raw_obs_processing_ms': 0.47711973617136894},\n",
              "  'time_since_restore': 1191.7053112983704,\n",
              "  'time_this_iter_s': 4.935065507888794,\n",
              "  'time_total_s': 1191.7053112983704,\n",
              "  'timers': {'apply_grad_throughput': 1657.5,\n",
              "   'apply_grad_time_ms': 6.033,\n",
              "   'grad_wait_time_ms': 35.262,\n",
              "   'update_time_ms': 9.265},\n",
              "  'timestamp': 1635588799,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 372610,\n",
              "  'training_iteration': 242},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-24',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 546.86,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -57.99804952365896,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2684,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996],\n",
              "   'episode_reward': [28.246263758716296,\n",
              "    -113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.203272,\n",
              "    'policy_loss': -45.870087,\n",
              "    'var_gnorm': 28.128479,\n",
              "    'vf_explained_var': 0.70157397,\n",
              "    'vf_loss': 124.91568},\n",
              "   'num_steps_sampled': 373560,\n",
              "   'num_steps_trained': 373560},\n",
              "  'iterations_since_restore': 243,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.05714285714286, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19413625672838652,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.218626590399498,\n",
              "   'mean_inference_ms': 4.238713486112897,\n",
              "   'mean_raw_obs_processing_ms': 0.47740112007013585},\n",
              "  'time_since_restore': 1196.6411690711975,\n",
              "  'time_this_iter_s': 4.935857772827148,\n",
              "  'time_total_s': 1196.6411690711975,\n",
              "  'timers': {'apply_grad_throughput': 1639.95,\n",
              "   'apply_grad_time_ms': 6.098,\n",
              "   'grad_wait_time_ms': 36.993,\n",
              "   'update_time_ms': 8.009},\n",
              "  'timestamp': 1635588804,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 373560,\n",
              "  'training_iteration': 243},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-29',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 542.81,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -59.35643651399414,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2685,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [149,\n",
              "    112,\n",
              "    317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595],\n",
              "   'episode_reward': [-113.91368982797106,\n",
              "    -113.93236068955505,\n",
              "    -84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.576214,\n",
              "    'policy_loss': 44.72879,\n",
              "    'var_gnorm': 28.122465,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 102.6531},\n",
              "   'num_steps_sampled': 374490,\n",
              "   'num_steps_trained': 374490},\n",
              "  'iterations_since_restore': 244,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1942579831155493,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2224091635412822,\n",
              "   'mean_inference_ms': 4.2405385982895405,\n",
              "   'mean_raw_obs_processing_ms': 0.47769935763237525},\n",
              "  'time_since_restore': 1201.604906320572,\n",
              "  'time_this_iter_s': 4.96373724937439,\n",
              "  'time_total_s': 1201.604906320572,\n",
              "  'timers': {'apply_grad_throughput': 1779.555,\n",
              "   'apply_grad_time_ms': 5.619,\n",
              "   'grad_wait_time_ms': 32.91,\n",
              "   'update_time_ms': 6.035},\n",
              "  'timestamp': 1635588809,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 374490,\n",
              "  'training_iteration': 244},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-34',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 555.06,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -58.262849814970394,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2687,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [317,\n",
              "    105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486],\n",
              "   'episode_reward': [-84.16763973881096,\n",
              "    -131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.460018,\n",
              "    'policy_loss': 3.8555908,\n",
              "    'var_gnorm': 28.121792,\n",
              "    'vf_explained_var': 0.5665819,\n",
              "    'vf_loss': 5.260459},\n",
              "   'num_steps_sampled': 375570,\n",
              "   'num_steps_trained': 375570},\n",
              "  'iterations_since_restore': 245,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.8, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19454414329054281,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2301476254388553,\n",
              "   'mean_inference_ms': 4.243951295036294,\n",
              "   'mean_raw_obs_processing_ms': 0.47830317132638284},\n",
              "  'time_since_restore': 1206.523647069931,\n",
              "  'time_this_iter_s': 4.918740749359131,\n",
              "  'time_total_s': 1206.523647069931,\n",
              "  'timers': {'apply_grad_throughput': 1716.599,\n",
              "   'apply_grad_time_ms': 5.825,\n",
              "   'grad_wait_time_ms': 28.265,\n",
              "   'update_time_ms': 7.835},\n",
              "  'timestamp': 1635588814,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 375570,\n",
              "  'training_iteration': 245},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-39',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 561.89,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -56.89900911695073,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2688,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000],\n",
              "   'episode_reward': [-131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.763891,\n",
              "    'policy_loss': 41.637897,\n",
              "    'var_gnorm': 28.131592,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 79.4943},\n",
              "   'num_steps_sampled': 376420,\n",
              "   'num_steps_trained': 376420},\n",
              "  'iterations_since_restore': 246,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.5, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1946290211280784,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2333835060592673,\n",
              "   'mean_inference_ms': 4.246180865521925,\n",
              "   'mean_raw_obs_processing_ms': 0.47860113449632924},\n",
              "  'time_since_restore': 1211.5121519565582,\n",
              "  'time_this_iter_s': 4.988504886627197,\n",
              "  'time_total_s': 1211.5121519565582,\n",
              "  'timers': {'apply_grad_throughput': 1993.329,\n",
              "   'apply_grad_time_ms': 5.017,\n",
              "   'grad_wait_time_ms': 49.088,\n",
              "   'update_time_ms': 8.779},\n",
              "  'timestamp': 1635588819,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 376420,\n",
              "  'training_iteration': 246},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-45',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 561.89,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -56.89900911695073,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2688,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [105,\n",
              "    346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000],\n",
              "   'episode_reward': [-131.86387414996676,\n",
              "    -84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 3.9670553,\n",
              "    'policy_loss': -1.1624529,\n",
              "    'var_gnorm': 28.129087,\n",
              "    'vf_explained_var': -0.018674612,\n",
              "    'vf_loss': 0.87623316},\n",
              "   'num_steps_sampled': 377230,\n",
              "   'num_steps_trained': 377230},\n",
              "  'iterations_since_restore': 247,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.05, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1946290211280784,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2333835060592673,\n",
              "   'mean_inference_ms': 4.246180865521925,\n",
              "   'mean_raw_obs_processing_ms': 0.47860113449632924},\n",
              "  'time_since_restore': 1216.5722918510437,\n",
              "  'time_this_iter_s': 5.060139894485474,\n",
              "  'time_total_s': 1216.5722918510437,\n",
              "  'timers': {'apply_grad_throughput': 1814.671,\n",
              "   'apply_grad_time_ms': 5.511,\n",
              "   'grad_wait_time_ms': 55.955,\n",
              "   'update_time_ms': 10.861},\n",
              "  'timestamp': 1635588825,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 377230,\n",
              "  'training_iteration': 247},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-49',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 569.28,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 198.55198356006582,\n",
              "  'episode_reward_mean': -54.34419322168852,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2689,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [346,\n",
              "    150,\n",
              "    473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844],\n",
              "   'episode_reward': [-84.54708650614005,\n",
              "    -113.50246538902071,\n",
              "    -97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.452689,\n",
              "    'policy_loss': -14.172531,\n",
              "    'var_gnorm': 28.13075,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 36.347256},\n",
              "   'num_steps_sampled': 378080,\n",
              "   'num_steps_trained': 378080},\n",
              "  'iterations_since_restore': 248,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142856, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19475737745149949,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2371432041135362,\n",
              "   'mean_inference_ms': 4.248148385570209,\n",
              "   'mean_raw_obs_processing_ms': 0.47891120008613264},\n",
              "  'time_since_restore': 1221.4098434448242,\n",
              "  'time_this_iter_s': 4.837551593780518,\n",
              "  'time_total_s': 1221.4098434448242,\n",
              "  'timers': {'apply_grad_throughput': 2195.706,\n",
              "   'apply_grad_time_ms': 4.554,\n",
              "   'grad_wait_time_ms': 43.911,\n",
              "   'update_time_ms': 6.988},\n",
              "  'timestamp': 1635588829,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 378080,\n",
              "  'training_iteration': 248},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-13-55',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 583.23,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 215.85492330560436,\n",
              "  'episode_reward_mean': -49.58678542617943,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2691,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [473,\n",
              "    590,\n",
              "    665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891],\n",
              "   'episode_reward': [-97.55764352261824,\n",
              "    -16.44683180866822,\n",
              "    -158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.848195,\n",
              "    'policy_loss': 13.270457,\n",
              "    'var_gnorm': 28.132572,\n",
              "    'vf_explained_var': 0.4833035,\n",
              "    'vf_loss': 182.09839},\n",
              "   'num_steps_sampled': 379070,\n",
              "   'num_steps_trained': 379070},\n",
              "  'iterations_since_restore': 249,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19501766362327508,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2451420116457474,\n",
              "   'mean_inference_ms': 4.252082293102134,\n",
              "   'mean_raw_obs_processing_ms': 0.4795446288337669},\n",
              "  'time_since_restore': 1226.326110124588,\n",
              "  'time_this_iter_s': 4.916266679763794,\n",
              "  'time_total_s': 1226.326110124588,\n",
              "  'timers': {'apply_grad_throughput': 1917.642,\n",
              "   'apply_grad_time_ms': 5.215,\n",
              "   'grad_wait_time_ms': 29.501,\n",
              "   'update_time_ms': 7.711},\n",
              "  'timestamp': 1635588835,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 379070,\n",
              "  'training_iteration': 249},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-00',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 586.09,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 215.85492330560436,\n",
              "  'episode_reward_mean': -50.07136660691771,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2693,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584],\n",
              "   'episode_reward': [-158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.178605,\n",
              "    'policy_loss': -188.82823,\n",
              "    'var_gnorm': 28.146938,\n",
              "    'vf_explained_var': 0.42158896,\n",
              "    'vf_loss': 1308.6053},\n",
              "   'num_steps_sampled': 380090,\n",
              "   'num_steps_trained': 380090},\n",
              "  'iterations_since_restore': 250,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.68571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1952818298040176,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2532069719051697,\n",
              "   'mean_inference_ms': 4.256080829570378,\n",
              "   'mean_raw_obs_processing_ms': 0.48018658286967847},\n",
              "  'time_since_restore': 1231.2519659996033,\n",
              "  'time_this_iter_s': 4.925855875015259,\n",
              "  'time_total_s': 1231.2519659996033,\n",
              "  'timers': {'apply_grad_throughput': 1496.816,\n",
              "   'apply_grad_time_ms': 6.681,\n",
              "   'grad_wait_time_ms': 33.945,\n",
              "   'update_time_ms': 9.884},\n",
              "  'timestamp': 1635588840,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 380090,\n",
              "  'training_iteration': 250},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-05',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 586.09,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 215.85492330560436,\n",
              "  'episode_reward_mean': -50.07136660691771,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2693,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [665,\n",
              "    106,\n",
              "    610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584],\n",
              "   'episode_reward': [-158.72329634766976,\n",
              "    -190.48983642001525,\n",
              "    -124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.735187,\n",
              "    'policy_loss': 28.175238,\n",
              "    'var_gnorm': 28.149824,\n",
              "    'vf_explained_var': -0.08543384,\n",
              "    'vf_loss': 58.736427},\n",
              "   'num_steps_sampled': 381020,\n",
              "   'num_steps_trained': 381020},\n",
              "  'iterations_since_restore': 251,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.15714285714284, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1952818298040176,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2532069719051697,\n",
              "   'mean_inference_ms': 4.256080829570378,\n",
              "   'mean_raw_obs_processing_ms': 0.48018658286967847},\n",
              "  'time_since_restore': 1236.2386939525604,\n",
              "  'time_this_iter_s': 4.986727952957153,\n",
              "  'time_total_s': 1236.2386939525604,\n",
              "  'timers': {'apply_grad_throughput': 1886.081,\n",
              "   'apply_grad_time_ms': 5.302,\n",
              "   'grad_wait_time_ms': 52.645,\n",
              "   'update_time_ms': 9.642},\n",
              "  'timestamp': 1635588845,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 381020,\n",
              "  'training_iteration': 251},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-10',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 596.2,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 215.85492330560436,\n",
              "  'episode_reward_mean': -47.330748285132394,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2695,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [610,\n",
              "    261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782],\n",
              "   'episode_reward': [-124.32343408992176,\n",
              "    -188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.052927,\n",
              "    'policy_loss': -5.132657,\n",
              "    'var_gnorm': 28.148388,\n",
              "    'vf_explained_var': 0.7037372,\n",
              "    'vf_loss': 5.745429},\n",
              "   'num_steps_sampled': 381780,\n",
              "   'num_steps_trained': 381780},\n",
              "  'iterations_since_restore': 252,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19559200178706754,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2621685762529493,\n",
              "   'mean_inference_ms': 4.2598252330405595,\n",
              "   'mean_raw_obs_processing_ms': 0.4808594050214496},\n",
              "  'time_since_restore': 1241.0857932567596,\n",
              "  'time_this_iter_s': 4.847099304199219,\n",
              "  'time_total_s': 1241.0857932567596,\n",
              "  'timers': {'apply_grad_throughput': 2042.813,\n",
              "   'apply_grad_time_ms': 4.895,\n",
              "   'grad_wait_time_ms': 42.005,\n",
              "   'update_time_ms': 8.886},\n",
              "  'timestamp': 1635588850,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 381780,\n",
              "  'training_iteration': 252},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-15',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 592.41,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 215.85492330560436,\n",
              "  'episode_reward_mean': -47.19365582195743,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2696,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231],\n",
              "   'episode_reward': [-188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.698759,\n",
              "    'policy_loss': 25.025179,\n",
              "    'var_gnorm': 28.160482,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 61.54142},\n",
              "   'num_steps_sampled': 382850,\n",
              "   'num_steps_trained': 382850},\n",
              "  'iterations_since_restore': 253,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.475, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19568888812229615,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2657805996334006,\n",
              "   'mean_inference_ms': 4.262246362953711,\n",
              "   'mean_raw_obs_processing_ms': 0.48118783569853785},\n",
              "  'time_since_restore': 1246.1463022232056,\n",
              "  'time_this_iter_s': 5.060508966445923,\n",
              "  'time_total_s': 1246.1463022232056,\n",
              "  'timers': {'apply_grad_throughput': 1743.638,\n",
              "   'apply_grad_time_ms': 5.735,\n",
              "   'grad_wait_time_ms': 39.712,\n",
              "   'update_time_ms': 9.761},\n",
              "  'timestamp': 1635588855,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 382850,\n",
              "  'training_iteration': 253},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-20',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 592.41,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 215.85492330560436,\n",
              "  'episode_reward_mean': -47.19365582195743,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2696,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [261,\n",
              "    221,\n",
              "    258,\n",
              "    204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231],\n",
              "   'episode_reward': [-188.69652045137514,\n",
              "    -90.66097014671669,\n",
              "    -174.89078887544844,\n",
              "    -57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 3.7665184,\n",
              "    'policy_loss': -0.25396162,\n",
              "    'var_gnorm': 28.141537,\n",
              "    'vf_explained_var': -0.019398093,\n",
              "    'vf_loss': 0.50083846},\n",
              "   'num_steps_sampled': 383600,\n",
              "   'num_steps_trained': 383600},\n",
              "  'iterations_since_restore': 254,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.28571428571429, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19568888812229615,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2657805996334006,\n",
              "   'mean_inference_ms': 4.262246362953711,\n",
              "   'mean_raw_obs_processing_ms': 0.48118783569853785},\n",
              "  'time_since_restore': 1251.092687368393,\n",
              "  'time_this_iter_s': 4.946385145187378,\n",
              "  'time_total_s': 1251.092687368393,\n",
              "  'timers': {'apply_grad_throughput': 1328.602,\n",
              "   'apply_grad_time_ms': 7.527,\n",
              "   'grad_wait_time_ms': 58.179,\n",
              "   'update_time_ms': 7.093},\n",
              "  'timestamp': 1635588860,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 383600,\n",
              "  'training_iteration': 254},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-25',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 606.99,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 215.85492330560436,\n",
              "  'episode_reward_mean': -40.90389469857099,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2699,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [204,\n",
              "    635,\n",
              "    118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000],\n",
              "   'episode_reward': [-57.11871115766316,\n",
              "    102.03154427381617,\n",
              "    -143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.163353,\n",
              "    'policy_loss': -23.713707,\n",
              "    'var_gnorm': 28.15882,\n",
              "    'vf_explained_var': 0.32118976,\n",
              "    'vf_loss': 26.539312},\n",
              "   'num_steps_sampled': 384650,\n",
              "   'num_steps_trained': 384650},\n",
              "  'iterations_since_restore': 255,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.07142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19602663508919932,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.277438310106885,\n",
              "   'mean_inference_ms': 4.269258999780962,\n",
              "   'mean_raw_obs_processing_ms': 0.4821989551974508},\n",
              "  'time_since_restore': 1255.9805510044098,\n",
              "  'time_this_iter_s': 4.887863636016846,\n",
              "  'time_total_s': 1255.9805510044098,\n",
              "  'timers': {'apply_grad_throughput': 2179.255,\n",
              "   'apply_grad_time_ms': 4.589,\n",
              "   'grad_wait_time_ms': 31.715,\n",
              "   'update_time_ms': 9.549},\n",
              "  'timestamp': 1635588865,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 384650,\n",
              "  'training_iteration': 255},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-30',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 611.56,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -36.94187783057196,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2701,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [118,\n",
              "    433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666],\n",
              "   'episode_reward': [-143.08427191021642,\n",
              "    -12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.150496,\n",
              "    'policy_loss': 16.966413,\n",
              "    'var_gnorm': 28.1782,\n",
              "    'vf_explained_var': -0.038846135,\n",
              "    'vf_loss': 49.953075},\n",
              "   'num_steps_sampled': 385630,\n",
              "   'num_steps_trained': 385630},\n",
              "  'iterations_since_restore': 256,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.07142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1963049908052579,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2860366427149779,\n",
              "   'mean_inference_ms': 4.273460972642344,\n",
              "   'mean_raw_obs_processing_ms': 0.48288106329206243},\n",
              "  'time_since_restore': 1260.898591518402,\n",
              "  'time_this_iter_s': 4.91804051399231,\n",
              "  'time_total_s': 1260.898591518402,\n",
              "  'timers': {'apply_grad_throughput': 2025.304,\n",
              "   'apply_grad_time_ms': 4.938,\n",
              "   'grad_wait_time_ms': 24.963,\n",
              "   'update_time_ms': 8.149},\n",
              "  'timestamp': 1635588870,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 385630,\n",
              "  'training_iteration': 256},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-35',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 612.56,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -35.73147981882944,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2702,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [433,\n",
              "    1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218],\n",
              "   'episode_reward': [-12.382923743066371,\n",
              "    21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.034849,\n",
              "    'policy_loss': -46.89806,\n",
              "    'var_gnorm': 28.186045,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 336.3207},\n",
              "   'num_steps_sampled': 386640,\n",
              "   'num_steps_trained': 386640},\n",
              "  'iterations_since_restore': 257,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.37142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19644256587199485,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.290287534206366,\n",
              "   'mean_inference_ms': 4.275623118774892,\n",
              "   'mean_raw_obs_processing_ms': 0.4832210609544849},\n",
              "  'time_since_restore': 1265.9128379821777,\n",
              "  'time_this_iter_s': 5.014246463775635,\n",
              "  'time_total_s': 1265.9128379821777,\n",
              "  'timers': {'apply_grad_throughput': 1625.819,\n",
              "   'apply_grad_time_ms': 6.151,\n",
              "   'grad_wait_time_ms': 39.611,\n",
              "   'update_time_ms': 7.748},\n",
              "  'timestamp': 1635588875,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 386640,\n",
              "  'training_iteration': 257},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-40',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 617.31,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -34.3784293486333,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2703,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908],\n",
              "   'episode_reward': [21.922316244042616,\n",
              "    -54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.998678,\n",
              "    'policy_loss': -9.736861,\n",
              "    'var_gnorm': 28.18301,\n",
              "    'vf_explained_var': 0.021713436,\n",
              "    'vf_loss': 16.398039},\n",
              "   'num_steps_sampled': 387490,\n",
              "   'num_steps_trained': 387490},\n",
              "  'iterations_since_restore': 258,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.05, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1965833647957721,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2947006082160653,\n",
              "   'mean_inference_ms': 4.277712488004286,\n",
              "   'mean_raw_obs_processing_ms': 0.4835671509167988},\n",
              "  'time_since_restore': 1270.8677971363068,\n",
              "  'time_this_iter_s': 4.954959154129028,\n",
              "  'time_total_s': 1270.8677971363068,\n",
              "  'timers': {'apply_grad_throughput': 1769.554,\n",
              "   'apply_grad_time_ms': 5.651,\n",
              "   'grad_wait_time_ms': 50.105,\n",
              "   'update_time_ms': 5.963},\n",
              "  'timestamp': 1635588880,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 387490,\n",
              "  'training_iteration': 258},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-45',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 617.31,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -35.28105951150512,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2704,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [167,\n",
              "    108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000],\n",
              "   'episode_reward': [-54.93678686174444,\n",
              "    -122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.946251,\n",
              "    'policy_loss': -23.031176,\n",
              "    'var_gnorm': 28.191208,\n",
              "    'vf_explained_var': 0.79234123,\n",
              "    'vf_loss': 39.87618},\n",
              "   'num_steps_sampled': 388460,\n",
              "   'num_steps_trained': 388460},\n",
              "  'iterations_since_restore': 259,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.37142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19671913908338134,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.2989171383191476,\n",
              "   'mean_inference_ms': 4.279869410957808,\n",
              "   'mean_raw_obs_processing_ms': 0.48390259567923605},\n",
              "  'time_since_restore': 1275.838314294815,\n",
              "  'time_this_iter_s': 4.970517158508301,\n",
              "  'time_total_s': 1275.838314294815,\n",
              "  'timers': {'apply_grad_throughput': 1170.472,\n",
              "   'apply_grad_time_ms': 8.544,\n",
              "   'grad_wait_time_ms': 35.352,\n",
              "   'update_time_ms': 9.707},\n",
              "  'timestamp': 1635588885,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 388460,\n",
              "  'training_iteration': 259},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-50',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 621.11,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -35.54933363003383,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2705,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [108,\n",
              "    94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547],\n",
              "   'episode_reward': [-122.60227026983631,\n",
              "    -99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.113363,\n",
              "    'policy_loss': 10.794708,\n",
              "    'var_gnorm': 28.196459,\n",
              "    'vf_explained_var': 0.69099796,\n",
              "    'vf_loss': 8.147291},\n",
              "   'num_steps_sampled': 389400,\n",
              "   'num_steps_trained': 389400},\n",
              "  'iterations_since_restore': 260,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.05714285714286, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1968566280815065,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3031596709462583,\n",
              "   'mean_inference_ms': 4.282054531653012,\n",
              "   'mean_raw_obs_processing_ms': 0.48424056535093063},\n",
              "  'time_since_restore': 1280.849370956421,\n",
              "  'time_this_iter_s': 5.011056661605835,\n",
              "  'time_total_s': 1280.849370956421,\n",
              "  'timers': {'apply_grad_throughput': 1735.493,\n",
              "   'apply_grad_time_ms': 5.762,\n",
              "   'grad_wait_time_ms': 46.446,\n",
              "   'update_time_ms': 10.915},\n",
              "  'timestamp': 1635588890,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 389400,\n",
              "  'training_iteration': 260},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-14-55',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 630.03,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -35.42902640617788,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2706,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [94,\n",
              "    1000,\n",
              "    281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000],\n",
              "   'episode_reward': [-99.19939103477957,\n",
              "    -21.100087089710343,\n",
              "    -63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 11.30633,\n",
              "    'policy_loss': 60.904358,\n",
              "    'var_gnorm': 28.20979,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 276.22565},\n",
              "   'num_steps_sampled': 390260,\n",
              "   'num_steps_trained': 390260},\n",
              "  'iterations_since_restore': 261,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19695999690979057,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3071056426881662,\n",
              "   'mean_inference_ms': 4.284563643251076,\n",
              "   'mean_raw_obs_processing_ms': 0.48458658301776786},\n",
              "  'time_since_restore': 1285.7696058750153,\n",
              "  'time_this_iter_s': 4.92023491859436,\n",
              "  'time_total_s': 1285.7696058750153,\n",
              "  'timers': {'apply_grad_throughput': 2044.925,\n",
              "   'apply_grad_time_ms': 4.89,\n",
              "   'grad_wait_time_ms': 39.694,\n",
              "   'update_time_ms': 10.599},\n",
              "  'timestamp': 1635588895,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 390260,\n",
              "  'training_iteration': 261},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-00',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 635.1,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -35.475872269204814,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2708,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000],\n",
              "   'episode_reward': [-63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.986184,\n",
              "    'policy_loss': -42.802258,\n",
              "    'var_gnorm': 28.21089,\n",
              "    'vf_explained_var': 0.6355003,\n",
              "    'vf_loss': 98.27928},\n",
              "   'num_steps_sampled': 391220,\n",
              "   'num_steps_trained': 391220},\n",
              "  'iterations_since_restore': 262,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.60000000000001, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.197248378542093,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3160143472512658,\n",
              "   'mean_inference_ms': 4.288944288129668,\n",
              "   'mean_raw_obs_processing_ms': 0.4852842076255471},\n",
              "  'time_since_restore': 1290.6626348495483,\n",
              "  'time_this_iter_s': 4.893028974533081,\n",
              "  'time_total_s': 1290.6626348495483,\n",
              "  'timers': {'apply_grad_throughput': 1664.875,\n",
              "   'apply_grad_time_ms': 6.006,\n",
              "   'grad_wait_time_ms': 34.528,\n",
              "   'update_time_ms': 8.754},\n",
              "  'timestamp': 1635588900,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 391220,\n",
              "  'training_iteration': 262},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-05',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 635.1,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -35.475872269204814,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2708,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [281,\n",
              "    239,\n",
              "    234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000],\n",
              "   'episode_reward': [-63.848976744648155,\n",
              "    -150.64313914862055,\n",
              "    -37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999992,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.458588,\n",
              "    'policy_loss': -13.023729,\n",
              "    'var_gnorm': 28.20534,\n",
              "    'vf_explained_var': 0.6265913,\n",
              "    'vf_loss': 8.376121},\n",
              "   'num_steps_sampled': 392120,\n",
              "   'num_steps_trained': 392120},\n",
              "  'iterations_since_restore': 263,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.67142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.197248378542093,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3160143472512658,\n",
              "   'mean_inference_ms': 4.288944288129668,\n",
              "   'mean_raw_obs_processing_ms': 0.4852842076255471},\n",
              "  'time_since_restore': 1295.6724801063538,\n",
              "  'time_this_iter_s': 5.00984525680542,\n",
              "  'time_total_s': 1295.6724801063538,\n",
              "  'timers': {'apply_grad_throughput': 1530.388,\n",
              "   'apply_grad_time_ms': 6.534,\n",
              "   'grad_wait_time_ms': 50.744,\n",
              "   'update_time_ms': 10.394},\n",
              "  'timestamp': 1635588905,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 392120,\n",
              "  'training_iteration': 263},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-10',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 649.9,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -34.435760772614714,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2710,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [234,\n",
              "    181,\n",
              "    210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000],\n",
              "   'episode_reward': [-37.17380148389394,\n",
              "    -162.18816641164227,\n",
              "    -173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.039886,\n",
              "    'policy_loss': 33.485985,\n",
              "    'var_gnorm': 28.20975,\n",
              "    'vf_explained_var': 0.8472958,\n",
              "    'vf_loss': 64.6187},\n",
              "   'num_steps_sampled': 392960,\n",
              "   'num_steps_trained': 392960},\n",
              "  'iterations_since_restore': 264,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.32499999999999, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19758326666422754,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3257836375921577,\n",
              "   'mean_inference_ms': 4.293007152631515,\n",
              "   'mean_raw_obs_processing_ms': 0.48599919177432993},\n",
              "  'time_since_restore': 1300.6318154335022,\n",
              "  'time_this_iter_s': 4.9593353271484375,\n",
              "  'time_total_s': 1300.6318154335022,\n",
              "  'timers': {'apply_grad_throughput': 1589.065,\n",
              "   'apply_grad_time_ms': 6.293,\n",
              "   'grad_wait_time_ms': 43.798,\n",
              "   'update_time_ms': 8.407},\n",
              "  'timestamp': 1635588910,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 392960,\n",
              "  'training_iteration': 264},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-15',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 655.63,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -31.491334414158203,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2712,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [210,\n",
              "    196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471],\n",
              "   'episode_reward': [-173.514263239738,\n",
              "    -74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.768335,\n",
              "    'policy_loss': 7.1969123,\n",
              "    'var_gnorm': 28.204477,\n",
              "    'vf_explained_var': 0.5904517,\n",
              "    'vf_loss': 9.984644},\n",
              "   'num_steps_sampled': 394080,\n",
              "   'num_steps_trained': 394080},\n",
              "  'iterations_since_restore': 265,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.37142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19783708554708632,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3344873281224339,\n",
              "   'mean_inference_ms': 4.2978441065647335,\n",
              "   'mean_raw_obs_processing_ms': 0.4867110199266722},\n",
              "  'time_since_restore': 1305.4994475841522,\n",
              "  'time_this_iter_s': 4.867632150650024,\n",
              "  'time_total_s': 1305.4994475841522,\n",
              "  'timers': {'apply_grad_throughput': 1974.887,\n",
              "   'apply_grad_time_ms': 5.064,\n",
              "   'grad_wait_time_ms': 19.622,\n",
              "   'update_time_ms': 9.513},\n",
              "  'timestamp': 1635588915,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 394080,\n",
              "  'training_iteration': 265},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-20',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 656.35,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -31.08102831562358,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2713,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [196,\n",
              "    601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282],\n",
              "   'episode_reward': [-74.11551328370665,\n",
              "    -147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 34.915073,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.025881,\n",
              "    'policy_loss': -3.5953717,\n",
              "    'var_gnorm': 28.207094,\n",
              "    'vf_explained_var': 0.77657545,\n",
              "    'vf_loss': 3.0441182},\n",
              "   'num_steps_sampled': 395130,\n",
              "   'num_steps_trained': 395130},\n",
              "  'iterations_since_restore': 266,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58571428571427, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19795098113051474,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.338617840677481,\n",
              "   'mean_inference_ms': 4.300440660357665,\n",
              "   'mean_raw_obs_processing_ms': 0.4870743719929166},\n",
              "  'time_since_restore': 1310.5019626617432,\n",
              "  'time_this_iter_s': 5.002515077590942,\n",
              "  'time_total_s': 1310.5019626617432,\n",
              "  'timers': {'apply_grad_throughput': 1748.217,\n",
              "   'apply_grad_time_ms': 5.72,\n",
              "   'grad_wait_time_ms': 39.529,\n",
              "   'update_time_ms': 5.385},\n",
              "  'timestamp': 1635588920,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 395130,\n",
              "  'training_iteration': 266},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-25',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 664.16,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -28.56966476002533,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2714,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [601,\n",
              "    1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977],\n",
              "   'episode_reward': [-147.89663928394415,\n",
              "    18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 30.710136,\n",
              "    'model': {},\n",
              "    'policy_entropy': 1.7267424,\n",
              "    'policy_loss': -0.042405624,\n",
              "    'var_gnorm': 28.21027,\n",
              "    'vf_explained_var': 0.37166715,\n",
              "    'vf_loss': 0.11169431},\n",
              "   'num_steps_sampled': 395980,\n",
              "   'num_steps_trained': 395980},\n",
              "  'iterations_since_restore': 267,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.95714285714284, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19813578962627518,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3438460509746568,\n",
              "   'mean_inference_ms': 4.302350291763041,\n",
              "   'mean_raw_obs_processing_ms': 0.4874386349010505},\n",
              "  'time_since_restore': 1315.46999168396,\n",
              "  'time_this_iter_s': 4.968029022216797,\n",
              "  'time_total_s': 1315.46999168396,\n",
              "  'timers': {'apply_grad_throughput': 1750.873,\n",
              "   'apply_grad_time_ms': 5.711,\n",
              "   'grad_wait_time_ms': 32.79,\n",
              "   'update_time_ms': 9.789},\n",
              "  'timestamp': 1635588925,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 395980,\n",
              "  'training_iteration': 267},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-30',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 667.58,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -25.56044717592506,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2715,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    155,\n",
              "    109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943],\n",
              "   'episode_reward': [18.232177015493107,\n",
              "    28.824803106812283,\n",
              "    -107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.665739,\n",
              "    'policy_loss': 1.1635473,\n",
              "    'var_gnorm': 28.237688,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 14.211283},\n",
              "   'num_steps_sampled': 396890,\n",
              "   'num_steps_trained': 396890},\n",
              "  'iterations_since_restore': 268,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571428, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1982540642894706,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3480364040048025,\n",
              "   'mean_inference_ms': 4.3049657818150475,\n",
              "   'mean_raw_obs_processing_ms': 0.48780667794005383},\n",
              "  'time_since_restore': 1320.410409450531,\n",
              "  'time_this_iter_s': 4.940417766571045,\n",
              "  'time_total_s': 1320.410409450531,\n",
              "  'timers': {'apply_grad_throughput': 2343.671,\n",
              "   'apply_grad_time_ms': 4.267,\n",
              "   'grad_wait_time_ms': 41.346,\n",
              "   'update_time_ms': 9.245},\n",
              "  'timestamp': 1635588930,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 396890,\n",
              "  'training_iteration': 268},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-35',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 668.89,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -22.57086475953367,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2717,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [109,\n",
              "    130,\n",
              "    1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769],\n",
              "   'episode_reward': [-107.38026576165295,\n",
              "    -165.55527485168759,\n",
              "    -56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.379068,\n",
              "    'policy_loss': -0.16799402,\n",
              "    'var_gnorm': 28.248781,\n",
              "    'vf_explained_var': 0.19075811,\n",
              "    'vf_loss': 11.882934},\n",
              "   'num_steps_sampled': 397870,\n",
              "   'num_steps_trained': 397870},\n",
              "  'iterations_since_restore': 269,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.48571428571427, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1985961577777244,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3579672817531436,\n",
              "   'mean_inference_ms': 4.309109370073633,\n",
              "   'mean_raw_obs_processing_ms': 0.4885387782792154},\n",
              "  'time_since_restore': 1325.34113407135,\n",
              "  'time_this_iter_s': 4.930724620819092,\n",
              "  'time_total_s': 1325.34113407135,\n",
              "  'timers': {'apply_grad_throughput': 1912.509,\n",
              "   'apply_grad_time_ms': 5.229,\n",
              "   'grad_wait_time_ms': 38.277,\n",
              "   'update_time_ms': 8.904},\n",
              "  'timestamp': 1635588935,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 397870,\n",
              "  'training_iteration': 269},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-40',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 679.93,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -19.300690581225293,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2719,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623],\n",
              "   'episode_reward': [-56.13149731121705,\n",
              "    -24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.479915,\n",
              "    'policy_loss': -30.65421,\n",
              "    'var_gnorm': 28.234554,\n",
              "    'vf_explained_var': 0.5026687,\n",
              "    'vf_loss': 67.131805},\n",
              "   'num_steps_sampled': 398790,\n",
              "   'num_steps_trained': 398790},\n",
              "  'iterations_since_restore': 270,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.5, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1989422819855474,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3680027178201106,\n",
              "   'mean_inference_ms': 4.313323019847474,\n",
              "   'mean_raw_obs_processing_ms': 0.4892817346061954},\n",
              "  'time_since_restore': 1330.2370073795319,\n",
              "  'time_this_iter_s': 4.895873308181763,\n",
              "  'time_total_s': 1330.2370073795319,\n",
              "  'timers': {'apply_grad_throughput': 1591.905,\n",
              "   'apply_grad_time_ms': 6.282,\n",
              "   'grad_wait_time_ms': 28.741,\n",
              "   'update_time_ms': 10.677},\n",
              "  'timestamp': 1635588940,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 398790,\n",
              "  'training_iteration': 270},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-45',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 675.06,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -20.204104626030208,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2720,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [194,\n",
              "    199,\n",
              "    1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513],\n",
              "   'episode_reward': [-24.889229611786817,\n",
              "    -74.67528939614448,\n",
              "    42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.800157,\n",
              "    'policy_loss': 13.206187,\n",
              "    'var_gnorm': 28.226171,\n",
              "    'vf_explained_var': 0.40742362,\n",
              "    'vf_loss': 44.463783},\n",
              "   'num_steps_sampled': 399710,\n",
              "   'num_steps_trained': 399710},\n",
              "  'iterations_since_restore': 271,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142856, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.1990893944895871,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3726865973937752,\n",
              "   'mean_inference_ms': 4.31565585534748,\n",
              "   'mean_raw_obs_processing_ms': 0.48964614563061104},\n",
              "  'time_since_restore': 1335.178228378296,\n",
              "  'time_this_iter_s': 4.941220998764038,\n",
              "  'time_total_s': 1335.178228378296,\n",
              "  'timers': {'apply_grad_throughput': 1685.915,\n",
              "   'apply_grad_time_ms': 5.931,\n",
              "   'grad_wait_time_ms': 38.873,\n",
              "   'update_time_ms': 9.357},\n",
              "  'timestamp': 1635588945,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 399710,\n",
              "  'training_iteration': 271},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-50',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 684.44,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -15.456196688891273,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2722,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531],\n",
              "   'episode_reward': [42.662503181978686,\n",
              "    -36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.143639,\n",
              "    'policy_loss': 4.10796,\n",
              "    'var_gnorm': 28.242628,\n",
              "    'vf_explained_var': 0.8694059,\n",
              "    'vf_loss': 5.4657016},\n",
              "   'num_steps_sampled': 400680,\n",
              "   'num_steps_trained': 400680},\n",
              "  'iterations_since_restore': 272,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.95714285714287, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19939426814112549,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3822284956004902,\n",
              "   'mean_inference_ms': 4.3202230750301815,\n",
              "   'mean_raw_obs_processing_ms': 0.4903804606367559},\n",
              "  'time_since_restore': 1340.101791381836,\n",
              "  'time_this_iter_s': 4.923563003540039,\n",
              "  'time_total_s': 1340.101791381836,\n",
              "  'timers': {'apply_grad_throughput': 1812.076,\n",
              "   'apply_grad_time_ms': 5.519,\n",
              "   'grad_wait_time_ms': 26.864,\n",
              "   'update_time_ms': 10.121},\n",
              "  'timestamp': 1635588950,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 400680,\n",
              "  'training_iteration': 272},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-15-55',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 679.7,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -16.86696889188479,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2723,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526],\n",
              "   'episode_reward': [-36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.372769,\n",
              "    'policy_loss': 20.521753,\n",
              "    'var_gnorm': 28.246109,\n",
              "    'vf_explained_var': 0.73638785,\n",
              "    'vf_loss': 71.42683},\n",
              "   'num_steps_sampled': 401710,\n",
              "   'num_steps_trained': 401710},\n",
              "  'iterations_since_restore': 273,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.35714285714286, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19955268468779455,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3869374946651931,\n",
              "   'mean_inference_ms': 4.322455798536276,\n",
              "   'mean_raw_obs_processing_ms': 0.49075174771876534},\n",
              "  'time_since_restore': 1345.0619628429413,\n",
              "  'time_this_iter_s': 4.960171461105347,\n",
              "  'time_total_s': 1345.0619628429413,\n",
              "  'timers': {'apply_grad_throughput': 2003.709,\n",
              "   'apply_grad_time_ms': 4.991,\n",
              "   'grad_wait_time_ms': 40.827,\n",
              "   'update_time_ms': 7.284},\n",
              "  'timestamp': 1635588955,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 401710,\n",
              "  'training_iteration': 273},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-00',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 679.7,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -16.86696889188479,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2723,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [165,\n",
              "    185,\n",
              "    1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526],\n",
              "   'episode_reward': [-36.996803633852295,\n",
              "    -87.84294349642461,\n",
              "    47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 6.5200753,\n",
              "    'policy_loss': 3.4651673,\n",
              "    'var_gnorm': 28.242876,\n",
              "    'vf_explained_var': 0.300955,\n",
              "    'vf_loss': 32.439507},\n",
              "   'num_steps_sampled': 402580,\n",
              "   'num_steps_trained': 402580},\n",
              "  'iterations_since_restore': 274,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571429, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19955268468779455,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.3869374946651931,\n",
              "   'mean_inference_ms': 4.322455798536276,\n",
              "   'mean_raw_obs_processing_ms': 0.49075174771876534},\n",
              "  'time_since_restore': 1350.0152373313904,\n",
              "  'time_this_iter_s': 4.953274488449097,\n",
              "  'time_total_s': 1350.0152373313904,\n",
              "  'timers': {'apply_grad_throughput': 1964.794,\n",
              "   'apply_grad_time_ms': 5.09,\n",
              "   'grad_wait_time_ms': 45.176,\n",
              "   'update_time_ms': 7.34},\n",
              "  'timestamp': 1635588960,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 402580,\n",
              "  'training_iteration': 274},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-05',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 695.41,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -13.99849991760243,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2725,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000],\n",
              "   'episode_reward': [47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.648315,\n",
              "    'policy_loss': -31.578339,\n",
              "    'var_gnorm': 28.256454,\n",
              "    'vf_explained_var': 0.17723095,\n",
              "    'vf_loss': 89.054375},\n",
              "   'num_steps_sampled': 403530,\n",
              "   'num_steps_trained': 403530},\n",
              "  'iterations_since_restore': 275,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58571428571429, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19990619983391397,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.397112877360302,\n",
              "   'mean_inference_ms': 4.326708027130946,\n",
              "   'mean_raw_obs_processing_ms': 0.49150092504584664},\n",
              "  'time_since_restore': 1354.8589367866516,\n",
              "  'time_this_iter_s': 4.8436994552612305,\n",
              "  'time_total_s': 1354.8589367866516,\n",
              "  'timers': {'apply_grad_throughput': 1835.244,\n",
              "   'apply_grad_time_ms': 5.449,\n",
              "   'grad_wait_time_ms': 31.235,\n",
              "   'update_time_ms': 13.446},\n",
              "  'timestamp': 1635588965,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 403530,\n",
              "  'training_iteration': 275},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-10',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 695.41,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -13.99849991760243,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2725,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    208,\n",
              "    456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000],\n",
              "   'episode_reward': [47.00484216066819,\n",
              "    -146.79429365224325,\n",
              "    -85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 16.023668,\n",
              "    'model': {},\n",
              "    'policy_entropy': 5.7386785,\n",
              "    'policy_loss': 0.030919032,\n",
              "    'var_gnorm': 28.268602,\n",
              "    'vf_explained_var': 0.038834512,\n",
              "    'vf_loss': 0.08355076},\n",
              "   'num_steps_sampled': 404460,\n",
              "   'num_steps_trained': 404460},\n",
              "  'iterations_since_restore': 276,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.225, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.19990619983391397,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.397112877360302,\n",
              "   'mean_inference_ms': 4.326708027130946,\n",
              "   'mean_raw_obs_processing_ms': 0.49150092504584664},\n",
              "  'time_since_restore': 1359.7927753925323,\n",
              "  'time_this_iter_s': 4.933838605880737,\n",
              "  'time_total_s': 1359.7927753925323,\n",
              "  'timers': {'apply_grad_throughput': 1921.93,\n",
              "   'apply_grad_time_ms': 5.203,\n",
              "   'grad_wait_time_ms': 36.651,\n",
              "   'update_time_ms': 8.718},\n",
              "  'timestamp': 1635588970,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 404460,\n",
              "  'training_iteration': 276},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-15',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 703.33,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -13.389302243862408,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2727,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [456,\n",
              "    390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000],\n",
              "   'episode_reward': [-85.66973539096436,\n",
              "    9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.9159,\n",
              "    'policy_loss': -14.90955,\n",
              "    'var_gnorm': 28.280325,\n",
              "    'vf_explained_var': 0.13559628,\n",
              "    'vf_loss': 18.794329},\n",
              "   'num_steps_sampled': 405470,\n",
              "   'num_steps_trained': 405470},\n",
              "  'iterations_since_restore': 277,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.3, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.200180492430973,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4061721653592696,\n",
              "   'mean_inference_ms': 4.331776987271336,\n",
              "   'mean_raw_obs_processing_ms': 0.4922566799544626},\n",
              "  'time_since_restore': 1364.7389538288116,\n",
              "  'time_this_iter_s': 4.946178436279297,\n",
              "  'time_total_s': 1364.7389538288116,\n",
              "  'timers': {'apply_grad_throughput': 1520.915,\n",
              "   'apply_grad_time_ms': 6.575,\n",
              "   'grad_wait_time_ms': 38.374,\n",
              "   'update_time_ms': 8.215},\n",
              "  'timestamp': 1635588975,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 405470,\n",
              "  'training_iteration': 277},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-20',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 704.92,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -10.817105252788968,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2728,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [390,\n",
              "    163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615],\n",
              "   'episode_reward': [9.93876465351157,\n",
              "    -12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.275752,\n",
              "    'policy_loss': -61.78942,\n",
              "    'var_gnorm': 28.279549,\n",
              "    'vf_explained_var': -0.14632452,\n",
              "    'vf_loss': 168.48836},\n",
              "   'num_steps_sampled': 406400,\n",
              "   'num_steps_trained': 406400},\n",
              "  'iterations_since_restore': 278,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.07142857142857, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20037392013988872,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.411640699805388,\n",
              "   'mean_inference_ms': 4.333778539484948,\n",
              "   'mean_raw_obs_processing_ms': 0.49263520769829783},\n",
              "  'time_since_restore': 1369.6467587947845,\n",
              "  'time_this_iter_s': 4.9078049659729,\n",
              "  'time_total_s': 1369.6467587947845,\n",
              "  'timers': {'apply_grad_throughput': 1915.513,\n",
              "   'apply_grad_time_ms': 5.221,\n",
              "   'grad_wait_time_ms': 37.07,\n",
              "   'update_time_ms': 7.55},\n",
              "  'timestamp': 1635588980,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 406400,\n",
              "  'training_iteration': 278},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-25',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 710.51,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -9.776408267979484,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2729,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [163,\n",
              "    161,\n",
              "    446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949],\n",
              "   'episode_reward': [-12.172202582581491,\n",
              "    -122.64180267515664,\n",
              "    -38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.830336,\n",
              "    'policy_loss': -2.8362284,\n",
              "    'var_gnorm': 28.292358,\n",
              "    'vf_explained_var': -0.11682117,\n",
              "    'vf_loss': 3.5247018},\n",
              "   'num_steps_sampled': 407290,\n",
              "   'num_steps_trained': 407290},\n",
              "  'iterations_since_restore': 279,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.85714285714286,\n",
              "   'ram_util_percent': 21.214285714285715},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20049663770284748,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.416005657245362,\n",
              "   'mean_inference_ms': 4.336473182777849,\n",
              "   'mean_raw_obs_processing_ms': 0.4930209364668727},\n",
              "  'time_since_restore': 1374.5828971862793,\n",
              "  'time_this_iter_s': 4.936138391494751,\n",
              "  'time_total_s': 1374.5828971862793,\n",
              "  'timers': {'apply_grad_throughput': 1570.642,\n",
              "   'apply_grad_time_ms': 6.367,\n",
              "   'grad_wait_time_ms': 34.623,\n",
              "   'update_time_ms': 10.363},\n",
              "  'timestamp': 1635588985,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 407290,\n",
              "  'training_iteration': 279},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-30',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 724.58,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -9.763921166153294,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2731,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000],\n",
              "   'episode_reward': [-38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.0652027,\n",
              "    'policy_loss': 8.05746,\n",
              "    'var_gnorm': 28.29469,\n",
              "    'vf_explained_var': 0.38723576,\n",
              "    'vf_loss': 13.334985},\n",
              "   'num_steps_sampled': 408170,\n",
              "   'num_steps_trained': 408170},\n",
              "  'iterations_since_restore': 280,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6857142857143, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20077524077080677,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4254369006985874,\n",
              "   'mean_inference_ms': 4.34157855614545,\n",
              "   'mean_raw_obs_processing_ms': 0.49378956437800325},\n",
              "  'time_since_restore': 1379.4787199497223,\n",
              "  'time_this_iter_s': 4.895822763442993,\n",
              "  'time_total_s': 1379.4787199497223,\n",
              "  'timers': {'apply_grad_throughput': 1482.353,\n",
              "   'apply_grad_time_ms': 6.746,\n",
              "   'grad_wait_time_ms': 44.269,\n",
              "   'update_time_ms': 7.131},\n",
              "  'timestamp': 1635588990,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 408170,\n",
              "  'training_iteration': 280},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-35',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 724.58,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -9.763921166153294,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2731,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [446,\n",
              "    884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000],\n",
              "   'episode_reward': [-38.33896913735495,\n",
              "    -119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.305115,\n",
              "    'policy_loss': -3.0175023,\n",
              "    'var_gnorm': 28.312794,\n",
              "    'vf_explained_var': 0.86072356,\n",
              "    'vf_loss': 6.49654},\n",
              "   'num_steps_sampled': 409180,\n",
              "   'num_steps_trained': 409180},\n",
              "  'iterations_since_restore': 281,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.67142857142856, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20077524077080677,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4254369006985874,\n",
              "   'mean_inference_ms': 4.34157855614545,\n",
              "   'mean_raw_obs_processing_ms': 0.49378956437800325},\n",
              "  'time_since_restore': 1384.4478986263275,\n",
              "  'time_this_iter_s': 4.969178676605225,\n",
              "  'time_total_s': 1384.4478986263275,\n",
              "  'timers': {'apply_grad_throughput': 1961.064,\n",
              "   'apply_grad_time_ms': 5.099,\n",
              "   'grad_wait_time_ms': 38.648,\n",
              "   'update_time_ms': 6.921},\n",
              "  'timestamp': 1635588995,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 409180,\n",
              "  'training_iteration': 281},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-40',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 727.62,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -8.18636045752578,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2732,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [884,\n",
              "    696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750],\n",
              "   'episode_reward': [-119.23181650476985,\n",
              "    -126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.565324,\n",
              "    'policy_loss': -6.287804,\n",
              "    'var_gnorm': 28.292318,\n",
              "    'vf_explained_var': -0.12094307,\n",
              "    'vf_loss': 14.8639145},\n",
              "   'num_steps_sampled': 410070,\n",
              "   'num_steps_trained': 410070},\n",
              "  'iterations_since_restore': 282,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.88571428571429, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20092919206535245,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4305200754596024,\n",
              "   'mean_inference_ms': 4.344002453051797,\n",
              "   'mean_raw_obs_processing_ms': 0.4941729378598192},\n",
              "  'time_since_restore': 1389.4040277004242,\n",
              "  'time_this_iter_s': 4.95612907409668,\n",
              "  'time_total_s': 1389.4040277004242,\n",
              "  'timers': {'apply_grad_throughput': 1945.961,\n",
              "   'apply_grad_time_ms': 5.139,\n",
              "   'grad_wait_time_ms': 39.645,\n",
              "   'update_time_ms': 9.852},\n",
              "  'timestamp': 1635589000,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 410070,\n",
              "  'training_iteration': 282},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-46',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 728.78,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -7.047721708193982,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2733,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [696,\n",
              "    445,\n",
              "    500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000],\n",
              "   'episode_reward': [-126.20075694257207,\n",
              "    -114.79886756673098,\n",
              "    -77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.153514,\n",
              "    'policy_loss': -33.943745,\n",
              "    'var_gnorm': 28.289608,\n",
              "    'vf_explained_var': 0.44275457,\n",
              "    'vf_loss': 67.16009},\n",
              "   'num_steps_sampled': 411030,\n",
              "   'num_steps_trained': 411030},\n",
              "  'iterations_since_restore': 283,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.5, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20109868345113924,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4354672868363252,\n",
              "   'mean_inference_ms': 4.346406517444162,\n",
              "   'mean_raw_obs_processing_ms': 0.49457003512697634},\n",
              "  'time_since_restore': 1394.3774137496948,\n",
              "  'time_this_iter_s': 4.97338604927063,\n",
              "  'time_total_s': 1394.3774137496948,\n",
              "  'timers': {'apply_grad_throughput': 1663.773,\n",
              "   'apply_grad_time_ms': 6.01,\n",
              "   'grad_wait_time_ms': 38.019,\n",
              "   'update_time_ms': 10.205},\n",
              "  'timestamp': 1635589006,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 411030,\n",
              "  'training_iteration': 283},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-51',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 735.34,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -6.461499015044932,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2735,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [500,\n",
              "    1000,\n",
              "    437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000],\n",
              "   'episode_reward': [-77.61862297820727,\n",
              "    30.08886919247034,\n",
              "    -90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 17.593866,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.753479,\n",
              "    'policy_loss': 1.410326,\n",
              "    'var_gnorm': 28.29721,\n",
              "    'vf_explained_var': 0.47053295,\n",
              "    'vf_loss': 5.769867},\n",
              "   'num_steps_sampled': 411990,\n",
              "   'num_steps_trained': 411990},\n",
              "  'iterations_since_restore': 284,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.15714285714286, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2013776345160302,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4449566995433833,\n",
              "   'mean_inference_ms': 4.351571172025647,\n",
              "   'mean_raw_obs_processing_ms': 0.49534942850989255},\n",
              "  'time_since_restore': 1399.2166905403137,\n",
              "  'time_this_iter_s': 4.8392767906188965,\n",
              "  'time_total_s': 1399.2166905403137,\n",
              "  'timers': {'apply_grad_throughput': 1811.81,\n",
              "   'apply_grad_time_ms': 5.519,\n",
              "   'grad_wait_time_ms': 24.737,\n",
              "   'update_time_ms': 7.109},\n",
              "  'timestamp': 1635589011,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 411990,\n",
              "  'training_iteration': 284},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-16-56',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 724.19,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -7.556645300526014,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2737,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [437,\n",
              "    1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127],\n",
              "   'episode_reward': [-90.90649201808267,\n",
              "    -52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 4.519997,\n",
              "    'policy_loss': 1.1744182,\n",
              "    'var_gnorm': 28.30465,\n",
              "    'vf_explained_var': -0.033704042,\n",
              "    'vf_loss': 32.225014},\n",
              "   'num_steps_sampled': 413040,\n",
              "   'num_steps_trained': 413040},\n",
              "  'iterations_since_restore': 285,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.89999999999999,\n",
              "   'ram_util_percent': 21.228571428571428},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2017318225740362,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4553460737252042,\n",
              "   'mean_inference_ms': 4.356056639396908,\n",
              "   'mean_raw_obs_processing_ms': 0.49611671604334945},\n",
              "  'time_since_restore': 1404.2061154842377,\n",
              "  'time_this_iter_s': 4.98942494392395,\n",
              "  'time_total_s': 1404.2061154842377,\n",
              "  'timers': {'apply_grad_throughput': 1966.913,\n",
              "   'apply_grad_time_ms': 5.084,\n",
              "   'grad_wait_time_ms': 43.676,\n",
              "   'update_time_ms': 7.552},\n",
              "  'timestamp': 1635589016,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 413040,\n",
              "  'training_iteration': 285},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-01',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 727.53,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -4.817348448164924,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2738,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771],\n",
              "   'episode_reward': [-52.79406826053797,\n",
              "    -199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.332818,\n",
              "    'policy_loss': -16.747175,\n",
              "    'var_gnorm': 28.30533,\n",
              "    'vf_explained_var': 0.5972743,\n",
              "    'vf_loss': 28.887997},\n",
              "   'num_steps_sampled': 413930,\n",
              "   'num_steps_trained': 413930},\n",
              "  'iterations_since_restore': 286,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20190322069624694,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4600908938887687,\n",
              "   'mean_inference_ms': 4.358475422654733,\n",
              "   'mean_raw_obs_processing_ms': 0.49651403088120544},\n",
              "  'time_since_restore': 1409.1171236038208,\n",
              "  'time_this_iter_s': 4.91100811958313,\n",
              "  'time_total_s': 1409.1171236038208,\n",
              "  'timers': {'apply_grad_throughput': 1828.189,\n",
              "   'apply_grad_time_ms': 5.47,\n",
              "   'grad_wait_time_ms': 47.107,\n",
              "   'update_time_ms': 11.004},\n",
              "  'timestamp': 1635589021,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 413930,\n",
              "  'training_iteration': 286},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-06',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 726.74,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -2.6801169797909528,\n",
              "  'episode_reward_min': -199.86705262724564,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2739,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [719,\n",
              "    173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921],\n",
              "   'episode_reward': [-199.86705262724564,\n",
              "    12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.503446,\n",
              "    'policy_loss': 3.822225,\n",
              "    'var_gnorm': 28.2963,\n",
              "    'vf_explained_var': 0.65821683,\n",
              "    'vf_loss': 4.3488784},\n",
              "   'num_steps_sampled': 414750,\n",
              "   'num_steps_trained': 414750},\n",
              "  'iterations_since_restore': 287,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.37142857142858, 'ram_util_percent': 21.2},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20205985265091717,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4651769109799517,\n",
              "   'mean_inference_ms': 4.36088160149304,\n",
              "   'mean_raw_obs_processing_ms': 0.49689693728460305},\n",
              "  'time_since_restore': 1413.9946224689484,\n",
              "  'time_this_iter_s': 4.8774988651275635,\n",
              "  'time_total_s': 1413.9946224689484,\n",
              "  'timers': {'apply_grad_throughput': 2059.727,\n",
              "   'apply_grad_time_ms': 4.855,\n",
              "   'grad_wait_time_ms': 35.601,\n",
              "   'update_time_ms': 7.795},\n",
              "  'timestamp': 1635589026,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 414750,\n",
              "  'training_iteration': 287},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-11',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 729.55,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -0.9372682779418413,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2740,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [173,\n",
              "    502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000],\n",
              "   'episode_reward': [12.862185494266328,\n",
              "    -65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.413765,\n",
              "    'policy_loss': 22.961517,\n",
              "    'var_gnorm': 28.299818,\n",
              "    'vf_explained_var': 0.5394127,\n",
              "    'vf_loss': 15.346016},\n",
              "   'num_steps_sampled': 415740,\n",
              "   'num_steps_trained': 415740},\n",
              "  'iterations_since_restore': 288,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.7, 'ram_util_percent': 21.214285714285715},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20223147527260119,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4699025970922737,\n",
              "   'mean_inference_ms': 4.36328572139764,\n",
              "   'mean_raw_obs_processing_ms': 0.49729424332518735},\n",
              "  'time_since_restore': 1418.9123332500458,\n",
              "  'time_this_iter_s': 4.917710781097412,\n",
              "  'time_total_s': 1418.9123332500458,\n",
              "  'timers': {'apply_grad_throughput': 2035.932,\n",
              "   'apply_grad_time_ms': 4.912,\n",
              "   'grad_wait_time_ms': 36.886,\n",
              "   'update_time_ms': 10.755},\n",
              "  'timestamp': 1635589031,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 415740,\n",
              "  'training_iteration': 288},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-16',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 737.82,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': -1.1826337999859582,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2741,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [502,\n",
              "    1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000],\n",
              "   'episode_reward': [-65.74319819990546,\n",
              "    -17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.134748,\n",
              "    'policy_loss': -13.274097,\n",
              "    'var_gnorm': 28.290085,\n",
              "    'vf_explained_var': 0.5664164,\n",
              "    'vf_loss': 16.652851},\n",
              "   'num_steps_sampled': 416620,\n",
              "   'num_steps_trained': 416620},\n",
              "  'iterations_since_restore': 289,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.5875, 'ram_util_percent': 21.2375},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20243489999737496,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4753709501297947,\n",
              "   'mean_inference_ms': 4.365393052161274,\n",
              "   'mean_raw_obs_processing_ms': 0.49768297893227414},\n",
              "  'time_since_restore': 1423.8401868343353,\n",
              "  'time_this_iter_s': 4.927853584289551,\n",
              "  'time_total_s': 1423.8401868343353,\n",
              "  'timers': {'apply_grad_throughput': 1734.101,\n",
              "   'apply_grad_time_ms': 5.767,\n",
              "   'grad_wait_time_ms': 33.913,\n",
              "   'update_time_ms': 8.242},\n",
              "  'timestamp': 1635589036,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 416620,\n",
              "  'training_iteration': 289},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-21',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 742.33,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 0.7739757882630474,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2742,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953],\n",
              "   'episode_reward': [-17.223452411791044,\n",
              "    14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 4.990134,\n",
              "    'policy_loss': 5.728382,\n",
              "    'var_gnorm': 28.27977,\n",
              "    'vf_explained_var': -0.4605298,\n",
              "    'vf_loss': 2.617324},\n",
              "   'num_steps_sampled': 417640,\n",
              "   'num_steps_trained': 417640},\n",
              "  'iterations_since_restore': 290,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857,\n",
              "   'ram_util_percent': 21.285714285714285},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20256662098989917,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4798984692784285,\n",
              "   'mean_inference_ms': 4.368156191466283,\n",
              "   'mean_raw_obs_processing_ms': 0.4980878646786755},\n",
              "  'time_since_restore': 1428.8451993465424,\n",
              "  'time_this_iter_s': 5.005012512207031,\n",
              "  'time_total_s': 1428.8451993465424,\n",
              "  'timers': {'apply_grad_throughput': 2199.368,\n",
              "   'apply_grad_time_ms': 4.547,\n",
              "   'grad_wait_time_ms': 34.291,\n",
              "   'update_time_ms': 9.984},\n",
              "  'timestamp': 1635589041,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 417640,\n",
              "  'training_iteration': 290},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-26',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 740.7,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 1.6393459495411264,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2743,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837],\n",
              "   'episode_reward': [14.360701497532755,\n",
              "    -106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.422747,\n",
              "    'policy_loss': -49.042324,\n",
              "    'var_gnorm': 28.272223,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 202.67041},\n",
              "   'num_steps_sampled': 418590,\n",
              "   'num_steps_trained': 418590},\n",
              "  'iterations_since_restore': 291,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.85714285714285, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2027713496101407,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.485308745776644,\n",
              "   'mean_inference_ms': 4.370241057795066,\n",
              "   'mean_raw_obs_processing_ms': 0.49847796283257323},\n",
              "  'time_since_restore': 1433.8584604263306,\n",
              "  'time_this_iter_s': 5.013261079788208,\n",
              "  'time_total_s': 1433.8584604263306,\n",
              "  'timers': {'apply_grad_throughput': 2124.344,\n",
              "   'apply_grad_time_ms': 4.707,\n",
              "   'grad_wait_time_ms': 48.851,\n",
              "   'update_time_ms': 7.19},\n",
              "  'timestamp': 1635589046,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 418590,\n",
              "  'training_iteration': 291},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-31',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 740.7,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 1.137955673846158,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2744,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [174,\n",
              "    241,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000],\n",
              "   'episode_reward': [-106.34962956659855,\n",
              "    -77.30462331969062,\n",
              "    -22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.006725,\n",
              "    'policy_loss': -19.78109,\n",
              "    'var_gnorm': 28.254343,\n",
              "    'vf_explained_var': -0.25031102,\n",
              "    'vf_loss': 234.16014},\n",
              "   'num_steps_sampled': 419300,\n",
              "   'num_steps_trained': 419300},\n",
              "  'iterations_since_restore': 292,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.17142857142858,\n",
              "   'ram_util_percent': 21.285714285714285},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20290161179656457,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.4898337819905898,\n",
              "   'mean_inference_ms': 4.37300109813288,\n",
              "   'mean_raw_obs_processing_ms': 0.4988789248236393},\n",
              "  'time_since_restore': 1438.8853571414948,\n",
              "  'time_this_iter_s': 5.026896715164185,\n",
              "  'time_total_s': 1438.8853571414948,\n",
              "  'timers': {'apply_grad_throughput': 1991.654,\n",
              "   'apply_grad_time_ms': 5.021,\n",
              "   'grad_wait_time_ms': 65.107,\n",
              "   'update_time_ms': 8.893},\n",
              "  'timestamp': 1635589051,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 419300,\n",
              "  'training_iteration': 292},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-36',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 751.86,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 2.402056714582155,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2746,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000],\n",
              "   'episode_reward': [-22.4779404736355,\n",
              "    -8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.842114,\n",
              "    'policy_loss': -8.778636,\n",
              "    'var_gnorm': 28.249897,\n",
              "    'vf_explained_var': 0.8736099,\n",
              "    'vf_loss': 25.216925},\n",
              "   'num_steps_sampled': 420180,\n",
              "   'num_steps_trained': 420180},\n",
              "  'iterations_since_restore': 293,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.75714285714285, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20319502145267979,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.499548985731379,\n",
              "   'mean_inference_ms': 4.378194873734786,\n",
              "   'mean_raw_obs_processing_ms': 0.4996717124989862},\n",
              "  'time_since_restore': 1443.6449201107025,\n",
              "  'time_this_iter_s': 4.759562969207764,\n",
              "  'time_total_s': 1443.6449201107025,\n",
              "  'timers': {'apply_grad_throughput': 1401.118,\n",
              "   'apply_grad_time_ms': 7.137,\n",
              "   'grad_wait_time_ms': 26.489,\n",
              "   'update_time_ms': 8.507},\n",
              "  'timestamp': 1635589056,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 420180,\n",
              "  'training_iteration': 293},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-41',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 745.26,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 1.9189222890914766,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2747,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340],\n",
              "   'episode_reward': [-8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.026798,\n",
              "    'policy_loss': 7.000087,\n",
              "    'var_gnorm': 28.27733,\n",
              "    'vf_explained_var': 0.8608958,\n",
              "    'vf_loss': 6.1045055},\n",
              "   'num_steps_sampled': 421300,\n",
              "   'num_steps_trained': 421300},\n",
              "  'iterations_since_restore': 294,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6625, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2034019539761258,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5049473522799837,\n",
              "   'mean_inference_ms': 4.380256456895597,\n",
              "   'mean_raw_obs_processing_ms': 0.500061279061473},\n",
              "  'time_since_restore': 1448.6955964565277,\n",
              "  'time_this_iter_s': 5.050676345825195,\n",
              "  'time_total_s': 1448.6955964565277,\n",
              "  'timers': {'apply_grad_throughput': 1683.026,\n",
              "   'apply_grad_time_ms': 5.942,\n",
              "   'grad_wait_time_ms': 41.159,\n",
              "   'update_time_ms': 9.946},\n",
              "  'timestamp': 1635589061,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 421300,\n",
              "  'training_iteration': 294},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-46',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 745.26,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 1.9189222890914766,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2747,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340],\n",
              "   'episode_reward': [-8.874665242810893,\n",
              "    -34.06367439029824,\n",
              "    86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 2.5750794,\n",
              "    'policy_loss': 2.4268496,\n",
              "    'var_gnorm': 28.30028,\n",
              "    'vf_explained_var': 0.65228045,\n",
              "    'vf_loss': 59.477028},\n",
              "   'num_steps_sampled': 422100,\n",
              "   'num_steps_trained': 422100},\n",
              "  'iterations_since_restore': 295,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571427,\n",
              "   'ram_util_percent': 21.285714285714285},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2034019539761258,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5049473522799837,\n",
              "   'mean_inference_ms': 4.380256456895597,\n",
              "   'mean_raw_obs_processing_ms': 0.500061279061473},\n",
              "  'time_since_restore': 1453.640861272812,\n",
              "  'time_this_iter_s': 4.94526481628418,\n",
              "  'time_total_s': 1453.640861272812,\n",
              "  'timers': {'apply_grad_throughput': 1533.365,\n",
              "   'apply_grad_time_ms': 6.522,\n",
              "   'grad_wait_time_ms': 55.944,\n",
              "   'update_time_ms': 9.225},\n",
              "  'timestamp': 1635589066,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 422100,\n",
              "  'training_iteration': 295},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-51',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 742.79,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 5.085853085149815,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2749,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    649,\n",
              "    1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777],\n",
              "   'episode_reward': [86.67073985503104,\n",
              "    -143.24715045979286,\n",
              "    31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.942277,\n",
              "    'policy_loss': -31.325554,\n",
              "    'var_gnorm': 28.315464,\n",
              "    'vf_explained_var': 0.88098186,\n",
              "    'vf_loss': 84.12214},\n",
              "   'num_steps_sampled': 423120,\n",
              "   'num_steps_trained': 423120},\n",
              "  'iterations_since_restore': 296,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20373800713387774,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5147068949114708,\n",
              "   'mean_inference_ms': 4.385034559971771,\n",
              "   'mean_raw_obs_processing_ms': 0.5008520523657857},\n",
              "  'time_since_restore': 1458.6034696102142,\n",
              "  'time_this_iter_s': 4.962608337402344,\n",
              "  'time_total_s': 1458.6034696102142,\n",
              "  'timers': {'apply_grad_throughput': 1950.667,\n",
              "   'apply_grad_time_ms': 5.126,\n",
              "   'grad_wait_time_ms': 47.19,\n",
              "   'update_time_ms': 8.916},\n",
              "  'timestamp': 1635589071,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 423120,\n",
              "  'training_iteration': 296},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-17-56',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 738.39,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 4.275700628833412,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2751,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607],\n",
              "   'episode_reward': [31.06845751436031,\n",
              "    -30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.859512,\n",
              "    'policy_loss': 0.2320478,\n",
              "    'var_gnorm': 28.31601,\n",
              "    'vf_explained_var': 0.78784686,\n",
              "    'vf_loss': 6.3881993},\n",
              "   'num_steps_sampled': 424160,\n",
              "   'num_steps_trained': 424160},\n",
              "  'iterations_since_restore': 297,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142856, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20406970802827376,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5242950595292808,\n",
              "   'mean_inference_ms': 4.389768629897255,\n",
              "   'mean_raw_obs_processing_ms': 0.5016331825880163},\n",
              "  'time_since_restore': 1463.5317566394806,\n",
              "  'time_this_iter_s': 4.928287029266357,\n",
              "  'time_total_s': 1463.5317566394806,\n",
              "  'timers': {'apply_grad_throughput': 1715.981,\n",
              "   'apply_grad_time_ms': 5.828,\n",
              "   'grad_wait_time_ms': 35.435,\n",
              "   'update_time_ms': 9.368},\n",
              "  'timestamp': 1635589076,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 424160,\n",
              "  'training_iteration': 297},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-01',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 733.77,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 2.872670532421982,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2752,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538],\n",
              "   'episode_reward': [-30.17178955286306,\n",
              "    -59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.336136,\n",
              "    'policy_loss': 10.566051,\n",
              "    'var_gnorm': 28.319323,\n",
              "    'vf_explained_var': 0.83176327,\n",
              "    'vf_loss': 13.068413},\n",
              "   'num_steps_sampled': 425020,\n",
              "   'num_steps_trained': 425020},\n",
              "  'iterations_since_restore': 298,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857,\n",
              "   'ram_util_percent': 21.285714285714285},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2041947381163705,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.528509341073808,\n",
              "   'mean_inference_ms': 4.3924553363404435,\n",
              "   'mean_raw_obs_processing_ms': 0.5020220508146224},\n",
              "  'time_since_restore': 1468.5104038715363,\n",
              "  'time_this_iter_s': 4.978647232055664,\n",
              "  'time_total_s': 1468.5104038715363,\n",
              "  'timers': {'apply_grad_throughput': 1614.038,\n",
              "   'apply_grad_time_ms': 6.196,\n",
              "   'grad_wait_time_ms': 50.479,\n",
              "   'update_time_ms': 8.33},\n",
              "  'timestamp': 1635589081,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 425020,\n",
              "  'training_iteration': 298},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-06',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 733.77,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 3.1208840378867575,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2753,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [187,\n",
              "    709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000],\n",
              "   'episode_reward': [-59.13425909053666,\n",
              "    -106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.291871,\n",
              "    'policy_loss': -70.83432,\n",
              "    'var_gnorm': 28.338257,\n",
              "    'vf_explained_var': 0.6166545,\n",
              "    'vf_loss': 230.11969},\n",
              "   'num_steps_sampled': 425800,\n",
              "   'num_steps_trained': 425800},\n",
              "  'iterations_since_restore': 299,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20439570168833998,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5337583140597122,\n",
              "   'mean_inference_ms': 4.39443810216676,\n",
              "   'mean_raw_obs_processing_ms': 0.5023991936039567},\n",
              "  'time_since_restore': 1473.3657493591309,\n",
              "  'time_this_iter_s': 4.8553454875946045,\n",
              "  'time_total_s': 1473.3657493591309,\n",
              "  'timers': {'apply_grad_throughput': 1859.787,\n",
              "   'apply_grad_time_ms': 5.377,\n",
              "   'grad_wait_time_ms': 27.423,\n",
              "   'update_time_ms': 8.185},\n",
              "  'timestamp': 1635589086,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 425800,\n",
              "  'training_iteration': 299},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-11',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 741.01,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 5.205275094352819,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2754,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [709,\n",
              "    236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911],\n",
              "   'episode_reward': [-106.6946680227969,\n",
              "    1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.474028,\n",
              "    'policy_loss': -17.996443,\n",
              "    'var_gnorm': 28.342522,\n",
              "    'vf_explained_var': 0.62956953,\n",
              "    'vf_loss': 37.462837},\n",
              "   'num_steps_sampled': 426710,\n",
              "   'num_steps_trained': 426710},\n",
              "  'iterations_since_restore': 300,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.85, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20452185076068374,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5380829530897129,\n",
              "   'mean_inference_ms': 4.397124865258835,\n",
              "   'mean_raw_obs_processing_ms': 0.5027901103447474},\n",
              "  'time_since_restore': 1478.377460718155,\n",
              "  'time_this_iter_s': 5.011711359024048,\n",
              "  'time_total_s': 1478.377460718155,\n",
              "  'timers': {'apply_grad_throughput': 1713.227,\n",
              "   'apply_grad_time_ms': 5.837,\n",
              "   'grad_wait_time_ms': 38.966,\n",
              "   'update_time_ms': 9.169},\n",
              "  'timestamp': 1635589091,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 426710,\n",
              "  'training_iteration': 300},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-16',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 743.92,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 6.152352295584517,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2755,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [236,\n",
              "    1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000],\n",
              "   'episode_reward': [1.7824867667513615,\n",
              "    -50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 4.524125,\n",
              "    'model': {},\n",
              "    'policy_entropy': 5.682288,\n",
              "    'policy_loss': -0.29336214,\n",
              "    'var_gnorm': 28.334375,\n",
              "    'vf_explained_var': -0.04216206,\n",
              "    'vf_loss': 0.021888755},\n",
              "   'num_steps_sampled': 427590,\n",
              "   'num_steps_trained': 427590},\n",
              "  'iterations_since_restore': 301,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20472243391667053,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5434036153088302,\n",
              "   'mean_inference_ms': 4.399101650384283,\n",
              "   'mean_raw_obs_processing_ms': 0.5031637271695815},\n",
              "  'time_since_restore': 1483.2874293327332,\n",
              "  'time_this_iter_s': 4.909968614578247,\n",
              "  'time_total_s': 1483.2874293327332,\n",
              "  'timers': {'apply_grad_throughput': 1462.424,\n",
              "   'apply_grad_time_ms': 6.838,\n",
              "   'grad_wait_time_ms': 43.629,\n",
              "   'update_time_ms': 6.57},\n",
              "  'timestamp': 1635589096,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 427590,\n",
              "  'training_iteration': 301},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-21',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 749.64,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 7.32736163219744,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2756,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808],\n",
              "   'episode_reward': [-50.387496886933626,\n",
              "    -120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.829685,\n",
              "    'policy_loss': -28.784,\n",
              "    'var_gnorm': 28.342182,\n",
              "    'vf_explained_var': 0.5803439,\n",
              "    'vf_loss': 54.769485},\n",
              "   'num_steps_sampled': 428670,\n",
              "   'num_steps_trained': 428670},\n",
              "  'iterations_since_restore': 302,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.57142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20489320024202776,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5479945925310048,\n",
              "   'mean_inference_ms': 4.401436618870212,\n",
              "   'mean_raw_obs_processing_ms': 0.5035534299560352},\n",
              "  'time_since_restore': 1488.2570736408234,\n",
              "  'time_this_iter_s': 4.96964430809021,\n",
              "  'time_total_s': 1488.2570736408234,\n",
              "  'timers': {'apply_grad_throughput': 2457.638,\n",
              "   'apply_grad_time_ms': 4.069,\n",
              "   'grad_wait_time_ms': 39.76,\n",
              "   'update_time_ms': 6.474},\n",
              "  'timestamp': 1635589101,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 428670,\n",
              "  'training_iteration': 302},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-26',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 746.39,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 6.508935972677113,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2757,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [752,\n",
              "    805,\n",
              "    152,\n",
              "    301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675],\n",
              "   'episode_reward': [-120.14672449623329,\n",
              "    111.93170741347932,\n",
              "    -17.846518945071935,\n",
              "    -109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.1695175,\n",
              "    'policy_loss': -13.838615,\n",
              "    'var_gnorm': 28.342537,\n",
              "    'vf_explained_var': 0.8034536,\n",
              "    'vf_loss': 16.929272},\n",
              "   'num_steps_sampled': 429570,\n",
              "   'num_steps_trained': 429570},\n",
              "  'iterations_since_restore': 303,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.05714285714285, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20504467490135078,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5530261694293168,\n",
              "   'mean_inference_ms': 4.403718720350484,\n",
              "   'mean_raw_obs_processing_ms': 0.5039212092867466},\n",
              "  'time_since_restore': 1493.1823916435242,\n",
              "  'time_this_iter_s': 4.925318002700806,\n",
              "  'time_total_s': 1493.1823916435242,\n",
              "  'timers': {'apply_grad_throughput': 1984.173,\n",
              "   'apply_grad_time_ms': 5.04,\n",
              "   'grad_wait_time_ms': 51.125,\n",
              "   'update_time_ms': 8.076},\n",
              "  'timestamp': 1635589106,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 429570,\n",
              "  'training_iteration': 303},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-31',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 748.37,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 4.911971322744107,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2760,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259],\n",
              "   'episode_reward': [-109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 12.980073,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.226038,\n",
              "    'policy_loss': -0.022964716,\n",
              "    'var_gnorm': 28.34581,\n",
              "    'vf_explained_var': 0.80447376,\n",
              "    'vf_loss': 3.1429067},\n",
              "   'num_steps_sampled': 430490,\n",
              "   'num_steps_trained': 430490},\n",
              "  'iterations_since_restore': 304,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.15714285714284, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20555811824226974,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5680132093062173,\n",
              "   'mean_inference_ms': 4.410201356643907,\n",
              "   'mean_raw_obs_processing_ms': 0.5050385331687197},\n",
              "  'time_since_restore': 1498.0761065483093,\n",
              "  'time_this_iter_s': 4.893714904785156,\n",
              "  'time_total_s': 1498.0761065483093,\n",
              "  'timers': {'apply_grad_throughput': 2289.017,\n",
              "   'apply_grad_time_ms': 4.369,\n",
              "   'grad_wait_time_ms': 28.1,\n",
              "   'update_time_ms': 7.689},\n",
              "  'timestamp': 1635589111,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 430490,\n",
              "  'training_iteration': 304},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-36',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 748.37,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 4.911971322744107,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2760,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [301,\n",
              "    226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259],\n",
              "   'episode_reward': [-109.68916960185221,\n",
              "    -119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 36.036686,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.220059,\n",
              "    'policy_loss': -2.4798884,\n",
              "    'var_gnorm': 28.354353,\n",
              "    'vf_explained_var': 0.8496541,\n",
              "    'vf_loss': 1.9210149},\n",
              "   'num_steps_sampled': 431480,\n",
              "   'num_steps_trained': 431480},\n",
              "  'iterations_since_restore': 305,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.35714285714286, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20555811824226974,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5680132093062173,\n",
              "   'mean_inference_ms': 4.410201356643907,\n",
              "   'mean_raw_obs_processing_ms': 0.5050385331687197},\n",
              "  'time_since_restore': 1503.078770160675,\n",
              "  'time_this_iter_s': 5.002663612365723,\n",
              "  'time_total_s': 1503.078770160675,\n",
              "  'timers': {'apply_grad_throughput': 1858.247,\n",
              "   'apply_grad_time_ms': 5.381,\n",
              "   'grad_wait_time_ms': 51.61,\n",
              "   'update_time_ms': 7.931},\n",
              "  'timestamp': 1635589116,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 431480,\n",
              "  'training_iteration': 305},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-41',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 752.52,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 4.794345608857251,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2761,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [226,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716],\n",
              "   'episode_reward': [-119.43971439148832,\n",
              "    25.61851890705302,\n",
              "    -12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.9425745,\n",
              "    'policy_loss': -15.346347,\n",
              "    'var_gnorm': 28.356327,\n",
              "    'vf_explained_var': 0.7446836,\n",
              "    'vf_loss': 15.133646},\n",
              "   'num_steps_sampled': 432350,\n",
              "   'num_steps_trained': 432350},\n",
              "  'iterations_since_restore': 306,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.3375, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2057280476855645,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5727696169259544,\n",
              "   'mean_inference_ms': 4.412526328055211,\n",
              "   'mean_raw_obs_processing_ms': 0.505426338620447},\n",
              "  'time_since_restore': 1508.0662424564362,\n",
              "  'time_this_iter_s': 4.987472295761108,\n",
              "  'time_total_s': 1508.0662424564362,\n",
              "  'timers': {'apply_grad_throughput': 1568.287,\n",
              "   'apply_grad_time_ms': 6.376,\n",
              "   'grad_wait_time_ms': 51.916,\n",
              "   'update_time_ms': 8.757},\n",
              "  'timestamp': 1635589121,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 432350,\n",
              "  'training_iteration': 306},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-46',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 759.55,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 6.757777242247626,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2763,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000],\n",
              "   'episode_reward': [-12.216469735460212,\n",
              "    -14.689827887067896,\n",
              "    -130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 5.200541,\n",
              "    'policy_loss': 186.47227,\n",
              "    'var_gnorm': 28.360836,\n",
              "    'vf_explained_var': 0.0,\n",
              "    'vf_loss': 58589.973},\n",
              "   'num_steps_sampled': 433280,\n",
              "   'num_steps_trained': 433280},\n",
              "  'iterations_since_restore': 307,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20604955932539026,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5826051577976648,\n",
              "   'mean_inference_ms': 4.417068398445366,\n",
              "   'mean_raw_obs_processing_ms': 0.5061713771133161},\n",
              "  'time_since_restore': 1512.9368641376495,\n",
              "  'time_this_iter_s': 4.870621681213379,\n",
              "  'time_total_s': 1512.9368641376495,\n",
              "  'timers': {'apply_grad_throughput': 1961.477,\n",
              "   'apply_grad_time_ms': 5.098,\n",
              "   'grad_wait_time_ms': 35.568,\n",
              "   'update_time_ms': 6.836},\n",
              "  'timestamp': 1635589126,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 433280,\n",
              "  'training_iteration': 307},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-51',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 750.14,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 8.038164691794782,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2765,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [329,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569],\n",
              "   'episode_reward': [-130.8616312505889,\n",
              "    42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 6.6214476,\n",
              "    'policy_loss': -21.83543,\n",
              "    'var_gnorm': 28.373825,\n",
              "    'vf_explained_var': -0.3885038,\n",
              "    'vf_loss': 165.64998},\n",
              "   'num_steps_sampled': 434310,\n",
              "   'num_steps_trained': 434310},\n",
              "  'iterations_since_restore': 308,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.98571428571428, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20636668658749227,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.5921099060294768,\n",
              "   'mean_inference_ms': 4.4215560975680805,\n",
              "   'mean_raw_obs_processing_ms': 0.5068986936394351},\n",
              "  'time_since_restore': 1517.8753814697266,\n",
              "  'time_this_iter_s': 4.938517332077026,\n",
              "  'time_total_s': 1517.8753814697266,\n",
              "  'timers': {'apply_grad_throughput': 2235.722,\n",
              "   'apply_grad_time_ms': 4.473,\n",
              "   'grad_wait_time_ms': 37.653,\n",
              "   'update_time_ms': 9.449},\n",
              "  'timestamp': 1635589131,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 434310,\n",
              "  'training_iteration': 308},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-18-56',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 751.24,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 8.363863137646689,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2766,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439],\n",
              "   'episode_reward': [42.03409076178046,\n",
              "    38.08821399456513,\n",
              "    -36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.788061,\n",
              "    'policy_loss': 0.25938284,\n",
              "    'var_gnorm': 28.386156,\n",
              "    'vf_explained_var': -0.92886186,\n",
              "    'vf_loss': 10.849529},\n",
              "   'num_steps_sampled': 435340,\n",
              "   'num_steps_trained': 435340},\n",
              "  'iterations_since_restore': 309,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.85714285714286, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20649221962643058,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.596288735219528,\n",
              "   'mean_inference_ms': 4.424198767388143,\n",
              "   'mean_raw_obs_processing_ms': 0.5072712229303583},\n",
              "  'time_since_restore': 1522.8173522949219,\n",
              "  'time_this_iter_s': 4.9419708251953125,\n",
              "  'time_total_s': 1522.8173522949219,\n",
              "  'timers': {'apply_grad_throughput': 1494.528,\n",
              "   'apply_grad_time_ms': 6.691,\n",
              "   'grad_wait_time_ms': 35.969,\n",
              "   'update_time_ms': 8.854},\n",
              "  'timestamp': 1635589136,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 435340,\n",
              "  'training_iteration': 309},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-01',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 742.37,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 5.302831689393442,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2768,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    541,\n",
              "    155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864],\n",
              "   'episode_reward': [-36.031274461370856,\n",
              "    -65.59590628882793,\n",
              "    -136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.8669405,\n",
              "    'policy_loss': -46.670628,\n",
              "    'var_gnorm': 28.388191,\n",
              "    'vf_explained_var': 0.7736757,\n",
              "    'vf_loss': 97.82335},\n",
              "   'num_steps_sampled': 436420,\n",
              "   'num_steps_trained': 436420},\n",
              "  'iterations_since_restore': 310,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.17142857142858, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20684580371530947,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6058010042228448,\n",
              "   'mean_inference_ms': 4.428331716155211,\n",
              "   'mean_raw_obs_processing_ms': 0.5079941491489878},\n",
              "  'time_since_restore': 1527.7369105815887,\n",
              "  'time_this_iter_s': 4.91955828666687,\n",
              "  'time_total_s': 1527.7369105815887,\n",
              "  'timers': {'apply_grad_throughput': 1523.207,\n",
              "   'apply_grad_time_ms': 6.565,\n",
              "   'grad_wait_time_ms': 36.018,\n",
              "   'update_time_ms': 11.757},\n",
              "  'timestamp': 1635589141,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 436420,\n",
              "  'training_iteration': 310},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-06',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 739.44,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 6.997205666336299,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2770,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603],\n",
              "   'episode_reward': [-136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.372227,\n",
              "    'policy_loss': 6.741915,\n",
              "    'var_gnorm': 28.387835,\n",
              "    'vf_explained_var': 0.8094462,\n",
              "    'vf_loss': 13.4050455},\n",
              "   'num_steps_sampled': 437460,\n",
              "   'num_steps_trained': 437460},\n",
              "  'iterations_since_restore': 311,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.84285714285714, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2071142280897959,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6144835212769275,\n",
              "   'mean_inference_ms': 4.433124864518911,\n",
              "   'mean_raw_obs_processing_ms': 0.5087153520245632},\n",
              "  'time_since_restore': 1532.7207889556885,\n",
              "  'time_this_iter_s': 4.9838783740997314,\n",
              "  'time_total_s': 1532.7207889556885,\n",
              "  'timers': {'apply_grad_throughput': 2150.429,\n",
              "   'apply_grad_time_ms': 4.65,\n",
              "   'grad_wait_time_ms': 37.825,\n",
              "   'update_time_ms': 8.324},\n",
              "  'timestamp': 1635589146,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 437460,\n",
              "  'training_iteration': 311},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-11',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 739.44,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 6.997205666336299,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2770,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [155,\n",
              "    666,\n",
              "    1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603],\n",
              "   'episode_reward': [-136.75312508059773,\n",
              "    -146.19932800957383,\n",
              "    -60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.803162,\n",
              "    'policy_loss': 20.293915,\n",
              "    'var_gnorm': 28.378738,\n",
              "    'vf_explained_var': 0.7838723,\n",
              "    'vf_loss': 25.943285},\n",
              "   'num_steps_sampled': 438270,\n",
              "   'num_steps_trained': 438270},\n",
              "  'iterations_since_restore': 312,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.32499999999999, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2071142280897959,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6144835212769275,\n",
              "   'mean_inference_ms': 4.433124864518911,\n",
              "   'mean_raw_obs_processing_ms': 0.5087153520245632},\n",
              "  'time_since_restore': 1537.7162425518036,\n",
              "  'time_this_iter_s': 4.995453596115112,\n",
              "  'time_total_s': 1537.7162425518036,\n",
              "  'timers': {'apply_grad_throughput': 2022.95,\n",
              "   'apply_grad_time_ms': 4.943,\n",
              "   'grad_wait_time_ms': 53.401,\n",
              "   'update_time_ms': 7.77},\n",
              "  'timestamp': 1635589151,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 438270,\n",
              "  'training_iteration': 312},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-16',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 751.15,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 10.913072412693133,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2772,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000],\n",
              "   'episode_reward': [-60.64189053214166,\n",
              "    -47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.282686,\n",
              "    'policy_loss': 8.987643,\n",
              "    'var_gnorm': 28.390108,\n",
              "    'vf_explained_var': 0.8401775,\n",
              "    'vf_loss': 9.024613},\n",
              "   'num_steps_sampled': 439060,\n",
              "   'num_steps_trained': 439060},\n",
              "  'iterations_since_restore': 313,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.68571428571428, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2074631952972835,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.624037072085599,\n",
              "   'mean_inference_ms': 4.437213006712252,\n",
              "   'mean_raw_obs_processing_ms': 0.5094364761904715},\n",
              "  'time_since_restore': 1542.580177307129,\n",
              "  'time_this_iter_s': 4.863934755325317,\n",
              "  'time_total_s': 1542.580177307129,\n",
              "  'timers': {'apply_grad_throughput': 1662.091,\n",
              "   'apply_grad_time_ms': 6.017,\n",
              "   'grad_wait_time_ms': 29.322,\n",
              "   'update_time_ms': 7.877},\n",
              "  'timestamp': 1635589156,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 439060,\n",
              "  'training_iteration': 313},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-21',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 746.29,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 11.072555023397936,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2773,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [303,\n",
              "    1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514],\n",
              "   'episode_reward': [-47.83031964054379,\n",
              "    -30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.4126635,\n",
              "    'policy_loss': 10.441446,\n",
              "    'var_gnorm': 28.39851,\n",
              "    'vf_explained_var': 0.7457865,\n",
              "    'vf_loss': 18.082022},\n",
              "   'num_steps_sampled': 440110,\n",
              "   'num_steps_trained': 440110},\n",
              "  'iterations_since_restore': 314,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6857142857143, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20760828238475354,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.628757419846826,\n",
              "   'mean_inference_ms': 4.439357540271544,\n",
              "   'mean_raw_obs_processing_ms': 0.5097785297606944},\n",
              "  'time_since_restore': 1547.4861690998077,\n",
              "  'time_this_iter_s': 4.905991792678833,\n",
              "  'time_total_s': 1547.4861690998077,\n",
              "  'timers': {'apply_grad_throughput': 1698.498,\n",
              "   'apply_grad_time_ms': 5.888,\n",
              "   'grad_wait_time_ms': 38.121,\n",
              "   'update_time_ms': 10.962},\n",
              "  'timestamp': 1635589161,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 440110,\n",
              "  'training_iteration': 314},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-27',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 749.42,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 10.251152479253738,\n",
              "  'episode_reward_min': -151.15649330618714,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2774,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    726,\n",
              "    478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616],\n",
              "   'episode_reward': [-30.478606093427373,\n",
              "    -77.56812162641202,\n",
              "    -36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 3.0210416,\n",
              "    'policy_loss': -3.4358153,\n",
              "    'var_gnorm': 28.413486,\n",
              "    'vf_explained_var': 0.10289174,\n",
              "    'vf_loss': 117.472176},\n",
              "   'num_steps_sampled': 441040,\n",
              "   'num_steps_trained': 441040},\n",
              "  'iterations_since_restore': 315,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.37142857142855, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2077305220666779,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6328147832845383,\n",
              "   'mean_inference_ms': 4.441960305071486,\n",
              "   'mean_raw_obs_processing_ms': 0.5101528268389227},\n",
              "  'time_since_restore': 1552.469970703125,\n",
              "  'time_this_iter_s': 4.983801603317261,\n",
              "  'time_total_s': 1552.469970703125,\n",
              "  'timers': {'apply_grad_throughput': 2332.138,\n",
              "   'apply_grad_time_ms': 4.288,\n",
              "   'grad_wait_time_ms': 40.005,\n",
              "   'update_time_ms': 7.373},\n",
              "  'timestamp': 1635589167,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 441040,\n",
              "  'training_iteration': 315},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-32',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 747.05,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 10.631844681532321,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2776,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [478,\n",
              "    486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763],\n",
              "   'episode_reward': [-36.540973037628234,\n",
              "    -30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.318922,\n",
              "    'policy_loss': 12.3037,\n",
              "    'var_gnorm': 28.4328,\n",
              "    'vf_explained_var': 0.8703877,\n",
              "    'vf_loss': 25.21956},\n",
              "   'num_steps_sampled': 442070,\n",
              "   'num_steps_trained': 442070},\n",
              "  'iterations_since_restore': 316,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.5, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2080355978011273,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6418300285891683,\n",
              "   'mean_inference_ms': 4.446405646109513,\n",
              "   'mean_raw_obs_processing_ms': 0.5108750471581527},\n",
              "  'time_since_restore': 1557.4357059001923,\n",
              "  'time_this_iter_s': 4.965735197067261,\n",
              "  'time_total_s': 1557.4357059001923,\n",
              "  'timers': {'apply_grad_throughput': 1787.411,\n",
              "   'apply_grad_time_ms': 5.595,\n",
              "   'grad_wait_time_ms': 37.977,\n",
              "   'update_time_ms': 10.695},\n",
              "  'timestamp': 1635589172,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 442070,\n",
              "  'training_iteration': 316},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-37',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 750.51,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 12.485314393140724,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2777,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [486,\n",
              "    711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824],\n",
              "   'episode_reward': [-30.315476987023175,\n",
              "    -136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 2.39913,\n",
              "    'policy_loss': -1.0750427,\n",
              "    'var_gnorm': 28.431822,\n",
              "    'vf_explained_var': -0.005009055,\n",
              "    'vf_loss': 7.4205093},\n",
              "   'num_steps_sampled': 442960,\n",
              "   'num_steps_trained': 442960},\n",
              "  'iterations_since_restore': 317,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2082188296960732,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6468707419885416,\n",
              "   'mean_inference_ms': 4.448233930864403,\n",
              "   'mean_raw_obs_processing_ms': 0.5112236679342953},\n",
              "  'time_since_restore': 1562.3451054096222,\n",
              "  'time_this_iter_s': 4.909399509429932,\n",
              "  'time_total_s': 1562.3451054096222,\n",
              "  'timers': {'apply_grad_throughput': 1731.96,\n",
              "   'apply_grad_time_ms': 5.774,\n",
              "   'grad_wait_time_ms': 29.414,\n",
              "   'update_time_ms': 11.309},\n",
              "  'timestamp': 1635589177,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 442960,\n",
              "  'training_iteration': 317},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-42',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 752.83,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 13.892571380779067,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2778,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [711,\n",
              "    651,\n",
              "    1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718],\n",
              "   'episode_reward': [-136.3144383410982,\n",
              "    -120.13811587150653,\n",
              "    119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.506168,\n",
              "    'policy_loss': -42.176437,\n",
              "    'var_gnorm': 28.431803,\n",
              "    'vf_explained_var': 0.7151805,\n",
              "    'vf_loss': 85.70321},\n",
              "   'num_steps_sampled': 443920,\n",
              "   'num_steps_trained': 443920},\n",
              "  'iterations_since_restore': 318,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.3125, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20837701739041642,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6511699162284277,\n",
              "   'mean_inference_ms': 4.450475610265677,\n",
              "   'mean_raw_obs_processing_ms': 0.511600322194597},\n",
              "  'time_since_restore': 1567.3233609199524,\n",
              "  'time_this_iter_s': 4.9782555103302,\n",
              "  'time_total_s': 1567.3233609199524,\n",
              "  'timers': {'apply_grad_throughput': 2119.074,\n",
              "   'apply_grad_time_ms': 4.719,\n",
              "   'grad_wait_time_ms': 55.224,\n",
              "   'update_time_ms': 8.432},\n",
              "  'timestamp': 1635589182,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 443920,\n",
              "  'training_iteration': 318},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-47',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 754.71,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 16.10627940422984,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2780,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763],\n",
              "   'episode_reward': [119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.257832,\n",
              "    'policy_loss': 13.649523,\n",
              "    'var_gnorm': 28.43501,\n",
              "    'vf_explained_var': 0.73988295,\n",
              "    'vf_loss': 78.895294},\n",
              "   'num_steps_sampled': 444810,\n",
              "   'num_steps_trained': 444810},\n",
              "  'iterations_since_restore': 319,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.77142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2086789789015009,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6602825772914078,\n",
              "   'mean_inference_ms': 4.454805031513082,\n",
              "   'mean_raw_obs_processing_ms': 0.5123161522527164},\n",
              "  'time_since_restore': 1572.2526042461395,\n",
              "  'time_this_iter_s': 4.929243326187134,\n",
              "  'time_total_s': 1572.2526042461395,\n",
              "  'timers': {'apply_grad_throughput': 1700.198,\n",
              "   'apply_grad_time_ms': 5.882,\n",
              "   'grad_wait_time_ms': 28.087,\n",
              "   'update_time_ms': 9.964},\n",
              "  'timestamp': 1635589187,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 444810,\n",
              "  'training_iteration': 319},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-52',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 754.71,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 16.10627940422984,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2780,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    831,\n",
              "    1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763],\n",
              "   'episode_reward': [119.3102795286251,\n",
              "    198.55198356006582,\n",
              "    6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.296837,\n",
              "    'policy_loss': -47.26158,\n",
              "    'var_gnorm': 28.429626,\n",
              "    'vf_explained_var': 0.5873066,\n",
              "    'vf_loss': 55.216953},\n",
              "   'num_steps_sampled': 445750,\n",
              "   'num_steps_trained': 445750},\n",
              "  'iterations_since_restore': 320,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.55714285714285, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2086789789015009,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6602825772914078,\n",
              "   'mean_inference_ms': 4.454805031513082,\n",
              "   'mean_raw_obs_processing_ms': 0.5123161522527164},\n",
              "  'time_since_restore': 1577.2481327056885,\n",
              "  'time_this_iter_s': 4.99552845954895,\n",
              "  'time_total_s': 1577.2481327056885,\n",
              "  'timers': {'apply_grad_throughput': 1773.745,\n",
              "   'apply_grad_time_ms': 5.638,\n",
              "   'grad_wait_time_ms': 51.26,\n",
              "   'update_time_ms': 9.862},\n",
              "  'timestamp': 1635589192,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 445750,\n",
              "  'training_iteration': 320},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-19-57',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 752.83,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 235.59301940279036,\n",
              "  'episode_reward_mean': 16.37272100717524,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2782,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    996,\n",
              "    595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832],\n",
              "   'episode_reward': [6.3512759327981865,\n",
              "    97.60986674217514,\n",
              "    -107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.427448,\n",
              "    'policy_loss': -10.674477,\n",
              "    'var_gnorm': 28.414347,\n",
              "    'vf_explained_var': 0.70398676,\n",
              "    'vf_loss': 9.235686},\n",
              "   'num_steps_sampled': 446800,\n",
              "   'num_steps_trained': 446800},\n",
              "  'iterations_since_restore': 321,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.17142857142858, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20897328954100108,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6693436931021757,\n",
              "   'mean_inference_ms': 4.459039673801957,\n",
              "   'mean_raw_obs_processing_ms': 0.5130211873470868},\n",
              "  'time_since_restore': 1582.206434726715,\n",
              "  'time_this_iter_s': 4.958302021026611,\n",
              "  'time_total_s': 1582.206434726715,\n",
              "  'timers': {'apply_grad_throughput': 2016.599,\n",
              "   'apply_grad_time_ms': 4.959,\n",
              "   'grad_wait_time_ms': 42.799,\n",
              "   'update_time_ms': 9.868},\n",
              "  'timestamp': 1635589197,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 446800,\n",
              "  'training_iteration': 321},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-02',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 744.28,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 16.465679500494904,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2784,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [595,\n",
              "    1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604],\n",
              "   'episode_reward': [-107.59243527480152,\n",
              "    -24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.848825,\n",
              "    'policy_loss': 0.40617824,\n",
              "    'var_gnorm': 28.429035,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 126.924515},\n",
              "   'num_steps_sampled': 447830,\n",
              "   'num_steps_trained': 447830},\n",
              "  'iterations_since_restore': 322,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 99.10000000000001, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20926207241082653,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6782345914895074,\n",
              "   'mean_inference_ms': 4.463180493079442,\n",
              "   'mean_raw_obs_processing_ms': 0.5137097836835536},\n",
              "  'time_since_restore': 1587.177886724472,\n",
              "  'time_this_iter_s': 4.971451997756958,\n",
              "  'time_total_s': 1587.177886724472,\n",
              "  'timers': {'apply_grad_throughput': 1487.774,\n",
              "   'apply_grad_time_ms': 6.721,\n",
              "   'grad_wait_time_ms': 30.966,\n",
              "   'update_time_ms': 8.767},\n",
              "  'timestamp': 1635589202,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 447830,\n",
              "  'training_iteration': 322},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-07',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 744.93,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 19.428577496874365,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2785,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660],\n",
              "   'episode_reward': [-24.769759869360303,\n",
              "    -93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.965321,\n",
              "    'policy_loss': 18.146233,\n",
              "    'var_gnorm': 28.428442,\n",
              "    'vf_explained_var': 0.5653809,\n",
              "    'vf_loss': 41.57231},\n",
              "   'num_steps_sampled': 448810,\n",
              "   'num_steps_trained': 448810},\n",
              "  'iterations_since_restore': 323,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.78571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2094122347544839,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6823973880594272,\n",
              "   'mean_inference_ms': 4.465306493942303,\n",
              "   'mean_raw_obs_processing_ms': 0.5140677805254151},\n",
              "  'time_since_restore': 1592.1265976428986,\n",
              "  'time_this_iter_s': 4.948710918426514,\n",
              "  'time_total_s': 1592.1265976428986,\n",
              "  'timers': {'apply_grad_throughput': 1967.697,\n",
              "   'apply_grad_time_ms': 5.082,\n",
              "   'grad_wait_time_ms': 35.361,\n",
              "   'update_time_ms': 9.645},\n",
              "  'timestamp': 1635589207,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 448810,\n",
              "  'training_iteration': 323},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-12',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 743.41,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 20.688609263970125,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2786,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [486,\n",
              "    1000,\n",
              "    844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848],\n",
              "   'episode_reward': [-93.7176207457916,\n",
              "    52.21643006315441,\n",
              "    123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.123527,\n",
              "    'policy_loss': -17.506065,\n",
              "    'var_gnorm': 28.428486,\n",
              "    'vf_explained_var': 0.23290491,\n",
              "    'vf_loss': 18.30427},\n",
              "   'num_steps_sampled': 449800,\n",
              "   'num_steps_trained': 449800},\n",
              "  'iterations_since_restore': 324,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.32499999999999, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20954753160093603,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.6869917839136832,\n",
              "   'mean_inference_ms': 4.467257052476817,\n",
              "   'mean_raw_obs_processing_ms': 0.5143885738393582},\n",
              "  'time_since_restore': 1597.008590221405,\n",
              "  'time_this_iter_s': 4.88199257850647,\n",
              "  'time_total_s': 1597.008590221405,\n",
              "  'timers': {'apply_grad_throughput': 1688.114,\n",
              "   'apply_grad_time_ms': 5.924,\n",
              "   'grad_wait_time_ms': 36.718,\n",
              "   'update_time_ms': 7.655},\n",
              "  'timestamp': 1635589212,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 449800,\n",
              "  'training_iteration': 324},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-17',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 742.43,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 18.035621491236963,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2788,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [844,\n",
              "    1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484],\n",
              "   'episode_reward': [123.61771537625495,\n",
              "    61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.963465,\n",
              "    'policy_loss': 0.46846414,\n",
              "    'var_gnorm': 28.45399,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 9.684485},\n",
              "   'num_steps_sampled': 450800,\n",
              "   'num_steps_trained': 450800},\n",
              "  'iterations_since_restore': 325,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142856, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20983332654082815,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.695714183316589,\n",
              "   'mean_inference_ms': 4.471340826332376,\n",
              "   'mean_raw_obs_processing_ms': 0.5150678118863058},\n",
              "  'time_since_restore': 1601.972661972046,\n",
              "  'time_this_iter_s': 4.964071750640869,\n",
              "  'time_total_s': 1601.972661972046,\n",
              "  'timers': {'apply_grad_throughput': 1289.837,\n",
              "   'apply_grad_time_ms': 7.753,\n",
              "   'grad_wait_time_ms': 33.063,\n",
              "   'update_time_ms': 9.641},\n",
              "  'timestamp': 1635589217,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 450800,\n",
              "  'training_iteration': 325},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-22',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 738.47,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 18.861907176952954,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2789,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    891,\n",
              "    765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448],\n",
              "   'episode_reward': [61.83630435014297,\n",
              "    215.85492330560436,\n",
              "    -51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.500835,\n",
              "    'policy_loss': -24.813824,\n",
              "    'var_gnorm': 28.457151,\n",
              "    'vf_explained_var': 0.74989194,\n",
              "    'vf_loss': 25.030556},\n",
              "   'num_steps_sampled': 451740,\n",
              "   'num_steps_trained': 451740},\n",
              "  'iterations_since_restore': 326,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.28571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.20994167910306213,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.699315119665146,\n",
              "   'mean_inference_ms': 4.47373775805183,\n",
              "   'mean_raw_obs_processing_ms': 0.5154126361986355},\n",
              "  'time_since_restore': 1606.8945007324219,\n",
              "  'time_this_iter_s': 4.921838760375977,\n",
              "  'time_total_s': 1606.8945007324219,\n",
              "  'timers': {'apply_grad_throughput': 2036.248,\n",
              "   'apply_grad_time_ms': 4.911,\n",
              "   'grad_wait_time_ms': 45.365,\n",
              "   'update_time_ms': 8.445},\n",
              "  'timestamp': 1635589222,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 451740,\n",
              "  'training_iteration': 326},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-27',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 733.11,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 15.790726671038902,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2791,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [765,\n",
              "    584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000],\n",
              "   'episode_reward': [-51.023013098830695,\n",
              "    -111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 6.9625998,\n",
              "    'policy_loss': 4.323247,\n",
              "    'var_gnorm': 28.44819,\n",
              "    'vf_explained_var': 0.8481507,\n",
              "    'vf_loss': 20.986338},\n",
              "   'num_steps_sampled': 452810,\n",
              "   'num_steps_trained': 452810},\n",
              "  'iterations_since_restore': 327,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.95714285714287, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21022325079167767,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.707609061269039,\n",
              "   'mean_inference_ms': 4.477742756551299,\n",
              "   'mean_raw_obs_processing_ms': 0.5160739636774399},\n",
              "  'time_since_restore': 1611.8266949653625,\n",
              "  'time_this_iter_s': 4.932194232940674,\n",
              "  'time_total_s': 1611.8266949653625,\n",
              "  'timers': {'apply_grad_throughput': 1826.096,\n",
              "   'apply_grad_time_ms': 5.476,\n",
              "   'grad_wait_time_ms': 37.159,\n",
              "   'update_time_ms': 7.199},\n",
              "  'timestamp': 1635589227,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 452810,\n",
              "  'training_iteration': 327},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-32',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 733.42,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 17.73255991685573,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2792,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [584,\n",
              "    1000,\n",
              "    782,\n",
              "    231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796],\n",
              "   'episode_reward': [-111.43958030628255,\n",
              "    72.30269323247191,\n",
              "    -147.45399382162503,\n",
              "    -110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.568531,\n",
              "    'policy_loss': 21.111263,\n",
              "    'var_gnorm': 28.453335,\n",
              "    'vf_explained_var': -0.29005885,\n",
              "    'vf_loss': 46.73805},\n",
              "   'num_steps_sampled': 453680,\n",
              "   'num_steps_trained': 453680},\n",
              "  'iterations_since_restore': 328,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.68571428571428, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21037138722978782,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7115123339547904,\n",
              "   'mean_inference_ms': 4.479829316671203,\n",
              "   'mean_raw_obs_processing_ms': 0.5164220824187238},\n",
              "  'time_since_restore': 1616.7958626747131,\n",
              "  'time_this_iter_s': 4.969167709350586,\n",
              "  'time_total_s': 1616.7958626747131,\n",
              "  'timers': {'apply_grad_throughput': 1617.193,\n",
              "   'apply_grad_time_ms': 6.184,\n",
              "   'grad_wait_time_ms': 45.408,\n",
              "   'update_time_ms': 7.044},\n",
              "  'timestamp': 1635589232,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 453680,\n",
              "  'training_iteration': 328},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-37',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 723.27,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 16.58396582218703,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2795,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [231,\n",
              "    829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257],\n",
              "   'episode_reward': [-110.6141877724257,\n",
              "    137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 6.2483816,\n",
              "    'policy_loss': 8.148602,\n",
              "    'var_gnorm': 28.47232,\n",
              "    'vf_explained_var': 0.6421047,\n",
              "    'vf_loss': 32.181625},\n",
              "   'num_steps_sampled': 454740,\n",
              "   'num_steps_trained': 454740},\n",
              "  'iterations_since_restore': 329,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.4857142857143, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21078598312103455,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.723943285461905,\n",
              "   'mean_inference_ms': 4.485674410922203,\n",
              "   'mean_raw_obs_processing_ms': 0.517381700792391},\n",
              "  'time_since_restore': 1621.6723742485046,\n",
              "  'time_this_iter_s': 4.876511573791504,\n",
              "  'time_total_s': 1621.6723742485046,\n",
              "  'timers': {'apply_grad_throughput': 1956.91,\n",
              "   'apply_grad_time_ms': 5.11,\n",
              "   'grad_wait_time_ms': 35.364,\n",
              "   'update_time_ms': 8.311},\n",
              "  'timestamp': 1635589237,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 454740,\n",
              "  'training_iteration': 329},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-42',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 725.59,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 19.009656895857475,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2796,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [829,\n",
              "    369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463],\n",
              "   'episode_reward': [137.59368426720158,\n",
              "    -33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.441367,\n",
              "    'policy_loss': -31.934052,\n",
              "    'var_gnorm': 28.476952,\n",
              "    'vf_explained_var': 0.15462905,\n",
              "    'vf_loss': 57.265633},\n",
              "   'num_steps_sampled': 455650,\n",
              "   'num_steps_trained': 455650},\n",
              "  'iterations_since_restore': 330,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 99.30000000000001, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21095762885269365,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.728577415169721,\n",
              "   'mean_inference_ms': 4.4872195513178035,\n",
              "   'mean_raw_obs_processing_ms': 0.5176855813964769},\n",
              "  'time_since_restore': 1626.6329143047333,\n",
              "  'time_this_iter_s': 4.960540056228638,\n",
              "  'time_total_s': 1626.6329143047333,\n",
              "  'timers': {'apply_grad_throughput': 2048.75,\n",
              "   'apply_grad_time_ms': 4.881,\n",
              "   'grad_wait_time_ms': 46.362,\n",
              "   'update_time_ms': 9.957},\n",
              "  'timestamp': 1635589242,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 455650,\n",
              "  'training_iteration': 330},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-47',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 727.05,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 19.27437910029634,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2797,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [369,\n",
              "    1000,\n",
              "    630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975],\n",
              "   'episode_reward': [-33.02177848877464,\n",
              "    70.15592708667748,\n",
              "    235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.598379,\n",
              "    'policy_loss': 4.653037,\n",
              "    'var_gnorm': 28.470076,\n",
              "    'vf_explained_var': 0.9119839,\n",
              "    'vf_loss': 9.897629},\n",
              "   'num_steps_sampled': 456550,\n",
              "   'num_steps_trained': 456550},\n",
              "  'iterations_since_restore': 331,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21110083933799723,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7323725097145803,\n",
              "   'mean_inference_ms': 4.489229391653638,\n",
              "   'mean_raw_obs_processing_ms': 0.5180185879591631},\n",
              "  'time_since_restore': 1631.5826585292816,\n",
              "  'time_this_iter_s': 4.94974422454834,\n",
              "  'time_total_s': 1631.5826585292816,\n",
              "  'timers': {'apply_grad_throughput': 2120.263,\n",
              "   'apply_grad_time_ms': 4.716,\n",
              "   'grad_wait_time_ms': 37.47,\n",
              "   'update_time_ms': 7.054},\n",
              "  'timestamp': 1635589247,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 456550,\n",
              "  'training_iteration': 331},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-52',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 726.09,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 19.585506054172505,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2799,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [630,\n",
              "    666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906],\n",
              "   'episode_reward': [235.59301940279036,\n",
              "    205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.788193,\n",
              "    'policy_loss': 11.086025,\n",
              "    'var_gnorm': 28.473955,\n",
              "    'vf_explained_var': 0.89920706,\n",
              "    'vf_loss': 5.9639597},\n",
              "   'num_steps_sampled': 457590,\n",
              "   'num_steps_trained': 457590},\n",
              "  'iterations_since_restore': 332,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21137339714389877,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.74027824142212,\n",
              "   'mean_inference_ms': 4.493107950009985,\n",
              "   'mean_raw_obs_processing_ms': 0.5186513801901544},\n",
              "  'time_since_restore': 1636.5101289749146,\n",
              "  'time_this_iter_s': 4.927470445632935,\n",
              "  'time_total_s': 1636.5101289749146,\n",
              "  'timers': {'apply_grad_throughput': 1935.526,\n",
              "   'apply_grad_time_ms': 5.167,\n",
              "   'grad_wait_time_ms': 31.951,\n",
              "   'update_time_ms': 6.602},\n",
              "  'timestamp': 1635589252,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 457590,\n",
              "  'training_iteration': 332},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-20-57',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 726.93,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 16.34728425601236,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2800,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [666,\n",
              "    218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714],\n",
              "   'episode_reward': [205.5215005132656,\n",
              "    -22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.846781,\n",
              "    'policy_loss': 51.77413,\n",
              "    'var_gnorm': 28.500648,\n",
              "    'vf_explained_var': -0.56823933,\n",
              "    'vf_loss': 180.16823},\n",
              "   'num_steps_sampled': 458540,\n",
              "   'num_steps_trained': 458540},\n",
              "  'iterations_since_restore': 333,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.07142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21151689585615052,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7440508119441787,\n",
              "   'mean_inference_ms': 4.49513269702876,\n",
              "   'mean_raw_obs_processing_ms': 0.5189858270564569},\n",
              "  'time_since_restore': 1641.4924631118774,\n",
              "  'time_this_iter_s': 4.982334136962891,\n",
              "  'time_total_s': 1641.4924631118774,\n",
              "  'timers': {'apply_grad_throughput': 1464.527,\n",
              "   'apply_grad_time_ms': 6.828,\n",
              "   'grad_wait_time_ms': 32.322,\n",
              "   'update_time_ms': 9.366},\n",
              "  'timestamp': 1635589257,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 458540,\n",
              "  'training_iteration': 333},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-02',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 730.27,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.32564182181286,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2801,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [218,\n",
              "    908,\n",
              "    1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000],\n",
              "   'episode_reward': [-22.044470735964907,\n",
              "    122.92212327654727,\n",
              "    -68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 6.892522,\n",
              "    'policy_loss': -6.095443,\n",
              "    'var_gnorm': 28.509869,\n",
              "    'vf_explained_var': 0.9047757,\n",
              "    'vf_loss': 1.9546714},\n",
              "   'num_steps_sampled': 459510,\n",
              "   'num_steps_trained': 459510},\n",
              "  'iterations_since_restore': 334,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.17142857142858, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2116466173407153,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7481914675389165,\n",
              "   'mean_inference_ms': 4.496980164081181,\n",
              "   'mean_raw_obs_processing_ms': 0.5192869418930187},\n",
              "  'time_since_restore': 1646.4005947113037,\n",
              "  'time_this_iter_s': 4.9081315994262695,\n",
              "  'time_total_s': 1646.4005947113037,\n",
              "  'timers': {'apply_grad_throughput': 1778.008,\n",
              "   'apply_grad_time_ms': 5.624,\n",
              "   'grad_wait_time_ms': 35.181,\n",
              "   'update_time_ms': 7.778},\n",
              "  'timestamp': 1635589262,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 459510,\n",
              "  'training_iteration': 334},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-07',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 734.85,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.62768305134573,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2803,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    547,\n",
              "    1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584],\n",
              "   'episode_reward': [-68.3407000431389,\n",
              "    -81.76419871461518,\n",
              "    -110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.747189,\n",
              "    'policy_loss': 10.631112,\n",
              "    'var_gnorm': 28.523808,\n",
              "    'vf_explained_var': 0.403951,\n",
              "    'vf_loss': 14.38458},\n",
              "   'num_steps_sampled': 460430,\n",
              "   'num_steps_trained': 460430},\n",
              "  'iterations_since_restore': 335,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142856, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21191697527735104,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.756083394489211,\n",
              "   'mean_inference_ms': 4.5008091434167525,\n",
              "   'mean_raw_obs_processing_ms': 0.5199149233156445},\n",
              "  'time_since_restore': 1651.335168838501,\n",
              "  'time_this_iter_s': 4.934574127197266,\n",
              "  'time_total_s': 1651.335168838501,\n",
              "  'timers': {'apply_grad_throughput': 1551.314,\n",
              "   'apply_grad_time_ms': 6.446,\n",
              "   'grad_wait_time_ms': 27.639,\n",
              "   'update_time_ms': 5.977},\n",
              "  'timestamp': 1635589267,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 460430,\n",
              "  'training_iteration': 335},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-12',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 726.92,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.200726425856054,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2805,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    601,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313],\n",
              "   'episode_reward': [-110.5715478842417,\n",
              "    -73.61151530819329,\n",
              "    -51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.432115,\n",
              "    'policy_loss': -3.3035862,\n",
              "    'var_gnorm': 28.540884,\n",
              "    'vf_explained_var': 0.44288796,\n",
              "    'vf_loss': 7.96069},\n",
              "   'num_steps_sampled': 461460,\n",
              "   'num_steps_trained': 461460},\n",
              "  'iterations_since_restore': 336,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.32499999999999, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21216811572831035,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7641146547545696,\n",
              "   'mean_inference_ms': 4.504357285975757,\n",
              "   'mean_raw_obs_processing_ms': 0.5204978318861886},\n",
              "  'time_since_restore': 1656.2906465530396,\n",
              "  'time_this_iter_s': 4.955477714538574,\n",
              "  'time_total_s': 1656.2906465530396,\n",
              "  'timers': {'apply_grad_throughput': 2040.915,\n",
              "   'apply_grad_time_ms': 4.9,\n",
              "   'grad_wait_time_ms': 33.006,\n",
              "   'update_time_ms': 7.024},\n",
              "  'timestamp': 1635589272,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 461460,\n",
              "  'training_iteration': 336},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-17',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 721.87,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.05712301794747,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2807,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383],\n",
              "   'episode_reward': [-51.37254911899046,\n",
              "    -44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.060982,\n",
              "    'policy_loss': 7.0357747,\n",
              "    'var_gnorm': 28.556532,\n",
              "    'vf_explained_var': 0.6678515,\n",
              "    'vf_loss': 17.278852},\n",
              "   'num_steps_sampled': 462510,\n",
              "   'num_steps_trained': 462510},\n",
              "  'iterations_since_restore': 337,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.17142857142856, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21246281713960236,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7720045943154918,\n",
              "   'mean_inference_ms': 4.507735912550399,\n",
              "   'mean_raw_obs_processing_ms': 0.5211011285942814},\n",
              "  'time_since_restore': 1661.2277812957764,\n",
              "  'time_this_iter_s': 4.937134742736816,\n",
              "  'time_total_s': 1661.2277812957764,\n",
              "  'timers': {'apply_grad_throughput': 1824.055,\n",
              "   'apply_grad_time_ms': 5.482,\n",
              "   'grad_wait_time_ms': 39.77,\n",
              "   'update_time_ms': 9.157},\n",
              "  'timestamp': 1635589277,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 462510,\n",
              "  'training_iteration': 337},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-22',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 718.76,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.153008011905875,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2808,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689],\n",
              "   'episode_reward': [-44.758635299991596,\n",
              "    -65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.67642,\n",
              "    'policy_loss': 14.759916,\n",
              "    'var_gnorm': 28.565401,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 39.04476},\n",
              "   'num_steps_sampled': 463520,\n",
              "   'num_steps_trained': 463520},\n",
              "  'iterations_since_restore': 338,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 99.2, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21256433600860647,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7751874877602096,\n",
              "   'mean_inference_ms': 4.509963000256365,\n",
              "   'mean_raw_obs_processing_ms': 0.5214274289272303},\n",
              "  'time_since_restore': 1666.1876385211945,\n",
              "  'time_this_iter_s': 4.959857225418091,\n",
              "  'time_total_s': 1666.1876385211945,\n",
              "  'timers': {'apply_grad_throughput': 1398.829,\n",
              "   'apply_grad_time_ms': 7.149,\n",
              "   'grad_wait_time_ms': 34.45,\n",
              "   'update_time_ms': 6.928},\n",
              "  'timestamp': 1635589282,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 463520,\n",
              "  'training_iteration': 338},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-27',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 715.98,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 11.188210455043079,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2809,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    517,\n",
              "    471,\n",
              "    282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722],\n",
              "   'episode_reward': [-65.72233093426776,\n",
              "    166.52823006459522,\n",
              "    -71.44756211448039,\n",
              "    -132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.096121,\n",
              "    'policy_loss': -17.847027,\n",
              "    'var_gnorm': 28.554499,\n",
              "    'vf_explained_var': 0.07729596,\n",
              "    'vf_loss': 49.505554},\n",
              "   'num_steps_sampled': 464520,\n",
              "   'num_steps_trained': 464520},\n",
              "  'iterations_since_restore': 339,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.25714285714287, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2127189762390119,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.77944550462282,\n",
              "   'mean_inference_ms': 4.511383820926428,\n",
              "   'mean_raw_obs_processing_ms': 0.521707129034723},\n",
              "  'time_since_restore': 1671.1050908565521,\n",
              "  'time_this_iter_s': 4.917452335357666,\n",
              "  'time_total_s': 1671.1050908565521,\n",
              "  'timers': {'apply_grad_throughput': 1747.124,\n",
              "   'apply_grad_time_ms': 5.724,\n",
              "   'grad_wait_time_ms': 44.391,\n",
              "   'update_time_ms': 8.127},\n",
              "  'timestamp': 1635589287,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 464520,\n",
              "  'training_iteration': 339},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-32',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 713.92,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.140680723291789,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2812,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633],\n",
              "   'episode_reward': [-132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.1318035,\n",
              "    'policy_loss': -9.714839,\n",
              "    'var_gnorm': 28.559248,\n",
              "    'vf_explained_var': 0.6057398,\n",
              "    'vf_loss': 15.52593},\n",
              "   'num_steps_sampled': 465570,\n",
              "   'num_steps_trained': 465570},\n",
              "  'iterations_since_restore': 340,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.24285714285713, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2130779365165243,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7896810588120289,\n",
              "   'mean_inference_ms': 4.5172685472817475,\n",
              "   'mean_raw_obs_processing_ms': 0.5226232142253322},\n",
              "  'time_since_restore': 1676.0272409915924,\n",
              "  'time_this_iter_s': 4.922150135040283,\n",
              "  'time_total_s': 1676.0272409915924,\n",
              "  'timers': {'apply_grad_throughput': 1463.255,\n",
              "   'apply_grad_time_ms': 6.834,\n",
              "   'grad_wait_time_ms': 23.861,\n",
              "   'update_time_ms': 8.118},\n",
              "  'timestamp': 1635589292,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 465570,\n",
              "  'training_iteration': 340},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-37',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 713.92,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.140680723291789,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2812,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [282,\n",
              "    977,\n",
              "    943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633],\n",
              "   'episode_reward': [-132.48365338627536,\n",
              "    177.02084227611851,\n",
              "    153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 1.043432,\n",
              "    'policy_loss': -0.04274349,\n",
              "    'var_gnorm': 28.558758,\n",
              "    'vf_explained_var': 0.44237375,\n",
              "    'vf_loss': 0.35407308},\n",
              "   'num_steps_sampled': 466480,\n",
              "   'num_steps_trained': 466480},\n",
              "  'iterations_since_restore': 341,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.89999999999999, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2130779365165243,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7896810588120289,\n",
              "   'mean_inference_ms': 4.5172685472817475,\n",
              "   'mean_raw_obs_processing_ms': 0.5226232142253322},\n",
              "  'time_since_restore': 1680.9989354610443,\n",
              "  'time_this_iter_s': 4.971694469451904,\n",
              "  'time_total_s': 1680.9989354610443,\n",
              "  'timers': {'apply_grad_throughput': 1485.856,\n",
              "   'apply_grad_time_ms': 6.73,\n",
              "   'grad_wait_time_ms': 46.557,\n",
              "   'update_time_ms': 7.614},\n",
              "  'timestamp': 1635589297,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 466480,\n",
              "  'training_iteration': 341},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-42',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 719.19,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 15.908560412812513,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2814,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [943,\n",
              "    517,\n",
              "    769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974],\n",
              "   'episode_reward': [153.02511912608335,\n",
              "    168.65757409769475,\n",
              "    177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.249496,\n",
              "    'policy_loss': -12.213136,\n",
              "    'var_gnorm': 28.549831,\n",
              "    'vf_explained_var': 0.35706913,\n",
              "    'vf_loss': 19.1789},\n",
              "   'num_steps_sampled': 467310,\n",
              "   'num_steps_trained': 467310},\n",
              "  'iterations_since_restore': 342,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.88571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21333169951945968,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.7970001270855176,\n",
              "   'mean_inference_ms': 4.520901797608633,\n",
              "   'mean_raw_obs_processing_ms': 0.5232162322687324},\n",
              "  'time_since_restore': 1685.8994929790497,\n",
              "  'time_this_iter_s': 4.900557518005371,\n",
              "  'time_total_s': 1685.8994929790497,\n",
              "  'timers': {'apply_grad_throughput': 2101.302,\n",
              "   'apply_grad_time_ms': 4.759,\n",
              "   'grad_wait_time_ms': 27.736,\n",
              "   'update_time_ms': 7.842},\n",
              "  'timestamp': 1635589302,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 467310,\n",
              "  'training_iteration': 342},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-47',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 718.18,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.544353754085938,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2816,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [769,\n",
              "    720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709],\n",
              "   'episode_reward': [177.35764766375019,\n",
              "    190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.9219007,\n",
              "    'policy_loss': 2.5185323,\n",
              "    'var_gnorm': 28.549387,\n",
              "    'vf_explained_var': 0.42098278,\n",
              "    'vf_loss': 5.948121},\n",
              "   'num_steps_sampled': 468300,\n",
              "   'num_steps_trained': 468300},\n",
              "  'iterations_since_restore': 343,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.4, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21360472468265082,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8046493439313434,\n",
              "   'mean_inference_ms': 4.524121801107936,\n",
              "   'mean_raw_obs_processing_ms': 0.5237853426309161},\n",
              "  'time_since_restore': 1690.8181405067444,\n",
              "  'time_this_iter_s': 4.918647527694702,\n",
              "  'time_total_s': 1690.8181405067444,\n",
              "  'timers': {'apply_grad_throughput': 1865.644,\n",
              "   'apply_grad_time_ms': 5.36,\n",
              "   'grad_wait_time_ms': 30.89,\n",
              "   'update_time_ms': 5.827},\n",
              "  'timestamp': 1635589307,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 468300,\n",
              "  'training_iteration': 343},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-52',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 715.27,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.366994609549636,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2817,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [720,\n",
              "    623,\n",
              "    513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478],\n",
              "   'episode_reward': [190.87767864007463,\n",
              "    -136.7958014225781,\n",
              "    -146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.839884,\n",
              "    'policy_loss': -40.665653,\n",
              "    'var_gnorm': 28.560535,\n",
              "    'vf_explained_var': 0.7596451,\n",
              "    'vf_loss': 96.81165},\n",
              "   'num_steps_sampled': 469380,\n",
              "   'num_steps_trained': 469380},\n",
              "  'iterations_since_restore': 344,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.15714285714286, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21372589414687806,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8083017630064397,\n",
              "   'mean_inference_ms': 4.525829189926377,\n",
              "   'mean_raw_obs_processing_ms': 0.5240588054182009},\n",
              "  'time_since_restore': 1695.765007019043,\n",
              "  'time_this_iter_s': 4.946866512298584,\n",
              "  'time_total_s': 1695.765007019043,\n",
              "  'timers': {'apply_grad_throughput': 1756.041,\n",
              "   'apply_grad_time_ms': 5.695,\n",
              "   'grad_wait_time_ms': 32.685,\n",
              "   'update_time_ms': 7.699},\n",
              "  'timestamp': 1635589312,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 469380,\n",
              "  'training_iteration': 344},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-21-57',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 712.13,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.039397325817895,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2819,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417],\n",
              "   'episode_reward': [-146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.850174,\n",
              "    'policy_loss': 10.134601,\n",
              "    'var_gnorm': 28.556795,\n",
              "    'vf_explained_var': 0.8557039,\n",
              "    'vf_loss': 10.398294},\n",
              "   'num_steps_sampled': 470510,\n",
              "   'num_steps_trained': 470510},\n",
              "  'iterations_since_restore': 345,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2139711884736757,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8153450978730812,\n",
              "   'mean_inference_ms': 4.529342673122946,\n",
              "   'mean_raw_obs_processing_ms': 0.5246352238256958},\n",
              "  'time_since_restore': 1700.6771185398102,\n",
              "  'time_this_iter_s': 4.912111520767212,\n",
              "  'time_total_s': 1700.6771185398102,\n",
              "  'timers': {'apply_grad_throughput': 1457.575,\n",
              "   'apply_grad_time_ms': 6.861,\n",
              "   'grad_wait_time_ms': 27.414,\n",
              "   'update_time_ms': 8.917},\n",
              "  'timestamp': 1635589317,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 470510,\n",
              "  'training_iteration': 345},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-02',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 712.13,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.039397325817895,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2819,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [513,\n",
              "    800,\n",
              "    531,\n",
              "    526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417],\n",
              "   'episode_reward': [-146.47290179170824,\n",
              "    168.85751502050556,\n",
              "    206.3687596854563,\n",
              "    -98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.044056,\n",
              "    'policy_loss': -23.724054,\n",
              "    'var_gnorm': 28.551098,\n",
              "    'vf_explained_var': 0.82359076,\n",
              "    'vf_loss': 19.470812},\n",
              "   'num_steps_sampled': 471320,\n",
              "   'num_steps_trained': 471320},\n",
              "  'iterations_since_restore': 346,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.27142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2139711884736757,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8153450978730812,\n",
              "   'mean_inference_ms': 4.529342673122946,\n",
              "   'mean_raw_obs_processing_ms': 0.5246352238256958},\n",
              "  'time_since_restore': 1705.6539685726166,\n",
              "  'time_this_iter_s': 4.9768500328063965,\n",
              "  'time_total_s': 1705.6539685726166,\n",
              "  'timers': {'apply_grad_throughput': 1925.026,\n",
              "   'apply_grad_time_ms': 5.195,\n",
              "   'grad_wait_time_ms': 53.128,\n",
              "   'update_time_ms': 7.555},\n",
              "  'timestamp': 1635589322,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 471320,\n",
              "  'training_iteration': 346},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-07',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 714.98,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 9.392681311896009,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2822,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [526,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815],\n",
              "   'episode_reward': [-98.41471711737339,\n",
              "    191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.5701537,\n",
              "    'policy_loss': -4.098745,\n",
              "    'var_gnorm': 28.551472,\n",
              "    'vf_explained_var': 0.87511367,\n",
              "    'vf_loss': 2.4594033},\n",
              "   'num_steps_sampled': 472350,\n",
              "   'num_steps_trained': 472350},\n",
              "  'iterations_since_restore': 347,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.54285714285713, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21431170841935845,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.825343796569729,\n",
              "   'mean_inference_ms': 4.53493817046308,\n",
              "   'mean_raw_obs_processing_ms': 0.5255256344325526},\n",
              "  'time_since_restore': 1710.5406346321106,\n",
              "  'time_this_iter_s': 4.8866660594940186,\n",
              "  'time_total_s': 1710.5406346321106,\n",
              "  'timers': {'apply_grad_throughput': 2539.97,\n",
              "   'apply_grad_time_ms': 3.937,\n",
              "   'grad_wait_time_ms': 34.874,\n",
              "   'update_time_ms': 6.99},\n",
              "  'timestamp': 1635589327,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 472350,\n",
              "  'training_iteration': 347},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-12',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 717.95,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 11.589906505343674,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2823,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823],\n",
              "   'episode_reward': [191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.9749975,\n",
              "    'policy_loss': -0.26600528,\n",
              "    'var_gnorm': 28.550694,\n",
              "    'vf_explained_var': -0.5799955,\n",
              "    'vf_loss': 23.225742},\n",
              "   'num_steps_sampled': 473380,\n",
              "   'num_steps_trained': 473380},\n",
              "  'iterations_since_restore': 348,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.35714285714286, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2144550592148586,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8294392864035194,\n",
              "   'mean_inference_ms': 4.536259676504965,\n",
              "   'mean_raw_obs_processing_ms': 0.525785923179406},\n",
              "  'time_since_restore': 1715.4776549339294,\n",
              "  'time_this_iter_s': 4.937020301818848,\n",
              "  'time_total_s': 1715.4776549339294,\n",
              "  'timers': {'apply_grad_throughput': 2131.079,\n",
              "   'apply_grad_time_ms': 4.692,\n",
              "   'grad_wait_time_ms': 30.824,\n",
              "   'update_time_ms': 8.086},\n",
              "  'timestamp': 1635589332,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 473380,\n",
              "  'training_iteration': 348},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-17',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 717.95,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 11.589906505343674,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2823,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [921,\n",
              "    1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823],\n",
              "   'episode_reward': [191.6519228310267,\n",
              "    -29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.888567,\n",
              "    'policy_loss': 10.812438,\n",
              "    'var_gnorm': 28.535988,\n",
              "    'vf_explained_var': 0.6596426,\n",
              "    'vf_loss': 13.622646},\n",
              "   'num_steps_sampled': 474210,\n",
              "   'num_steps_trained': 474210},\n",
              "  'iterations_since_restore': 349,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.0625, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2144550592148586,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8294392864035194,\n",
              "   'mean_inference_ms': 4.536259676504965,\n",
              "   'mean_raw_obs_processing_ms': 0.525785923179406},\n",
              "  'time_since_restore': 1720.509976387024,\n",
              "  'time_this_iter_s': 5.032321453094482,\n",
              "  'time_total_s': 1720.509976387024,\n",
              "  'timers': {'apply_grad_throughput': 1532.037,\n",
              "   'apply_grad_time_ms': 6.527,\n",
              "   'grad_wait_time_ms': 56.793,\n",
              "   'update_time_ms': 8.257},\n",
              "  'timestamp': 1635589337,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 474210,\n",
              "  'training_iteration': 349},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-22',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 718.74,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 9.379048127561433,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2824,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    1000,\n",
              "    615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000],\n",
              "   'episode_reward': [-29.64477253306719,\n",
              "    5.031480441752524,\n",
              "    -43.901164559325366,\n",
              "    171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.673459,\n",
              "    'policy_loss': 24.972376,\n",
              "    'var_gnorm': 28.53895,\n",
              "    'vf_explained_var': 0.46493655,\n",
              "    'vf_loss': 194.96442},\n",
              "   'num_steps_sampled': 475040,\n",
              "   'num_steps_trained': 475040},\n",
              "  'iterations_since_restore': 350,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.88571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21457645827595628,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8329221698979017,\n",
              "   'mean_inference_ms': 4.538021899230398,\n",
              "   'mean_raw_obs_processing_ms': 0.5260807231990285},\n",
              "  'time_since_restore': 1725.4001252651215,\n",
              "  'time_this_iter_s': 4.890148878097534,\n",
              "  'time_total_s': 1725.4001252651215,\n",
              "  'timers': {'apply_grad_throughput': 2166.345,\n",
              "   'apply_grad_time_ms': 4.616,\n",
              "   'grad_wait_time_ms': 41.798,\n",
              "   'update_time_ms': 8.133},\n",
              "  'timestamp': 1635589342,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 475040,\n",
              "  'training_iteration': 350},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-27',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 705.58,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.330860281703554,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2827,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [615,\n",
              "    949,\n",
              "    731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998],\n",
              "   'episode_reward': [171.54996371638,\n",
              "    114.00846313445976,\n",
              "    -151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.451567,\n",
              "    'policy_loss': -13.500219,\n",
              "    'var_gnorm': 28.529947,\n",
              "    'vf_explained_var': 0.8878736,\n",
              "    'vf_loss': 10.573188},\n",
              "   'num_steps_sampled': 476170,\n",
              "   'num_steps_trained': 476170},\n",
              "  'iterations_since_restore': 351,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.15714285714284, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21490148467521908,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8426600003154323,\n",
              "   'mean_inference_ms': 4.543397877295934,\n",
              "   'mean_raw_obs_processing_ms': 0.5269379010117089},\n",
              "  'time_since_restore': 1730.3261127471924,\n",
              "  'time_this_iter_s': 4.925987482070923,\n",
              "  'time_total_s': 1730.3261127471924,\n",
              "  'timers': {'apply_grad_throughput': 2087.071,\n",
              "   'apply_grad_time_ms': 4.791,\n",
              "   'grad_wait_time_ms': 27.456,\n",
              "   'update_time_ms': 8.297},\n",
              "  'timestamp': 1635589347,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 476170,\n",
              "  'training_iteration': 351},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-32',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 702.35,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 8.239694927739459,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2829,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [731,\n",
              "    1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700],\n",
              "   'episode_reward': [-151.15649330618714,\n",
              "    17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.661601,\n",
              "    'policy_loss': -3.5809834,\n",
              "    'var_gnorm': 28.540396,\n",
              "    'vf_explained_var': 0.8990888,\n",
              "    'vf_loss': 6.76679},\n",
              "   'num_steps_sampled': 477240,\n",
              "   'num_steps_trained': 477240},\n",
              "  'iterations_since_restore': 352,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 99.1, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21513078005537145,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8493511850483002,\n",
              "   'mean_inference_ms': 4.546673916170729,\n",
              "   'mean_raw_obs_processing_ms': 0.5274787398515071},\n",
              "  'time_since_restore': 1735.2664487361908,\n",
              "  'time_this_iter_s': 4.940335988998413,\n",
              "  'time_total_s': 1735.2664487361908,\n",
              "  'timers': {'apply_grad_throughput': 1685.299,\n",
              "   'apply_grad_time_ms': 5.934,\n",
              "   'grad_wait_time_ms': 31.416,\n",
              "   'update_time_ms': 7.478},\n",
              "  'timestamp': 1635589352,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 477240,\n",
              "  'training_iteration': 352},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-38',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 700.64,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 9.421178685642984,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2830,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560],\n",
              "   'episode_reward': [17.59119823106789,\n",
              "    119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 0.7718009,\n",
              "    'policy_loss': -0.06622935,\n",
              "    'var_gnorm': 28.528444,\n",
              "    'vf_explained_var': -0.018887043,\n",
              "    'vf_loss': 1.0954458},\n",
              "   'num_steps_sampled': 478170,\n",
              "   'num_steps_trained': 478170},\n",
              "  'iterations_since_restore': 353,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.68571428571428, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21524832301460933,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8525687773063655,\n",
              "   'mean_inference_ms': 4.54837023750445,\n",
              "   'mean_raw_obs_processing_ms': 0.5277607690950342},\n",
              "  'time_since_restore': 1740.2779529094696,\n",
              "  'time_this_iter_s': 5.011504173278809,\n",
              "  'time_total_s': 1740.2779529094696,\n",
              "  'timers': {'apply_grad_throughput': 1699.385,\n",
              "   'apply_grad_time_ms': 5.884,\n",
              "   'grad_wait_time_ms': 42.428,\n",
              "   'update_time_ms': 8.619},\n",
              "  'timestamp': 1635589358,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 478170,\n",
              "  'training_iteration': 353},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-43',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 698.86,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.623288842415114,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2831,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [750,\n",
              "    1000,\n",
              "    797,\n",
              "    1000,\n",
              "    258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822],\n",
              "   'episode_reward': [119.41710172539652,\n",
              "    -5.367941571589881,\n",
              "    -132.58618989801639,\n",
              "    -49.7911652963818,\n",
              "    -65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.7143135,\n",
              "    'policy_loss': -17.773888,\n",
              "    'var_gnorm': 28.541285,\n",
              "    'vf_explained_var': 0.88142097,\n",
              "    'vf_loss': 22.101412},\n",
              "   'num_steps_sampled': 479160,\n",
              "   'num_steps_trained': 479160},\n",
              "  'iterations_since_restore': 354,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21535919990506208,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8559369394243725,\n",
              "   'mean_inference_ms': 4.549950249667234,\n",
              "   'mean_raw_obs_processing_ms': 0.5280168489155198},\n",
              "  'time_since_restore': 1745.239655971527,\n",
              "  'time_this_iter_s': 4.961703062057495,\n",
              "  'time_total_s': 1745.239655971527,\n",
              "  'timers': {'apply_grad_throughput': 1522.472,\n",
              "   'apply_grad_time_ms': 6.568,\n",
              "   'grad_wait_time_ms': 42.395,\n",
              "   'update_time_ms': 7.202},\n",
              "  'timestamp': 1635589363,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 479160,\n",
              "  'training_iteration': 354},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-48',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 683.53,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.688329389662464,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 4,\n",
              "  'episodes_total': 2835,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [258,\n",
              "    127,\n",
              "    771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375],\n",
              "   'episode_reward': [-65.8637064150155,\n",
              "    -91.18067591882959,\n",
              "    183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 6.988845,\n",
              "    'policy_loss': 22.78747,\n",
              "    'var_gnorm': 28.556732,\n",
              "    'vf_explained_var': 0.6711081,\n",
              "    'vf_loss': 291.1134},\n",
              "   'num_steps_sampled': 480200,\n",
              "   'num_steps_trained': 480200},\n",
              "  'iterations_since_restore': 355,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21579709971208852,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8686458234608188,\n",
              "   'mean_inference_ms': 4.556259739327644,\n",
              "   'mean_raw_obs_processing_ms': 0.5290536221983126},\n",
              "  'time_since_restore': 1750.1293823719025,\n",
              "  'time_this_iter_s': 4.889726400375366,\n",
              "  'time_total_s': 1750.1293823719025,\n",
              "  'timers': {'apply_grad_throughput': 1736.65,\n",
              "   'apply_grad_time_ms': 5.758,\n",
              "   'grad_wait_time_ms': 31.356,\n",
              "   'update_time_ms': 8.611},\n",
              "  'timestamp': 1635589368,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 480200,\n",
              "  'training_iteration': 355},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-53',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 689.59,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 14.692499569295338,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2837,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [771,\n",
              "    921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498],\n",
              "   'episode_reward': [183.02319321802648,\n",
              "    160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 4.2466307,\n",
              "    'policy_loss': 67.95988,\n",
              "    'var_gnorm': 28.559946,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 447.2938},\n",
              "   'num_steps_sampled': 481240,\n",
              "   'num_steps_trained': 481240},\n",
              "  'iterations_since_restore': 356,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.225, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21598259649171106,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8742840599657093,\n",
              "   'mean_inference_ms': 4.559677511855133,\n",
              "   'mean_raw_obs_processing_ms': 0.5295739811466018},\n",
              "  'time_since_restore': 1755.0179572105408,\n",
              "  'time_this_iter_s': 4.888574838638306,\n",
              "  'time_total_s': 1755.0179572105408,\n",
              "  'timers': {'apply_grad_throughput': 1514.222,\n",
              "   'apply_grad_time_ms': 6.604,\n",
              "   'grad_wait_time_ms': 31.523,\n",
              "   'update_time_ms': 9.026},\n",
              "  'timestamp': 1635589373,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 481240,\n",
              "  'training_iteration': 356},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-22-58',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 687.17,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 14.634790639466488,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2838,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [921,\n",
              "    1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529],\n",
              "   'episode_reward': [160.92907857685893,\n",
              "    -25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.6334114,\n",
              "    'policy_loss': -21.140417,\n",
              "    'var_gnorm': 28.563284,\n",
              "    'vf_explained_var': 0.8479221,\n",
              "    'vf_loss': 19.350386},\n",
              "   'num_steps_sampled': 482210,\n",
              "   'num_steps_trained': 482210},\n",
              "  'iterations_since_restore': 357,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.71428571428571, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2160924689971852,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.877327947701682,\n",
              "   'mean_inference_ms': 4.56130902524697,\n",
              "   'mean_raw_obs_processing_ms': 0.5298366293129065},\n",
              "  'time_since_restore': 1760.0039126873016,\n",
              "  'time_this_iter_s': 4.985955476760864,\n",
              "  'time_total_s': 1760.0039126873016,\n",
              "  'timers': {'apply_grad_throughput': 1707.341,\n",
              "   'apply_grad_time_ms': 5.857,\n",
              "   'grad_wait_time_ms': 30.042,\n",
              "   'update_time_ms': 9.018},\n",
              "  'timestamp': 1635589378,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 482210,\n",
              "  'training_iteration': 357},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-03',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 683.04,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.100020348073171,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2839,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508],\n",
              "   'episode_reward': [-25.582182442334766,\n",
              "    -11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 17.585606,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.755919,\n",
              "    'policy_loss': -2.0900934,\n",
              "    'var_gnorm': 28.579357,\n",
              "    'vf_explained_var': -0.06947148,\n",
              "    'vf_loss': 11.703688},\n",
              "   'num_steps_sampled': 483180,\n",
              "   'num_steps_trained': 483180},\n",
              "  'iterations_since_restore': 358,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.95714285714287, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2161947260635586,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8803739375789832,\n",
              "   'mean_inference_ms': 4.562819530168539,\n",
              "   'mean_raw_obs_processing_ms': 0.530081056445883},\n",
              "  'time_since_restore': 1764.9753274917603,\n",
              "  'time_this_iter_s': 4.971414804458618,\n",
              "  'time_total_s': 1764.9753274917603,\n",
              "  'timers': {'apply_grad_throughput': 2133.16,\n",
              "   'apply_grad_time_ms': 4.688,\n",
              "   'grad_wait_time_ms': 40.852,\n",
              "   'update_time_ms': 7.167},\n",
              "  'timestamp': 1635589383,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 483180,\n",
              "  'training_iteration': 358},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-08',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 683.04,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 11.843720344426726,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2840,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    953,\n",
              "    837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000],\n",
              "   'episode_reward': [-11.67436671014537,\n",
              "    129.91776062499514,\n",
              "    69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.527828,\n",
              "    'policy_loss': -19.085232,\n",
              "    'var_gnorm': 28.595934,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 19.612831},\n",
              "   'num_steps_sampled': 484070,\n",
              "   'num_steps_trained': 484070},\n",
              "  'iterations_since_restore': 359,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.18571428571428, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21630434263575374,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8833773478815183,\n",
              "   'mean_inference_ms': 4.564443185929693,\n",
              "   'mean_raw_obs_processing_ms': 0.530344539886367},\n",
              "  'time_since_restore': 1769.8045206069946,\n",
              "  'time_this_iter_s': 4.829193115234375,\n",
              "  'time_total_s': 1769.8045206069946,\n",
              "  'timers': {'apply_grad_throughput': 1452.507,\n",
              "   'apply_grad_time_ms': 6.885,\n",
              "   'grad_wait_time_ms': 31.826,\n",
              "   'update_time_ms': 7.449},\n",
              "  'timestamp': 1635589388,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 484070,\n",
              "  'training_iteration': 359},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-13',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 676.09,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 11.053442876010926,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2842,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [837,\n",
              "    1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973],\n",
              "   'episode_reward': [69.31356371601694,\n",
              "    -35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.645321,\n",
              "    'policy_loss': -11.136492,\n",
              "    'var_gnorm': 28.613428,\n",
              "    'vf_explained_var': 0.7281208,\n",
              "    'vf_loss': 21.133528},\n",
              "   'num_steps_sampled': 485130,\n",
              "   'num_steps_trained': 485130},\n",
              "  'iterations_since_restore': 360,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.68571428571428, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21651151377583477,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8892975175917694,\n",
              "   'mean_inference_ms': 4.56753573501696,\n",
              "   'mean_raw_obs_processing_ms': 0.5308434473819538},\n",
              "  'time_since_restore': 1774.7499451637268,\n",
              "  'time_this_iter_s': 4.945424556732178,\n",
              "  'time_total_s': 1774.7499451637268,\n",
              "  'timers': {'apply_grad_throughput': 1374.605,\n",
              "   'apply_grad_time_ms': 7.275,\n",
              "   'grad_wait_time_ms': 36.026,\n",
              "   'update_time_ms': 8.967},\n",
              "  'timestamp': 1635589393,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 485130,\n",
              "  'training_iteration': 360},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-18',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 673.0,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 9.362730237896505,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2843,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528],\n",
              "   'episode_reward': [-35.77832607196422,\n",
              "    -110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.948758,\n",
              "    'policy_loss': -5.7353096,\n",
              "    'var_gnorm': 28.61516,\n",
              "    'vf_explained_var': 0.36804986,\n",
              "    'vf_loss': 6.7495537},\n",
              "   'num_steps_sampled': 486060,\n",
              "   'num_steps_trained': 486060},\n",
              "  'iterations_since_restore': 361,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.28571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21658518774655708,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8915187644502938,\n",
              "   'mean_inference_ms': 4.569457408514143,\n",
              "   'mean_raw_obs_processing_ms': 0.5311128403263687},\n",
              "  'time_since_restore': 1779.7336807250977,\n",
              "  'time_this_iter_s': 4.98373556137085,\n",
              "  'time_total_s': 1779.7336807250977,\n",
              "  'timers': {'apply_grad_throughput': 1889.845,\n",
              "   'apply_grad_time_ms': 5.291,\n",
              "   'grad_wait_time_ms': 43.84,\n",
              "   'update_time_ms': 7.863},\n",
              "  'timestamp': 1635589398,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 486060,\n",
              "  'training_iteration': 361},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-23',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 673.0,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 9.37645763801919,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2844,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [531,\n",
              "    1000,\n",
              "    340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000],\n",
              "   'episode_reward': [-110.08226704914874,\n",
              "    52.838118236459295,\n",
              "    -70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.049362,\n",
              "    'policy_loss': -21.995857,\n",
              "    'var_gnorm': 28.626757,\n",
              "    'vf_explained_var': 0.6175208,\n",
              "    'vf_loss': 29.058605},\n",
              "   'num_steps_sampled': 486920,\n",
              "   'num_steps_trained': 486920},\n",
              "  'iterations_since_restore': 362,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.775, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.216715323470527,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.8950530712823026,\n",
              "   'mean_inference_ms': 4.5706146899627615,\n",
              "   'mean_raw_obs_processing_ms': 0.53133491207366},\n",
              "  'time_since_restore': 1784.6858367919922,\n",
              "  'time_this_iter_s': 4.952156066894531,\n",
              "  'time_total_s': 1784.6858367919922,\n",
              "  'timers': {'apply_grad_throughput': 1807.89,\n",
              "   'apply_grad_time_ms': 5.531,\n",
              "   'grad_wait_time_ms': 40.309,\n",
              "   'update_time_ms': 6.997},\n",
              "  'timestamp': 1635589403,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 486920,\n",
              "  'training_iteration': 362},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-28',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 671.46,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.050585824687767,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2846,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [340,\n",
              "    976,\n",
              "    777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469],\n",
              "   'episode_reward': [-70.7913830227032,\n",
              "    120.19903557723316,\n",
              "    153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.508007,\n",
              "    'policy_loss': 22.23452,\n",
              "    'var_gnorm': 28.639046,\n",
              "    'vf_explained_var': 0.7190912,\n",
              "    'vf_loss': 37.364597},\n",
              "   'num_steps_sampled': 487930,\n",
              "   'num_steps_trained': 487930},\n",
              "  'iterations_since_restore': 363,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.7, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2169173886257332,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.900729107566606,\n",
              "   'mean_inference_ms': 4.573690630091355,\n",
              "   'mean_raw_obs_processing_ms': 0.5318253356355992},\n",
              "  'time_since_restore': 1789.60742354393,\n",
              "  'time_this_iter_s': 4.921586751937866,\n",
              "  'time_total_s': 1789.60742354393,\n",
              "  'timers': {'apply_grad_throughput': 1246.22,\n",
              "   'apply_grad_time_ms': 8.024,\n",
              "   'grad_wait_time_ms': 37.155,\n",
              "   'update_time_ms': 10.485},\n",
              "  'timestamp': 1635589408,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 487930,\n",
              "  'training_iteration': 363},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-33',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 667.14,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 8.00162305927855,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2848,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [777,\n",
              "    602,\n",
              "    607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503],\n",
              "   'episode_reward': [153.55570439549172,\n",
              "    -55.96050360542937,\n",
              "    -81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999992,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.774112,\n",
              "    'policy_loss': -14.300065,\n",
              "    'var_gnorm': 28.635239,\n",
              "    'vf_explained_var': 0.8077972,\n",
              "    'vf_loss': 14.213185},\n",
              "   'num_steps_sampled': 488930,\n",
              "   'num_steps_trained': 488930},\n",
              "  'iterations_since_restore': 364,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.28571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21711587646757838,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9062716620228375,\n",
              "   'mean_inference_ms': 4.576728276319745,\n",
              "   'mean_raw_obs_processing_ms': 0.5323040274067578},\n",
              "  'time_since_restore': 1794.5630130767822,\n",
              "  'time_this_iter_s': 4.955589532852173,\n",
              "  'time_total_s': 1794.5630130767822,\n",
              "  'timers': {'apply_grad_throughput': 1370.598,\n",
              "   'apply_grad_time_ms': 7.296,\n",
              "   'grad_wait_time_ms': 38.422,\n",
              "   'update_time_ms': 9.917},\n",
              "  'timestamp': 1635589413,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 488930,\n",
              "  'training_iteration': 364},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-38',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 666.0,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 9.636725916012876,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2850,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [607,\n",
              "    538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629],\n",
              "   'episode_reward': [-81.63115263097292,\n",
              "    -109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.229765,\n",
              "    'policy_loss': -19.95076,\n",
              "    'var_gnorm': 28.631687,\n",
              "    'vf_explained_var': 0.7652904,\n",
              "    'vf_loss': 23.003029},\n",
              "   'num_steps_sampled': 489950,\n",
              "   'num_steps_trained': 489950},\n",
              "  'iterations_since_restore': 365,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.15714285714286, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21731070578047051,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9117120298646402,\n",
              "   'mean_inference_ms': 4.579738705353799,\n",
              "   'mean_raw_obs_processing_ms': 0.5327757775371764},\n",
              "  'time_since_restore': 1799.4460380077362,\n",
              "  'time_this_iter_s': 4.8830249309539795,\n",
              "  'time_total_s': 1799.4460380077362,\n",
              "  'timers': {'apply_grad_throughput': 1377.155,\n",
              "   'apply_grad_time_ms': 7.261,\n",
              "   'grad_wait_time_ms': 31.75,\n",
              "   'update_time_ms': 9.789},\n",
              "  'timestamp': 1635589418,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 489950,\n",
              "  'training_iteration': 365},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-43',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 665.13,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 9.433516361048895,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2851,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [538,\n",
              "    1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520],\n",
              "   'episode_reward': [-109.23455212678249,\n",
              "    -5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000008,\n",
              "    'model': {},\n",
              "    'policy_entropy': 0.5839742,\n",
              "    'policy_loss': 0.18672578,\n",
              "    'var_gnorm': 28.633509,\n",
              "    'vf_explained_var': 0.02929908,\n",
              "    'vf_loss': 24.200752},\n",
              "   'num_steps_sampled': 490920,\n",
              "   'num_steps_trained': 490920},\n",
              "  'iterations_since_restore': 366,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2174068073511734,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9144806725637493,\n",
              "   'mean_inference_ms': 4.58122512998331,\n",
              "   'mean_raw_obs_processing_ms': 0.5330068673518606},\n",
              "  'time_since_restore': 1804.40549492836,\n",
              "  'time_this_iter_s': 4.959456920623779,\n",
              "  'time_total_s': 1804.40549492836,\n",
              "  'timers': {'apply_grad_throughput': 2307.771,\n",
              "   'apply_grad_time_ms': 4.333,\n",
              "   'grad_wait_time_ms': 32.295,\n",
              "   'update_time_ms': 11.035},\n",
              "  'timestamp': 1635589423,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 490920,\n",
              "  'training_iteration': 366},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-48',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 669.75,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.547749683066478,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2852,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    911,\n",
              "    1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000],\n",
              "   'episode_reward': [-5.350439006385718,\n",
              "    149.30484655606966,\n",
              "    -11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.938456,\n",
              "    'policy_loss': 13.155279,\n",
              "    'var_gnorm': 28.64391,\n",
              "    'vf_explained_var': 0.7344637,\n",
              "    'vf_loss': 37.20472},\n",
              "   'num_steps_sampled': 491890,\n",
              "   'num_steps_trained': 491890},\n",
              "  'iterations_since_restore': 367,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.69999999999997, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21750601679031334,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9171621710037068,\n",
              "   'mean_inference_ms': 4.582771960210284,\n",
              "   'mean_raw_obs_processing_ms': 0.5332489239909609},\n",
              "  'time_since_restore': 1809.3336024284363,\n",
              "  'time_this_iter_s': 4.928107500076294,\n",
              "  'time_total_s': 1809.3336024284363,\n",
              "  'timers': {'apply_grad_throughput': 1499.626,\n",
              "   'apply_grad_time_ms': 6.668,\n",
              "   'grad_wait_time_ms': 38.079,\n",
              "   'update_time_ms': 10.179},\n",
              "  'timestamp': 1635589428,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 491890,\n",
              "  'training_iteration': 367},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-53',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 665.32,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 8.852814755930641,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2854,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995],\n",
              "   'episode_reward': [-11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.151446,\n",
              "    'policy_loss': 1.1513064,\n",
              "    'var_gnorm': 28.655457,\n",
              "    'vf_explained_var': 0.07853937,\n",
              "    'vf_loss': 9.466843},\n",
              "   'num_steps_sampled': 492800,\n",
              "   'num_steps_trained': 492800},\n",
              "  'iterations_since_restore': 368,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6857142857143, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21770128186662363,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9223709335238164,\n",
              "   'mean_inference_ms': 4.585818383241551,\n",
              "   'mean_raw_obs_processing_ms': 0.5337175872720752},\n",
              "  'time_since_restore': 1814.2474133968353,\n",
              "  'time_this_iter_s': 4.913810968399048,\n",
              "  'time_total_s': 1814.2474133968353,\n",
              "  'timers': {'apply_grad_throughput': 1490.296,\n",
              "   'apply_grad_time_ms': 6.71,\n",
              "   'grad_wait_time_ms': 29.21,\n",
              "   'update_time_ms': 11.536},\n",
              "  'timestamp': 1635589433,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 492800,\n",
              "  'training_iteration': 368},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-23-58',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 665.32,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 8.852814755930641,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2854,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    808,\n",
              "    675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995],\n",
              "   'episode_reward': [-11.986947899627227,\n",
              "    119.28342042804393,\n",
              "    -132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.724162,\n",
              "    'policy_loss': 5.4332414,\n",
              "    'var_gnorm': 28.656841,\n",
              "    'vf_explained_var': 0.87760204,\n",
              "    'vf_loss': 5.0609837},\n",
              "   'num_steps_sampled': 493690,\n",
              "   'num_steps_trained': 493690},\n",
              "  'iterations_since_restore': 369,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 97.85, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21770128186662363,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9223709335238164,\n",
              "   'mean_inference_ms': 4.585818383241551,\n",
              "   'mean_raw_obs_processing_ms': 0.5337175872720752},\n",
              "  'time_since_restore': 1819.25137758255,\n",
              "  'time_this_iter_s': 5.003964185714722,\n",
              "  'time_total_s': 1819.25137758255,\n",
              "  'timers': {'apply_grad_throughput': 1720.627,\n",
              "   'apply_grad_time_ms': 5.812,\n",
              "   'grad_wait_time_ms': 53.103,\n",
              "   'update_time_ms': 8.032},\n",
              "  'timestamp': 1635589438,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 493690,\n",
              "  'training_iteration': 369},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-03',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 663.91,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.533694041346932,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2856,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [675,\n",
              "    1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793],\n",
              "   'episode_reward': [-132.23006283896626,\n",
              "    -56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.079299,\n",
              "    'policy_loss': -17.136333,\n",
              "    'var_gnorm': 28.659725,\n",
              "    'vf_explained_var': 0.12968057,\n",
              "    'vf_loss': 49.739315},\n",
              "   'num_steps_sampled': 494620,\n",
              "   'num_steps_trained': 494620},\n",
              "  'iterations_since_restore': 370,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.60000000000001, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21789780383230503,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.927511894811466,\n",
              "   'mean_inference_ms': 4.5888759066836515,\n",
              "   'mean_raw_obs_processing_ms': 0.5341890746366622},\n",
              "  'time_since_restore': 1824.1753644943237,\n",
              "  'time_this_iter_s': 4.923986911773682,\n",
              "  'time_total_s': 1824.1753644943237,\n",
              "  'timers': {'apply_grad_throughput': 1811.192,\n",
              "   'apply_grad_time_ms': 5.521,\n",
              "   'grad_wait_time_ms': 30.132,\n",
              "   'update_time_ms': 6.061},\n",
              "  'timestamp': 1635589443,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 494620,\n",
              "  'training_iteration': 370},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-08',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 662.97,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.981656152790167,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2857,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    648,\n",
              "    259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581],\n",
              "   'episode_reward': [-56.33843347600606,\n",
              "    -60.395036185581375,\n",
              "    -69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.607686,\n",
              "    'policy_loss': -17.898834,\n",
              "    'var_gnorm': 28.649189,\n",
              "    'vf_explained_var': 0.84343284,\n",
              "    'vf_loss': 18.465548},\n",
              "   'num_steps_sampled': 495580,\n",
              "   'num_steps_trained': 495580},\n",
              "  'iterations_since_restore': 371,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.57142857142856, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21796959346071734,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9293277090248406,\n",
              "   'mean_inference_ms': 4.590800577588226,\n",
              "   'mean_raw_obs_processing_ms': 0.5344457546011546},\n",
              "  'time_since_restore': 1829.1294367313385,\n",
              "  'time_this_iter_s': 4.9540722370147705,\n",
              "  'time_total_s': 1829.1294367313385,\n",
              "  'timers': {'apply_grad_throughput': 1579.056,\n",
              "   'apply_grad_time_ms': 6.333,\n",
              "   'grad_wait_time_ms': 44.151,\n",
              "   'update_time_ms': 6.069},\n",
              "  'timestamp': 1635589448,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 495580,\n",
              "  'training_iteration': 371},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-13',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 659.8,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 15.254722994596937,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2859,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [259,\n",
              "    716,\n",
              "    929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912],\n",
              "   'episode_reward': [-69.02453135953968,\n",
              "    -121.45174099053757,\n",
              "    137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.133332,\n",
              "    'policy_loss': -23.170397,\n",
              "    'var_gnorm': 28.656893,\n",
              "    'vf_explained_var': 0.72717446,\n",
              "    'vf_loss': 37.633068},\n",
              "   'num_steps_sampled': 496570,\n",
              "   'num_steps_trained': 496570},\n",
              "  'iterations_since_restore': 372,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142858, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21816592389201422,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9343607499646331,\n",
              "   'mean_inference_ms': 4.593808020354913,\n",
              "   'mean_raw_obs_processing_ms': 0.5349097079915544},\n",
              "  'time_since_restore': 1834.0210008621216,\n",
              "  'time_this_iter_s': 4.891564130783081,\n",
              "  'time_total_s': 1834.0210008621216,\n",
              "  'timers': {'apply_grad_throughput': 1823.476,\n",
              "   'apply_grad_time_ms': 5.484,\n",
              "   'grad_wait_time_ms': 28.582,\n",
              "   'update_time_ms': 8.856},\n",
              "  'timestamp': 1635589453,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 496570,\n",
              "  'training_iteration': 372},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-18',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 663.13,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.228777227910427,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2861,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599],\n",
              "   'episode_reward': [137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.375627,\n",
              "    'policy_loss': 57.16136,\n",
              "    'var_gnorm': 28.665918,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 245.21024},\n",
              "   'num_steps_sampled': 497670,\n",
              "   'num_steps_trained': 497670},\n",
              "  'iterations_since_restore': 373,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.57142857142857, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21836072799319298,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.939353459720599,\n",
              "   'mean_inference_ms': 4.596808529703059,\n",
              "   'mean_raw_obs_processing_ms': 0.5353739977426746},\n",
              "  'time_since_restore': 1838.976569890976,\n",
              "  'time_this_iter_s': 4.95556902885437,\n",
              "  'time_total_s': 1838.976569890976,\n",
              "  'timers': {'apply_grad_throughput': 1291.676,\n",
              "   'apply_grad_time_ms': 7.742,\n",
              "   'grad_wait_time_ms': 24.276,\n",
              "   'update_time_ms': 10.272},\n",
              "  'timestamp': 1635589458,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 497670,\n",
              "  'training_iteration': 373},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-23',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 663.13,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.228777227910427,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 0,\n",
              "  'episodes_total': 2861,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [929,\n",
              "    1000,\n",
              "    490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599],\n",
              "   'episode_reward': [137.06729386484227,\n",
              "    -34.545326010240295,\n",
              "    -83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.540277,\n",
              "    'policy_loss': 25.706427,\n",
              "    'var_gnorm': 28.656961,\n",
              "    'vf_explained_var': -0.05844021,\n",
              "    'vf_loss': 64.59333},\n",
              "   'num_steps_sampled': 498600,\n",
              "   'num_steps_trained': 498600},\n",
              "  'iterations_since_restore': 374,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.1875, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21836072799319298,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.939353459720599,\n",
              "   'mean_inference_ms': 4.596808529703059,\n",
              "   'mean_raw_obs_processing_ms': 0.5353739977426746},\n",
              "  'time_since_restore': 1843.9779696464539,\n",
              "  'time_this_iter_s': 5.001399755477905,\n",
              "  'time_total_s': 1843.9779696464539,\n",
              "  'timers': {'apply_grad_throughput': 1953.438,\n",
              "   'apply_grad_time_ms': 5.119,\n",
              "   'grad_wait_time_ms': 53.207,\n",
              "   'update_time_ms': 7.649},\n",
              "  'timestamp': 1635589463,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 498600,\n",
              "  'training_iteration': 374},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-28',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 661.75,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.40755659933388,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2863,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [490,\n",
              "    569,\n",
              "    439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791],\n",
              "   'episode_reward': [-83.791241775428,\n",
              "    184.9236891076153,\n",
              "    -98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999992,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.65043,\n",
              "    'policy_loss': 1.53008,\n",
              "    'var_gnorm': 28.67376,\n",
              "    'vf_explained_var': 0.92616653,\n",
              "    'vf_loss': 2.06964},\n",
              "   'num_steps_sampled': 499530,\n",
              "   'num_steps_trained': 499530},\n",
              "  'iterations_since_restore': 375,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21855246918813337,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9442500923450823,\n",
              "   'mean_inference_ms': 4.599789442571797,\n",
              "   'mean_raw_obs_processing_ms': 0.5358370516062583},\n",
              "  'time_since_restore': 1848.8715102672577,\n",
              "  'time_this_iter_s': 4.893540620803833,\n",
              "  'time_total_s': 1848.8715102672577,\n",
              "  'timers': {'apply_grad_throughput': 1948.492,\n",
              "   'apply_grad_time_ms': 5.132,\n",
              "   'grad_wait_time_ms': 31.59,\n",
              "   'update_time_ms': 6.616},\n",
              "  'timestamp': 1635589468,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 499530,\n",
              "  'training_iteration': 375},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-33',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 659.31,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 11.897679418298189,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2865,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [439,\n",
              "    249,\n",
              "    864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502],\n",
              "   'episode_reward': [-98.29178666539808,\n",
              "    -103.45170643332818,\n",
              "    -122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000008,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.952327,\n",
              "    'policy_loss': -41.136276,\n",
              "    'var_gnorm': 28.683384,\n",
              "    'vf_explained_var': 0.5763593,\n",
              "    'vf_loss': 83.574615},\n",
              "   'num_steps_sampled': 500560,\n",
              "   'num_steps_trained': 500560},\n",
              "  'iterations_since_restore': 376,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142858, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2187424512732847,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.949117207615385,\n",
              "   'mean_inference_ms': 4.60276805198384,\n",
              "   'mean_raw_obs_processing_ms': 0.536299600963884},\n",
              "  'time_since_restore': 1853.847086906433,\n",
              "  'time_this_iter_s': 4.975576639175415,\n",
              "  'time_total_s': 1853.847086906433,\n",
              "  'timers': {'apply_grad_throughput': 1498.758,\n",
              "   'apply_grad_time_ms': 6.672,\n",
              "   'grad_wait_time_ms': 38.447,\n",
              "   'update_time_ms': 11.421},\n",
              "  'timestamp': 1635589473,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 500560,\n",
              "  'training_iteration': 376},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-38',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 663.8,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.986857687316649,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2867,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [864,\n",
              "    645,\n",
              "    603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677],\n",
              "   'episode_reward': [-122.52913363565061,\n",
              "    -91.58853170800799,\n",
              "    159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.8781915,\n",
              "    'policy_loss': -16.707075,\n",
              "    'var_gnorm': 28.688845,\n",
              "    'vf_explained_var': 0.9546016,\n",
              "    'vf_loss': 22.205112},\n",
              "   'num_steps_sampled': 501600,\n",
              "   'num_steps_trained': 501600},\n",
              "  'iterations_since_restore': 377,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.4, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2189525015139769,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9547082326471683,\n",
              "   'mean_inference_ms': 4.605262866772834,\n",
              "   'mean_raw_obs_processing_ms': 0.5367312144239854},\n",
              "  'time_since_restore': 1858.854445695877,\n",
              "  'time_this_iter_s': 5.00735878944397,\n",
              "  'time_total_s': 1858.854445695877,\n",
              "  'timers': {'apply_grad_throughput': 2002.217,\n",
              "   'apply_grad_time_ms': 4.994,\n",
              "   'grad_wait_time_ms': 36.351,\n",
              "   'update_time_ms': 9.762},\n",
              "  'timestamp': 1635589478,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 501600,\n",
              "  'training_iteration': 377},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-43',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 658.85,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 13.810725535458714,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2869,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [603,\n",
              "    992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476],\n",
              "   'episode_reward': [159.39874865209487,\n",
              "    139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.475498,\n",
              "    'policy_loss': -11.1272745,\n",
              "    'var_gnorm': 28.70611,\n",
              "    'vf_explained_var': 0.7495103,\n",
              "    'vf_loss': 10.195084},\n",
              "   'num_steps_sampled': 502620,\n",
              "   'num_steps_trained': 502620},\n",
              "  'iterations_since_restore': 378,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.18571428571428, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21913843902842822,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.959508738381298,\n",
              "   'mean_inference_ms': 4.608202410909378,\n",
              "   'mean_raw_obs_processing_ms': 0.5371892134641867},\n",
              "  'time_since_restore': 1863.7365238666534,\n",
              "  'time_this_iter_s': 4.882078170776367,\n",
              "  'time_total_s': 1863.7365238666534,\n",
              "  'timers': {'apply_grad_throughput': 1433.681,\n",
              "   'apply_grad_time_ms': 6.975,\n",
              "   'grad_wait_time_ms': 34.676,\n",
              "   'update_time_ms': 8.32},\n",
              "  'timestamp': 1635589483,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 502620,\n",
              "  'training_iteration': 378},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-48',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 659.56,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.681970712407203,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2870,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [992,\n",
              "    1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674],\n",
              "   'episode_reward': [139.74174978220242,\n",
              "    -31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.85857,\n",
              "    'policy_loss': -13.494415,\n",
              "    'var_gnorm': 28.686346,\n",
              "    'vf_explained_var': 0.86440164,\n",
              "    'vf_loss': 8.588208},\n",
              "   'num_steps_sampled': 503600,\n",
              "   'num_steps_trained': 503600},\n",
              "  'iterations_since_restore': 379,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.17142857142856, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2192098646301408,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.961123334415248,\n",
              "   'mean_inference_ms': 4.610119207692255,\n",
              "   'mean_raw_obs_processing_ms': 0.5374434101328174},\n",
              "  'time_since_restore': 1868.7047061920166,\n",
              "  'time_this_iter_s': 4.968182325363159,\n",
              "  'time_total_s': 1868.7047061920166,\n",
              "  'timers': {'apply_grad_throughput': 1594.162,\n",
              "   'apply_grad_time_ms': 6.273,\n",
              "   'grad_wait_time_ms': 33.449,\n",
              "   'update_time_ms': 8.649},\n",
              "  'timestamp': 1635589488,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 503600,\n",
              "  'training_iteration': 379},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-53',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 658.32,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.755420054487711,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2871,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    514,\n",
              "    616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868],\n",
              "   'episode_reward': [-31.10752823669059,\n",
              "    -44.693629461661246,\n",
              "    -129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.1003,\n",
              "    'policy_loss': -13.7879,\n",
              "    'var_gnorm': 28.69002,\n",
              "    'vf_explained_var': 0.73727643,\n",
              "    'vf_loss': 9.805937},\n",
              "   'num_steps_sampled': 504530,\n",
              "   'num_steps_trained': 504530},\n",
              "  'iterations_since_restore': 380,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21932417724575515,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9641835186471288,\n",
              "   'mean_inference_ms': 4.6111446123256385,\n",
              "   'mean_raw_obs_processing_ms': 0.5376477353723909},\n",
              "  'time_since_restore': 1873.6270155906677,\n",
              "  'time_this_iter_s': 4.922309398651123,\n",
              "  'time_total_s': 1873.6270155906677,\n",
              "  'timers': {'apply_grad_throughput': 1540.562,\n",
              "   'apply_grad_time_ms': 6.491,\n",
              "   'grad_wait_time_ms': 32.573,\n",
              "   'update_time_ms': 9.233},\n",
              "  'timestamp': 1635589493,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 504530,\n",
              "  'training_iteration': 380},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-24-58',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 655.05,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.317204494316126,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2873,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [616,\n",
              "    726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399],\n",
              "   'episode_reward': [-129.97057405496332,\n",
              "    174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 30.612074,\n",
              "    'model': {},\n",
              "    'policy_entropy': 10.067242,\n",
              "    'policy_loss': -2.089009,\n",
              "    'var_gnorm': 28.6993,\n",
              "    'vf_explained_var': 0.8991955,\n",
              "    'vf_loss': 1.7075454},\n",
              "   'num_steps_sampled': 505490,\n",
              "   'num_steps_trained': 505490},\n",
              "  'iterations_since_restore': 381,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.9375, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.219462107720304,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9672498456473284,\n",
              "   'mean_inference_ms': 4.6149406925351615,\n",
              "   'mean_raw_obs_processing_ms': 0.5381464869477002},\n",
              "  'time_since_restore': 1878.5768644809723,\n",
              "  'time_this_iter_s': 4.949848890304565,\n",
              "  'time_total_s': 1878.5768644809723,\n",
              "  'timers': {'apply_grad_throughput': 1947.036,\n",
              "   'apply_grad_time_ms': 5.136,\n",
              "   'grad_wait_time_ms': 37.982,\n",
              "   'update_time_ms': 8.578},\n",
              "  'timestamp': 1635589498,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 505490,\n",
              "  'training_iteration': 381},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-03',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 657.88,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.748597413225129,\n",
              "  'episode_reward_min': -244.74292672970006,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2874,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [726,\n",
              "    763,\n",
              "    824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899],\n",
              "   'episode_reward': [174.7654192377188,\n",
              "    -244.74292672970006,\n",
              "    148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 0.48067915,\n",
              "    'policy_loss': -0.03356637,\n",
              "    'var_gnorm': 28.709455,\n",
              "    'vf_explained_var': 1.8119812e-05,\n",
              "    'vf_loss': 2.2097282},\n",
              "   'num_steps_sampled': 506260,\n",
              "   'num_steps_trained': 506260},\n",
              "  'iterations_since_restore': 382,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.47142857142856, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.21957761817349222,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9704212353156967,\n",
              "   'mean_inference_ms': 4.615935945409224,\n",
              "   'mean_raw_obs_processing_ms': 0.5383460099542049},\n",
              "  'time_since_restore': 1883.48011302948,\n",
              "  'time_this_iter_s': 4.90324854850769,\n",
              "  'time_total_s': 1883.48011302948,\n",
              "  'timers': {'apply_grad_throughput': 1863.903,\n",
              "   'apply_grad_time_ms': 5.365,\n",
              "   'grad_wait_time_ms': 65.504,\n",
              "   'update_time_ms': 8.665},\n",
              "  'timestamp': 1635589503,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 506260,\n",
              "  'training_iteration': 382},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-08',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 654.27,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 14.174812604272695,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2876,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [824,\n",
              "    718,\n",
              "    787,\n",
              "    763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406],\n",
              "   'episode_reward': [148.80599812321236,\n",
              "    110.410221776811,\n",
              "    -180.29758095522914,\n",
              "    145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 39.999996,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.292858,\n",
              "    'policy_loss': -8.1308,\n",
              "    'var_gnorm': 28.704483,\n",
              "    'vf_explained_var': 0.8956635,\n",
              "    'vf_loss': 2.8495178},\n",
              "   'num_steps_sampled': 507380,\n",
              "   'num_steps_trained': 507380},\n",
              "  'iterations_since_restore': 383,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.38571428571427, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2197621920757515,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9750554506780713,\n",
              "   'mean_inference_ms': 4.618774066837717,\n",
              "   'mean_raw_obs_processing_ms': 0.538788173640028},\n",
              "  'time_since_restore': 1888.4047451019287,\n",
              "  'time_this_iter_s': 4.9246320724487305,\n",
              "  'time_total_s': 1888.4047451019287,\n",
              "  'timers': {'apply_grad_throughput': 1821.164,\n",
              "   'apply_grad_time_ms': 5.491,\n",
              "   'grad_wait_time_ms': 37.83,\n",
              "   'update_time_ms': 9.382},\n",
              "  'timestamp': 1635589508,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 507380,\n",
              "  'training_iteration': 383},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-13',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 645.38,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 12.99346876408916,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2879,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [763,\n",
              "    811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273],\n",
              "   'episode_reward': [145.2158290877016,\n",
              "    187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 26.273712,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.748559,\n",
              "    'policy_loss': -2.7871609,\n",
              "    'var_gnorm': 28.710344,\n",
              "    'vf_explained_var': 0.74299264,\n",
              "    'vf_loss': 5.626816},\n",
              "   'num_steps_sampled': 508370,\n",
              "   'num_steps_trained': 508370},\n",
              "  'iterations_since_restore': 384,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6857142857143, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22006112502853917,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9826079858783856,\n",
              "   'mean_inference_ms': 4.622491692070967,\n",
              "   'mean_raw_obs_processing_ms': 0.5394167464458157},\n",
              "  'time_since_restore': 1893.3446271419525,\n",
              "  'time_this_iter_s': 4.939882040023804,\n",
              "  'time_total_s': 1893.3446271419525,\n",
              "  'timers': {'apply_grad_throughput': 1862.537,\n",
              "   'apply_grad_time_ms': 5.369,\n",
              "   'grad_wait_time_ms': 26.822,\n",
              "   'update_time_ms': 10.558},\n",
              "  'timestamp': 1635589513,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 508370,\n",
              "  'training_iteration': 384},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-18',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 643.03,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 10.631543351396461,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2880,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [811,\n",
              "    832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528],\n",
              "   'episode_reward': [187.17069248547256,\n",
              "    157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.556036,\n",
              "    'policy_loss': -13.281724,\n",
              "    'var_gnorm': 28.71767,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 39.493324},\n",
              "   'num_steps_sampled': 509420,\n",
              "   'num_steps_trained': 509420},\n",
              "  'iterations_since_restore': 385,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.1857142857143, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22015153167624418,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9848568204939778,\n",
              "   'mean_inference_ms': 4.623905338795342,\n",
              "   'mean_raw_obs_processing_ms': 0.5396379600831489},\n",
              "  'time_since_restore': 1898.3573060035706,\n",
              "  'time_this_iter_s': 5.012678861618042,\n",
              "  'time_total_s': 1898.3573060035706,\n",
              "  'timers': {'apply_grad_throughput': 1510.459,\n",
              "   'apply_grad_time_ms': 6.621,\n",
              "   'grad_wait_time_ms': 36.013,\n",
              "   'update_time_ms': 10.092},\n",
              "  'timestamp': 1635589518,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 509420,\n",
              "  'training_iteration': 385},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-23',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 640.3,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 7.757243058082043,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2881,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [832,\n",
              "    537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538],\n",
              "   'episode_reward': [157.33573089775837,\n",
              "    242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.348972,\n",
              "    'policy_loss': -20.460394,\n",
              "    'var_gnorm': 28.721054,\n",
              "    'vf_explained_var': 0.61817586,\n",
              "    'vf_loss': 25.406775},\n",
              "   'num_steps_sampled': 510410,\n",
              "   'num_steps_trained': 510410},\n",
              "  'iterations_since_restore': 386,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.5875, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22026611749308572,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.987820343589729,\n",
              "   'mean_inference_ms': 4.6248220499312,\n",
              "   'mean_raw_obs_processing_ms': 0.5398253198291905},\n",
              "  'time_since_restore': 1903.336587190628,\n",
              "  'time_this_iter_s': 4.979281187057495,\n",
              "  'time_total_s': 1903.336587190628,\n",
              "  'timers': {'apply_grad_throughput': 1686.817,\n",
              "   'apply_grad_time_ms': 5.928,\n",
              "   'grad_wait_time_ms': 38.557,\n",
              "   'update_time_ms': 8.025},\n",
              "  'timestamp': 1635589523,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 510410,\n",
              "  'training_iteration': 386},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-28',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 641.98,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 242.72477831704657,\n",
              "  'episode_reward_mean': 6.215785069237985,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2882,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [537,\n",
              "    604,\n",
              "    660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000],\n",
              "   'episode_reward': [242.72477831704657,\n",
              "    -129.46778631010744,\n",
              "    188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.858859,\n",
              "    'policy_loss': -37.217735,\n",
              "    'var_gnorm': 28.727152,\n",
              "    'vf_explained_var': -1.0,\n",
              "    'vf_loss': 160.45105},\n",
              "   'num_steps_sampled': 511440,\n",
              "   'num_steps_trained': 511440},\n",
              "  'iterations_since_restore': 387,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.37142857142858, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22033060644671998,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.989155484336162,\n",
              "   'mean_inference_ms': 4.626681204462872,\n",
              "   'mean_raw_obs_processing_ms': 0.5400593627633545},\n",
              "  'time_since_restore': 1908.2616918087006,\n",
              "  'time_this_iter_s': 4.92510461807251,\n",
              "  'time_total_s': 1908.2616918087006,\n",
              "  'timers': {'apply_grad_throughput': 2099.903,\n",
              "   'apply_grad_time_ms': 4.762,\n",
              "   'grad_wait_time_ms': 36.65,\n",
              "   'update_time_ms': 6.155},\n",
              "  'timestamp': 1635589528,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 511440,\n",
              "  'training_iteration': 387},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-33',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 643.67,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 228.54552271412243,\n",
              "  'episode_reward_mean': 3.373997715452971,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2884,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [660,\n",
              "    848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677],\n",
              "   'episode_reward': [188.6973643631447,\n",
              "    101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.9108305,\n",
              "    'policy_loss': 5.8737335,\n",
              "    'var_gnorm': 28.735266,\n",
              "    'vf_explained_var': 0.945574,\n",
              "    'vf_loss': 5.328782},\n",
              "   'num_steps_sampled': 512510,\n",
              "   'num_steps_trained': 512510},\n",
              "  'iterations_since_restore': 388,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.58571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22050994622491393,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.9934582089653257,\n",
              "   'mean_inference_ms': 4.6294694276634685,\n",
              "   'mean_raw_obs_processing_ms': 0.5404839923291861},\n",
              "  'time_since_restore': 1913.2010016441345,\n",
              "  'time_this_iter_s': 4.93930983543396,\n",
              "  'time_total_s': 1913.2010016441345,\n",
              "  'timers': {'apply_grad_throughput': 1802.42,\n",
              "   'apply_grad_time_ms': 5.548,\n",
              "   'grad_wait_time_ms': 37.85,\n",
              "   'update_time_ms': 7.274},\n",
              "  'timestamp': 1635589533,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 512510,\n",
              "  'training_iteration': 388},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-39',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 644.73,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 228.54552271412243,\n",
              "  'episode_reward_mean': 2.338659854806202,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2885,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [848,\n",
              "    904,\n",
              "    484,\n",
              "    448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766],\n",
              "   'episode_reward': [101.23341684021565,\n",
              "    -211.45794092086555,\n",
              "    -95.34202703508777,\n",
              "    206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.831517,\n",
              "    'policy_loss': 17.267855,\n",
              "    'var_gnorm': 28.733648,\n",
              "    'vf_explained_var': -0.38503158,\n",
              "    'vf_loss': 33.25834},\n",
              "   'num_steps_sampled': 513500,\n",
              "   'num_steps_trained': 513500},\n",
              "  'iterations_since_restore': 389,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.79999999999998, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22062243002867557,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 1.996424721775737,\n",
              "   'mean_inference_ms': 4.6303876538957125,\n",
              "   'mean_raw_obs_processing_ms': 0.5406730947495771},\n",
              "  'time_since_restore': 1918.2079441547394,\n",
              "  'time_this_iter_s': 5.006942510604858,\n",
              "  'time_total_s': 1918.2079441547394,\n",
              "  'timers': {'apply_grad_throughput': 2032.715,\n",
              "   'apply_grad_time_ms': 4.92,\n",
              "   'grad_wait_time_ms': 37.74,\n",
              "   'update_time_ms': 7.866},\n",
              "  'timestamp': 1635589539,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 513500,\n",
              "  'training_iteration': 389},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-44',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 642.61,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 9.109484114775409,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2888,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [448,\n",
              "    355,\n",
              "    1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727],\n",
              "   'episode_reward': [206.24628394785418,\n",
              "    -37.344419343774206,\n",
              "    7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.530613,\n",
              "    'policy_loss': -6.996665,\n",
              "    'var_gnorm': 28.743525,\n",
              "    'vf_explained_var': 0.59967095,\n",
              "    'vf_loss': 4.9890714},\n",
              "   'num_steps_sampled': 514550,\n",
              "   'num_steps_trained': 514550},\n",
              "  'iterations_since_restore': 390,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.25714285714285, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22086272258713735,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.0019153897560558,\n",
              "   'mean_inference_ms': 4.6349606888369514,\n",
              "   'mean_raw_obs_processing_ms': 0.5413226442327881},\n",
              "  'time_since_restore': 1923.0997967720032,\n",
              "  'time_this_iter_s': 4.891852617263794,\n",
              "  'time_total_s': 1923.0997967720032,\n",
              "  'timers': {'apply_grad_throughput': 1657.618,\n",
              "   'apply_grad_time_ms': 6.033,\n",
              "   'grad_wait_time_ms': 27.767,\n",
              "   'update_time_ms': 8.47},\n",
              "  'timestamp': 1635589544,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 514550,\n",
              "  'training_iteration': 390},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-49',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 647.6,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 3.9677167444164256,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2890,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689],\n",
              "   'episode_reward': [7.917596408116568,\n",
              "    143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.071227,\n",
              "    'policy_loss': -5.9053197,\n",
              "    'var_gnorm': 28.740805,\n",
              "    'vf_explained_var': 0.7873368,\n",
              "    'vf_loss': 4.8068132},\n",
              "   'num_steps_sampled': 515900,\n",
              "   'num_steps_trained': 515900},\n",
              "  'iterations_since_restore': 391,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.45714285714284, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2210536446598332,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.006841403303567,\n",
              "   'mean_inference_ms': 4.6370954470183605,\n",
              "   'mean_raw_obs_processing_ms': 0.5416935130458289},\n",
              "  'time_since_restore': 1927.9711031913757,\n",
              "  'time_this_iter_s': 4.871306419372559,\n",
              "  'time_total_s': 1927.9711031913757,\n",
              "  'timers': {'apply_grad_throughput': 2644.764,\n",
              "   'apply_grad_time_ms': 3.781,\n",
              "   'grad_wait_time_ms': 17.685,\n",
              "   'update_time_ms': 4.42},\n",
              "  'timestamp': 1635589549,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 515900,\n",
              "  'training_iteration': 391},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-54',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 645.87,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 5.54624781765084,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2891,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [796,\n",
              "    241,\n",
              "    853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827],\n",
              "   'episode_reward': [143.16031148285197,\n",
              "    -81.92224995989983,\n",
              "    -154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.207283,\n",
              "    'policy_loss': -10.703926,\n",
              "    'var_gnorm': 28.736288,\n",
              "    'vf_explained_var': 0.8395299,\n",
              "    'vf_loss': 7.8841686},\n",
              "   'num_steps_sampled': 517180,\n",
              "   'num_steps_trained': 517180},\n",
              "  'iterations_since_restore': 392,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.2375, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2211125732156041,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.0080815297958496,\n",
              "   'mean_inference_ms': 4.638840572901482,\n",
              "   'mean_raw_obs_processing_ms': 0.5419145011637644},\n",
              "  'time_since_restore': 1933.0089633464813,\n",
              "  'time_this_iter_s': 5.037860155105591,\n",
              "  'time_total_s': 1933.0089633464813,\n",
              "  'timers': {'apply_grad_throughput': 2227.364,\n",
              "   'apply_grad_time_ms': 4.49,\n",
              "   'grad_wait_time_ms': 39.778,\n",
              "   'update_time_ms': 5.285},\n",
              "  'timestamp': 1635589554,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 517180,\n",
              "  'training_iteration': 392},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-25-59',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 648.6,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 4.897269059357116,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2893,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [853,\n",
              "    257,\n",
              "    463,\n",
              "    975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962],\n",
              "   'episode_reward': [-154.41519062964537,\n",
              "    -65.11284977276063,\n",
              "    131.95491959461881,\n",
              "    164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.373998,\n",
              "    'policy_loss': 2.9037127,\n",
              "    'var_gnorm': 28.764215,\n",
              "    'vf_explained_var': 0.72881067,\n",
              "    'vf_loss': 5.830923},\n",
              "   'num_steps_sampled': 518520,\n",
              "   'num_steps_trained': 518520},\n",
              "  'iterations_since_restore': 393,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.28571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22129145459570573,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.012985965267737,\n",
              "   'mean_inference_ms': 4.640790836808743,\n",
              "   'mean_raw_obs_processing_ms': 0.5422559763283393},\n",
              "  'time_since_restore': 1937.9193160533905,\n",
              "  'time_this_iter_s': 4.91035270690918,\n",
              "  'time_total_s': 1937.9193160533905,\n",
              "  'timers': {'apply_grad_throughput': 2873.913,\n",
              "   'apply_grad_time_ms': 3.48,\n",
              "   'grad_wait_time_ms': 24.553,\n",
              "   'update_time_ms': 6.35},\n",
              "  'timestamp': 1635589559,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 518520,\n",
              "  'training_iteration': 393},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-26-04',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 650.27,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 3.5789531223702165,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2896,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [975,\n",
              "    367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962,\n",
              "    1000,\n",
              "    464,\n",
              "    276],\n",
              "   'episode_reward': [164.0659047110884,\n",
              "    -77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481,\n",
              "    -38.96715205997363,\n",
              "    -76.92853406126368,\n",
              "    -103.5090283852399]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.9347887,\n",
              "    'policy_loss': 27.536531,\n",
              "    'var_gnorm': 28.781624,\n",
              "    'vf_explained_var': -0.46689236,\n",
              "    'vf_loss': 47.669155},\n",
              "   'num_steps_sampled': 520040,\n",
              "   'num_steps_trained': 520040},\n",
              "  'iterations_since_restore': 394,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.28571428571429, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2214879374364444,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.0181171005037184,\n",
              "   'mean_inference_ms': 4.644885444033393,\n",
              "   'mean_raw_obs_processing_ms': 0.5428464699718089},\n",
              "  'time_since_restore': 1942.835773229599,\n",
              "  'time_this_iter_s': 4.916457176208496,\n",
              "  'time_total_s': 1942.835773229599,\n",
              "  'timers': {'apply_grad_throughput': 3133.679,\n",
              "   'apply_grad_time_ms': 3.191,\n",
              "   'grad_wait_time_ms': 21.751,\n",
              "   'update_time_ms': 5.233},\n",
              "  'timestamp': 1635589564,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 520040,\n",
              "  'training_iteration': 394},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-26-09',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 644.87,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 1.4916526863627553,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2897,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [367,\n",
              "    906,\n",
              "    714,\n",
              "    1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962,\n",
              "    1000,\n",
              "    464,\n",
              "    276,\n",
              "    435],\n",
              "   'episode_reward': [-77.51764269936815,\n",
              "    145.76448668488737,\n",
              "    -88.2291604132241,\n",
              "    -96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481,\n",
              "    -38.96715205997363,\n",
              "    -76.92853406126368,\n",
              "    -103.5090283852399,\n",
              "    -44.66413888965768]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.023875,\n",
              "    'policy_loss': 9.65281,\n",
              "    'var_gnorm': 28.783175,\n",
              "    'vf_explained_var': 0.90471315,\n",
              "    'vf_loss': 21.28712},\n",
              "   'num_steps_sampled': 521380,\n",
              "   'num_steps_trained': 521380},\n",
              "  'iterations_since_restore': 395,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.4857142857143, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.2215598767956193,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.0199786281129977,\n",
              "   'mean_inference_ms': 4.645974821565068,\n",
              "   'mean_raw_obs_processing_ms': 0.5430041433137288},\n",
              "  'time_since_restore': 1947.7688145637512,\n",
              "  'time_this_iter_s': 4.933041334152222,\n",
              "  'time_total_s': 1947.7688145637512,\n",
              "  'timers': {'apply_grad_throughput': 2629.064,\n",
              "   'apply_grad_time_ms': 3.804,\n",
              "   'grad_wait_time_ms': 31.834,\n",
              "   'update_time_ms': 4.781},\n",
              "  'timestamp': 1635589569,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 521380,\n",
              "  'training_iteration': 395},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-26-14',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 649.66,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 4.163112077657197,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2900,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [1000,\n",
              "    1000,\n",
              "    584,\n",
              "    441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962,\n",
              "    1000,\n",
              "    464,\n",
              "    276,\n",
              "    435,\n",
              "    1000,\n",
              "    874,\n",
              "    592],\n",
              "   'episode_reward': [-96.64274290668425,\n",
              "    -68.78846146249592,\n",
              "    -100.1297630436352,\n",
              "    -105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481,\n",
              "    -38.96715205997363,\n",
              "    -76.92853406126368,\n",
              "    -103.5090283852399,\n",
              "    -44.66413888965768,\n",
              "    -9.245000387713434,\n",
              "    96.63735562037945,\n",
              "    159.77126746907345]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.877564,\n",
              "    'policy_loss': -35.35548,\n",
              "    'var_gnorm': 28.78566,\n",
              "    'vf_explained_var': 0.55187833,\n",
              "    'vf_loss': 74.69565},\n",
              "   'num_steps_sampled': 522880,\n",
              "   'num_steps_trained': 522880},\n",
              "  'iterations_since_restore': 396,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6857142857143, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22178744936826386,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.026507020918033,\n",
              "   'mean_inference_ms': 4.648732046900356,\n",
              "   'mean_raw_obs_processing_ms': 0.5434767618372094},\n",
              "  'time_since_restore': 1952.7297177314758,\n",
              "  'time_this_iter_s': 4.960903167724609,\n",
              "  'time_total_s': 1952.7297177314758,\n",
              "  'timers': {'apply_grad_throughput': 2094.68,\n",
              "   'apply_grad_time_ms': 4.774,\n",
              "   'grad_wait_time_ms': 18.496,\n",
              "   'update_time_ms': 5.425},\n",
              "  'timestamp': 1635589574,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 522880,\n",
              "  'training_iteration': 396},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-26-19',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 639.98,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 5.524535217385658,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 3,\n",
              "  'episodes_total': 2903,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [441,\n",
              "    313,\n",
              "    713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962,\n",
              "    1000,\n",
              "    464,\n",
              "    276,\n",
              "    435,\n",
              "    1000,\n",
              "    874,\n",
              "    592,\n",
              "    755,\n",
              "    245,\n",
              "    616],\n",
              "   'episode_reward': [-105.02590994308191,\n",
              "    -87.77465136363963,\n",
              "    185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481,\n",
              "    -38.96715205997363,\n",
              "    -76.92853406126368,\n",
              "    -103.5090283852399,\n",
              "    -44.66413888965768,\n",
              "    -9.245000387713434,\n",
              "    96.63735562037945,\n",
              "    159.77126746907345,\n",
              "    -192.10120322333472,\n",
              "    -115.73467654971623,\n",
              "    178.4172263330817]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 7.949495,\n",
              "    'policy_loss': 22.814268,\n",
              "    'var_gnorm': 28.7959,\n",
              "    'vf_explained_var': 0.87387973,\n",
              "    'vf_loss': 33.118202},\n",
              "   'num_steps_sampled': 524210,\n",
              "   'num_steps_trained': 524210},\n",
              "  'iterations_since_restore': 397,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 99.2, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22197661529993087,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.0319533011628037,\n",
              "   'mean_inference_ms': 4.6518115184695015,\n",
              "   'mean_raw_obs_processing_ms': 0.5439494126907914},\n",
              "  'time_since_restore': 1957.6909992694855,\n",
              "  'time_this_iter_s': 4.9612815380096436,\n",
              "  'time_total_s': 1957.6909992694855,\n",
              "  'timers': {'apply_grad_throughput': 1930.857,\n",
              "   'apply_grad_time_ms': 5.179,\n",
              "   'grad_wait_time_ms': 27.539,\n",
              "   'update_time_ms': 5.329},\n",
              "  'timestamp': 1635589579,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 524210,\n",
              "  'training_iteration': 397},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-26-24',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 642.35,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 10.67337078200971,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 2,\n",
              "  'episodes_total': 2905,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [713,\n",
              "    383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962,\n",
              "    1000,\n",
              "    464,\n",
              "    276,\n",
              "    435,\n",
              "    1000,\n",
              "    874,\n",
              "    592,\n",
              "    755,\n",
              "    245,\n",
              "    616,\n",
              "    489,\n",
              "    502],\n",
              "   'episode_reward': [185.6075738899477,\n",
              "    -84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481,\n",
              "    -38.96715205997363,\n",
              "    -76.92853406126368,\n",
              "    -103.5090283852399,\n",
              "    -44.66413888965768,\n",
              "    -9.245000387713434,\n",
              "    96.63735562037945,\n",
              "    159.77126746907345,\n",
              "    -192.10120322333472,\n",
              "    -115.73467654971623,\n",
              "    178.4172263330817,\n",
              "    146.56141538661302,\n",
              "    175.52157976907057]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.000004,\n",
              "    'model': {},\n",
              "    'policy_entropy': 9.51801,\n",
              "    'policy_loss': -13.692656,\n",
              "    'var_gnorm': 28.814629,\n",
              "    'vf_explained_var': 0.77550507,\n",
              "    'vf_loss': 11.26666},\n",
              "   'num_steps_sampled': 525720,\n",
              "   'num_steps_trained': 525720},\n",
              "  'iterations_since_restore': 398,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 99.2, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22207312289450448,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.034738006863932,\n",
              "   'mean_inference_ms': 4.654249313515805,\n",
              "   'mean_raw_obs_processing_ms': 0.5442780689930008},\n",
              "  'time_since_restore': 1962.6374788284302,\n",
              "  'time_this_iter_s': 4.946479558944702,\n",
              "  'time_total_s': 1962.6374788284302,\n",
              "  'timers': {'apply_grad_throughput': 2739.728,\n",
              "   'apply_grad_time_ms': 3.65,\n",
              "   'grad_wait_time_ms': 25.896,\n",
              "   'update_time_ms': 6.518},\n",
              "  'timestamp': 1635589584,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 525720,\n",
              "  'training_iteration': 398},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-26-29',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 644.23,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 6.554902971478895,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2906,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [383,\n",
              "    689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962,\n",
              "    1000,\n",
              "    464,\n",
              "    276,\n",
              "    435,\n",
              "    1000,\n",
              "    874,\n",
              "    592,\n",
              "    755,\n",
              "    245,\n",
              "    616,\n",
              "    489,\n",
              "    502,\n",
              "    901],\n",
              "   'episode_reward': [-84.15097787324036,\n",
              "    -141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481,\n",
              "    -38.96715205997363,\n",
              "    -76.92853406126368,\n",
              "    -103.5090283852399,\n",
              "    -44.66413888965768,\n",
              "    -9.245000387713434,\n",
              "    96.63735562037945,\n",
              "    159.77126746907345,\n",
              "    -192.10120322333472,\n",
              "    -115.73467654971623,\n",
              "    178.4172263330817,\n",
              "    146.56141538661302,\n",
              "    175.52157976907057,\n",
              "    -226.23920716313378]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.123106,\n",
              "    'policy_loss': 4.3859043,\n",
              "    'var_gnorm': 28.828932,\n",
              "    'vf_explained_var': 0.7829537,\n",
              "    'vf_loss': 6.2054944},\n",
              "   'num_steps_sampled': 526990,\n",
              "   'num_steps_trained': 526990},\n",
              "  'iterations_since_restore': 399,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.6875, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22212836554746507,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.0363809644709847,\n",
              "   'mean_inference_ms': 4.655139527039106,\n",
              "   'mean_raw_obs_processing_ms': 0.5444028004984525},\n",
              "  'time_since_restore': 1967.5804562568665,\n",
              "  'time_this_iter_s': 4.942977428436279,\n",
              "  'time_total_s': 1967.5804562568665,\n",
              "  'timers': {'apply_grad_throughput': 2886.81,\n",
              "   'apply_grad_time_ms': 3.464,\n",
              "   'grad_wait_time_ms': 34.536,\n",
              "   'update_time_ms': 4.641},\n",
              "  'timestamp': 1635589589,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 526990,\n",
              "  'training_iteration': 399},\n",
              " {'agent_timesteps_total': 0,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.01,\n",
              "   'env': 'LunarLander-v2',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': 40.0,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 0.0001,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 5,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [256, 256],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': True,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.A3CTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 1,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_workers': 2,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 10,\n",
              "   'sample_async': True,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 200,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_loss_coeff': 0.5},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-30_10-26-34',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 649.12,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 231.1330298662697,\n",
              "  'episode_reward_mean': 9.088296706833269,\n",
              "  'episode_reward_min': -228.60796018024976,\n",
              "  'episodes_this_iter': 1,\n",
              "  'episodes_total': 2907,\n",
              "  'experiment_id': '2d7bed1522834d98b74b294c3d825c91',\n",
              "  'hist_stats': {'episode_lengths': [689,\n",
              "    722,\n",
              "    857,\n",
              "    292,\n",
              "    633,\n",
              "    812,\n",
              "    974,\n",
              "    650,\n",
              "    709,\n",
              "    478,\n",
              "    612,\n",
              "    417,\n",
              "    1000,\n",
              "    314,\n",
              "    815,\n",
              "    823,\n",
              "    1000,\n",
              "    465,\n",
              "    221,\n",
              "    998,\n",
              "    541,\n",
              "    700,\n",
              "    560,\n",
              "    822,\n",
              "    794,\n",
              "    319,\n",
              "    526,\n",
              "    375,\n",
              "    493,\n",
              "    498,\n",
              "    529,\n",
              "    508,\n",
              "    1000,\n",
              "    285,\n",
              "    973,\n",
              "    528,\n",
              "    1000,\n",
              "    908,\n",
              "    469,\n",
              "    381,\n",
              "    503,\n",
              "    636,\n",
              "    629,\n",
              "    520,\n",
              "    1000,\n",
              "    473,\n",
              "    995,\n",
              "    874,\n",
              "    793,\n",
              "    581,\n",
              "    419,\n",
              "    912,\n",
              "    709,\n",
              "    599,\n",
              "    1000,\n",
              "    791,\n",
              "    313,\n",
              "    502,\n",
              "    460,\n",
              "    677,\n",
              "    538,\n",
              "    476,\n",
              "    674,\n",
              "    868,\n",
              "    788,\n",
              "    399,\n",
              "    899,\n",
              "    722,\n",
              "    406,\n",
              "    770,\n",
              "    397,\n",
              "    273,\n",
              "    528,\n",
              "    538,\n",
              "    1000,\n",
              "    633,\n",
              "    677,\n",
              "    766,\n",
              "    923,\n",
              "    374,\n",
              "    727,\n",
              "    613,\n",
              "    689,\n",
              "    827,\n",
              "    348,\n",
              "    962,\n",
              "    1000,\n",
              "    464,\n",
              "    276,\n",
              "    435,\n",
              "    1000,\n",
              "    874,\n",
              "    592,\n",
              "    755,\n",
              "    245,\n",
              "    616,\n",
              "    489,\n",
              "    502,\n",
              "    901,\n",
              "    872],\n",
              "   'episode_reward': [-141.78404972315005,\n",
              "    -141.23839098627096,\n",
              "    190.44085779403088,\n",
              "    -84.64170317268847,\n",
              "    118.80620921937565,\n",
              "    141.3576519198084,\n",
              "    179.9675059221064,\n",
              "    144.92196417738353,\n",
              "    -159.65993682626282,\n",
              "    159.62173321011977,\n",
              "    128.2859316799247,\n",
              "    -106.9637828356025,\n",
              "    -3.7101529453977804,\n",
              "    -117.27101853237889,\n",
              "    85.06294299984224,\n",
              "    121.30780222739298,\n",
              "    -29.43391494719774,\n",
              "    202.18801168321443,\n",
              "    -41.895704859148275,\n",
              "    166.37445193950612,\n",
              "    -119.41239790808912,\n",
              "    -104.1457106374805,\n",
              "    -33.008117515834826,\n",
              "    137.80221390828092,\n",
              "    112.25028880509242,\n",
              "    -51.649187199024134,\n",
              "    228.54552271412243,\n",
              "    -150.9707646360472,\n",
              "    184.2220135501152,\n",
              "    -140.8493779206728,\n",
              "    177.2523002351412,\n",
              "    -92.54795056247215,\n",
              "    -51.212182806979115,\n",
              "    -108.2623024779052,\n",
              "    147.47794955117485,\n",
              "    -99.75770009542532,\n",
              "    -34.40558605969575,\n",
              "    120.74109345963464,\n",
              "    -110.57242360546648,\n",
              "    -93.05541442451569,\n",
              "    -62.43320956187623,\n",
              "    113.53941525505343,\n",
              "    147.56607120844188,\n",
              "    -101.95210812737122,\n",
              "    2.1887800749756714,\n",
              "    -121.28431672658144,\n",
              "    95.745231562682,\n",
              "    115.21470409943149,\n",
              "    160.16969697061435,\n",
              "    212.56614830535725,\n",
              "    -108.55287294198793,\n",
              "    119.1260874610773,\n",
              "    -228.60796018024976,\n",
              "    -164.46288883847853,\n",
              "    -110.44267042728612,\n",
              "    130.8425754242337,\n",
              "    -111.94389221272597,\n",
              "    162.08862144134412,\n",
              "    -140.74433752275164,\n",
              "    147.9186713258711,\n",
              "    -134.72401222179013,\n",
              "    -97.00686830766233,\n",
              "    -153.47673365305607,\n",
              "    147.0866839902533,\n",
              "    48.23152513926023,\n",
              "    -167.8542388547707,\n",
              "    113.16871783593731,\n",
              "    163.20897654921495,\n",
              "    -90.56496493643948,\n",
              "    145.22730802374573,\n",
              "    -97.94933433611794,\n",
              "    -86.49371876118715,\n",
              "    -90.97671218156849,\n",
              "    -100.25933684596895,\n",
              "    3.189932013352246,\n",
              "    -76.64257752525572,\n",
              "    -94.27916584630654,\n",
              "    85.16357829846787,\n",
              "    126.5931101680097,\n",
              "    231.1330298662697,\n",
              "    113.78973484690367,\n",
              "    -143.57972158953686,\n",
              "    -201.6951508422814,\n",
              "    165.77070373155755,\n",
              "    -61.2681865052749,\n",
              "    57.60837219885481,\n",
              "    -38.96715205997363,\n",
              "    -76.92853406126368,\n",
              "    -103.5090283852399,\n",
              "    -44.66413888965768,\n",
              "    -9.245000387713434,\n",
              "    96.63735562037945,\n",
              "    159.77126746907345,\n",
              "    -192.10120322333472,\n",
              "    -115.73467654971623,\n",
              "    178.4172263330817,\n",
              "    146.56141538661302,\n",
              "    175.52157976907057,\n",
              "    -226.23920716313378,\n",
              "    169.18839566219697]},\n",
              "  'hostname': '483eb5ef320d',\n",
              "  'info': {'learner': {'cur_lr': 9.999999747378752e-05,\n",
              "    'grad_gnorm': 40.0,\n",
              "    'model': {},\n",
              "    'policy_entropy': 8.3324585,\n",
              "    'policy_loss': -4.8797636,\n",
              "    'var_gnorm': 28.822899,\n",
              "    'vf_explained_var': 0.44806725,\n",
              "    'vf_loss': 5.0828795},\n",
              "   'num_steps_sampled': 528260,\n",
              "   'num_steps_trained': 528260},\n",
              "  'iterations_since_restore': 400,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 2,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 98.37142857142855, 'ram_util_percent': 21.3},\n",
              "  'pid': 1026,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.22218294721338588,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 2.0382870144258036,\n",
              "   'mean_inference_ms': 4.656066845984996,\n",
              "   'mean_raw_obs_processing_ms': 0.5445511935755953},\n",
              "  'time_since_restore': 1972.5398304462433,\n",
              "  'time_this_iter_s': 4.959374189376831,\n",
              "  'time_total_s': 1972.5398304462433,\n",
              "  'timers': {'apply_grad_throughput': 1942.752,\n",
              "   'apply_grad_time_ms': 5.147,\n",
              "   'grad_wait_time_ms': 37.91,\n",
              "   'update_time_ms': 5.34},\n",
              "  'timestamp': 1635589594,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 528260,\n",
              "  'training_iteration': 400}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7fT1ZzX7emgi",
        "outputId": "ee2ccf46-6900-4967-feda-027072e01b15"
      },
      "source": [
        "# Convert to df and inspect\n",
        "df = pd.DataFrame(data=episode_data)\n",
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>episode_reward_min</th>\n",
              "      <th>episode_reward_mean</th>\n",
              "      <th>episode_reward_max</th>\n",
              "      <th>episode_len_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-268.96956</td>\n",
              "      <td>-101.766928</td>\n",
              "      <td>138.582719</td>\n",
              "      <td>323.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-268.96956</td>\n",
              "      <td>-101.766928</td>\n",
              "      <td>138.582719</td>\n",
              "      <td>323.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-268.96956</td>\n",
              "      <td>-98.509393</td>\n",
              "      <td>138.582719</td>\n",
              "      <td>337.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-268.96956</td>\n",
              "      <td>-96.341148</td>\n",
              "      <td>138.582719</td>\n",
              "      <td>329.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-268.96956</td>\n",
              "      <td>-95.055361</td>\n",
              "      <td>138.582719</td>\n",
              "      <td>330.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>195</td>\n",
              "      <td>-228.60796</td>\n",
              "      <td>4.163112</td>\n",
              "      <td>231.133030</td>\n",
              "      <td>649.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>196</td>\n",
              "      <td>-228.60796</td>\n",
              "      <td>5.524535</td>\n",
              "      <td>231.133030</td>\n",
              "      <td>639.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>197</td>\n",
              "      <td>-228.60796</td>\n",
              "      <td>10.673371</td>\n",
              "      <td>231.133030</td>\n",
              "      <td>642.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>198</td>\n",
              "      <td>-228.60796</td>\n",
              "      <td>6.554903</td>\n",
              "      <td>231.133030</td>\n",
              "      <td>644.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>199</td>\n",
              "      <td>-228.60796</td>\n",
              "      <td>9.088297</td>\n",
              "      <td>231.133030</td>\n",
              "      <td>649.12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       n  episode_reward_min  ...  episode_reward_max  episode_len_mean\n",
              "0      0          -268.96956  ...          138.582719            323.94\n",
              "1      1          -268.96956  ...          138.582719            323.94\n",
              "2      2          -268.96956  ...          138.582719            337.27\n",
              "3      3          -268.96956  ...          138.582719            329.08\n",
              "4      4          -268.96956  ...          138.582719            330.53\n",
              "..   ...                 ...  ...                 ...               ...\n",
              "195  195          -228.60796  ...          231.133030            649.66\n",
              "196  196          -228.60796  ...          231.133030            639.98\n",
              "197  197          -228.60796  ...          231.133030            642.35\n",
              "198  198          -228.60796  ...          231.133030            644.23\n",
              "199  199          -228.60796  ...          231.133030            649.12\n",
              "\n",
              "[200 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O12lx7-ogbiV"
      },
      "source": [
        "Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "T4g0uNPfgdhz",
        "outputId": "1bb7307e-1ceb-4bcd-d819-124e3d1d82c4"
      },
      "source": [
        "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb3ecf60d90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6wOpWfUgqv5"
      },
      "source": [
        "### Print out the policy and model to see the results of training in detail…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzC2fFaNgr9I",
        "outputId": "8f598298-16b0-44b8-ee21-72c710d0d13b"
      },
      "source": [
        "import pprint\n",
        "\n",
        "policy = agent.get_policy()\n",
        "model = policy.model\n",
        "\n",
        "pprint.pprint(model.variables())\n",
        "pprint.pprint(model.value_function())\n",
        "\n",
        "print(model.base_model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(8, 256) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 4) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_out/bias:0' shape=(4,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
            " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
            "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "observations (InputLayer)       [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "fc_1 (Dense)                    (None, 256)          2304        observations[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "fc_out (Dense)                  (None, 4)            1028        fc_2[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "value_out (Dense)               (None, 1)            257         fc_2[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 69,381\n",
            "Trainable params: 69,381\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvjU5Dr6g_p3"
      },
      "source": [
        "### Model Rollout\n",
        "\n",
        "Once we have trained a policy, we deploy it in the environment.\n",
        "\n",
        "A 'Rollout' is the application of the trained policy to the environment. This is, for a given state, the policy function will output the best action to take.\n",
        "\n",
        "****************************************************************************\n",
        "\n",
        "WARNING: The rllib rollout command discussed next won't work in a cloud environment, because it attempts to pop up a window.\n",
        "\n",
        "\n",
        "https://docs.ray.io/en/latest/rllib-concepts.html#policy-evaluation\n",
        "\n",
        "***************************************************************************\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "Next we'll use the RLlib rollout CLI, to evaluate the trained policy.\n",
        "\n",
        "This visualizes the CartPole agent operating within the simulation: moving the cart left or right to avoid having the pole fall over.\n",
        "\n",
        "We'll use the last saved checkpoint, checkpoint_10 (or whatever you set for N_ITER above) for the rollout, evaluated through 2000 steps.\n",
        "\n",
        "    Notes:\n",
        "\n",
        "        If you changed checkpoint_root above to be different than tmp/ppo/cart, then change it here, too. Note that bugs in variable substitution in Jupyter notebooks, we can't use variables in the next cell, unfortunately.\n",
        "        If you changed the model parameters, specifically the fcnet_hiddens array in the config object above, make the same change here.\n",
        "\n",
        "You may need to make one more modification, depending on how you are running this tutorial:\n",
        "\n",
        "    Running on your laptop? - Remove the line --no-render.\n",
        "    Running on the Anyscale Service? The popup windows that would normally be created by the rollout can't be viewed in this case. Hence, the --no-render flag suppresses them. The code cell afterwords provides a sample video. You can try adding --video-dir tmp/ppo/cart, which will generate MP4 videos, then download them to view them. Or copy the Video cell below and use it to view the movies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4kezNVWhRiF"
      },
      "source": [
        "# On the command line, run the following:\n",
        "\n",
        "#!rllib rollout\\\n",
        "# tmp/ppo/cart/checkpoint_10/checkpoint-10 \\\n",
        "#    --config \"{\\\"env\\\": \\\"CartPole-v1\\\", \\\"model\\\": {\\\"fcnet_hiddens\\\": [100, 50]}}\" \\\n",
        "#    --run PPO \\\n",
        "#     --steps 2000"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hofaz0-9GOa"
      },
      "source": [
        "### Tensorboard results\n",
        "Note: one can also use WandB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYBnmkXI9J2P"
      },
      "source": [
        "#From command line:\n",
        "#tensorboard - logdir=$HOME/ray_results/"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvi_08jVhwsQ"
      },
      "source": [
        "### Shut down the service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkVVwtL6hw8k"
      },
      "source": [
        "#ray.shutdown()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_ZX0ceW6Xh"
      },
      "source": [
        "## Ray Paralellization\n",
        "\n",
        "https://github.com/anyscale/academy/blob/64b5b7d149d1dfd3883948bbde0a247b57fbef0c/ray-rllib/explore-rllib/01-Application-Cart-Pole.ipynb\n",
        "\n",
        "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a\n",
        "\n",
        "TL,DR:\n",
        "\n",
        "RAY basically triggers several ROLLOUTS (i.e. episodes) at the same time (by distributing it to many “WORKERS”) so the whole thing runs faster. \"Rollout workers\" collect data in mini batches, then a central worker aggregates these into a single batch of data.\n",
        "\n",
        "LONG ANSWER: https://arxiv.org/pdf/1712.09381.pdf\n",
        "\n",
        "Ray implements a \"centralized parallelization\" where there is a *central scheduler* that sends instructions to the *remote workers*. These workers can, for example, explore the environment and collect rewards (distributed sampling), then the centralized agent will optimize the policy, update parameters, handle the replay buffer, etc.\n",
        "\n",
        "More on Ray Parallelization, at the end of:\n",
        "\n",
        "https://github.com/anyscale/academy/blob/64b5b7d149d1dfd3883948bbde0a247b57fbef0c/ray-rllib/02-Introduction-to-RLlib.ipynb\n",
        ".\n",
        "\n",
        ".\n",
        "### Parallelization parameters:\n",
        "\n",
        "```\n",
        "num_workers -  is the number of actors that the agent will create. It sets the number of CPU processors for parallelization, which determines the degree of parallelism that will be used. In a cluster, these actors will be spread over the available nodes.\n",
        "\n",
        "model  - contains a dictionary of parameters describing the neural net used to parameterize the policy. The fcnet_hiddens parameter is a list of the sizes of the hidden layers. Here, we have two hidden layers of size 100, each.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0jqXw-yrK1o"
      },
      "source": [
        "In short, the entire Ray code for PPO is like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA62frf6rIwo",
        "outputId": "fa8e7093-62b6-467a-83b1-2d5f81f7d7ad"
      },
      "source": [
        "import ray\n",
        "from ray.rllib import agents\n",
        "\n",
        "#Initialize service and pass the number of resources available\n",
        "ray.init(num_cpus = 1,\n",
        "         num_gpus = 0,\n",
        "         ignore_reinit_error = True)\n",
        "\n",
        "\n",
        "config = {'gamma': 0.9,\n",
        "          'lr': 1e-2,\n",
        "          'num_workers':0, #if we set this to 1, we will pin one worker per core, if we set it to 0, it will utilize several workers per core (workers = process)\n",
        "          'train_batch_size': 1000,\n",
        "          'model': {\n",
        "              'fcnet_hiddens': [128, 128]\n",
        "          }}\n",
        "\n",
        "trainer = agents.ppo.PPOTrainer(env='LunarLander-v2', config=config)\n",
        "results = trainer.train()   "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-30 09:36:34,810\tINFO services.py:1252 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
            "2021-10-30 09:36:39,522\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
            "2021-10-30 09:36:39,540\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            "2021-10-30 09:36:41,791\tWARNING deprecation.py:39 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkCnl2QosyH2"
      },
      "source": [
        "### Additional parameters we can pass to the config dict\n",
        "\n",
        "If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger sgd_minibatch_size, a smaller num_sgd_iter, or a larger num_workers.\n",
        "\n",
        "```\n",
        "num_sgd_iter -  is the number of epochs of SGD (stochastic gradient descent, i.e., passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO, for each minibatch (\"chunk\") of training data. Using minibatches is more efficient than training with one record at a time.\n",
        "\n",
        "sgd_minibatch_size  - is the SGD minibatch size (batches of data) that will be used to optimize the PPO surrogate objective.\n",
        "\n",
        "num_cpus_per_worker  - when set to 0 prevents Ray from pinning a CPU core to \n",
        "each worker, which means we could run out of workers in a constrained environment like a laptop or a cloud VM.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBvGCbaFXDdv"
      },
      "source": [
        "config = a3c.DEFAULT_CONFIG.copy()              # default configuration. \n",
        "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed.\n",
        "\n",
        "# Other settings we might adjust:\n",
        "config[\"num_workers\"] = 1                       # Use > 1 for using more CPU cores, including over a cluster\n",
        "config[\"num_sgd_iter\"] = 10                     # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
        "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
        "config[\"sgd_minibatch_size\"] = 250              # The amount of data records per minibatch\n",
        "config[\"model\"][\"fcnet_hiddens\"] = [100, 50]    # Neural network with two hidden layers, the list contains the number of weights on each layer\n",
        "config[\"num_cpus_per_worker\"] = 0               # This avoids running out of resources in the notebook environment when this cell is re-executed"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNyu5Bp_nxjD"
      },
      "source": [
        "With the above configs, we can train again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "pKsEe1SLn0uw",
        "outputId": "ed6d8f1d-4a91-4954-b628-058296b0d10d"
      },
      "source": [
        "agent = a3c.a3cTrainer(config, env=SELECT_ENV)\n",
        "\n",
        "results = []\n",
        "episode_data = []\n",
        "episode_json = []\n",
        "\n",
        "for n in range(N_ITER):\n",
        "    result = agent.train()\n",
        "    results.append(result)\n",
        "    \n",
        "    episode = {'n': n, \n",
        "               'episode_reward_min': result['episode_reward_min'], \n",
        "               'episode_reward_mean': result['episode_reward_mean'], \n",
        "               'episode_reward_max': result['episode_reward_max'],  \n",
        "               'episode_len_mean': result['episode_len_mean']}\n",
        "    \n",
        "    episode_data.append(episode)\n",
        "    episode_json.append(json.dumps(episode))\n",
        "    file_name = agent.save(CHECKPOINT_ROOT)\n",
        "    \n",
        "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-501f09760854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma3c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma3cTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSELECT_ENV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepisode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepisode_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'ray.rllib.agents.a3c' has no attribute 'a3cTrainer'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd_Yy_nm_vs5"
      },
      "source": [
        "### Load from Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiA9hZ_H6795"
      },
      "source": [
        "# Bring the model config\n",
        "trained_config = config.copy()\n",
        "\n",
        "# Load trained model\n",
        "test_agent = ppo.PPOTrainer(trained_config, SELECT_ENV) #initialize object\n",
        "test_agent.restore(file_name)  #above we have defined: file_name = agent.save(CHECKPOINT_ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB_7yZOiBYl3"
      },
      "source": [
        "Example of rollout: \n",
        "\n",
        "Reuse the trained policy to act in an environment\n",
        "The line: `test_agent.compute_action(state)` uses the trained policy to pick an action given the state.\n",
        "\n",
        "The reward received should match the training reward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTk2iZfKBctW"
      },
      "source": [
        "env   = gym.make(SELECT_ENV)\n",
        "state = env.reset()\n",
        "done  = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while not done:\n",
        "  action = test_agent.compute_single_action(state) #gets the next action given a state\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  cumulative_reward += reward\n",
        "\n",
        "print(cumulative_reward)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k94AfEtXi_QT"
      },
      "source": [
        "### Use Ray Tune for the Parameters \n",
        "Now we will use DQN\n",
        "and Ray Tune runner to train the algo\n",
        "\n",
        "https://www.codeproject.com/Articles/5271939/Cartpole-The-Hello-World-of-Reinforcement-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2iXOaXCoGx1"
      },
      "source": [
        "from ray import tune\n",
        "from ray.rllib.agents.dqn import DQNTrainer\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.progress_reporter import JupyterNotebookReporter\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(\n",
        "    ignore_reinit_error=True\n",
        ")\n",
        "\n",
        "ENV = 'CartPole-v0'\n",
        "TARGET_REWARD = 195  #it stops when this reward has been achieved\n",
        "TRAINER = DQNTrainer\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "#Stopping criteria\n",
        "stop_dict ={\"training_iteration\": 3,\n",
        "            \"timesteps_total\"   : 5,\n",
        "            \"episode_reward_mean\": TARGET_REWARD # stop as soon as we \"solve\" the environment            \n",
        "            }  \n",
        "\n",
        "# Parameters for the trainer function - if we use PPO, we can add the Net layers here as above\n",
        "config_dict = { \"env\": ENV,\n",
        "                \"num_workers\": 0,  # run in a single process\n",
        "                \"num_gpus\": 0\n",
        "                }\n",
        "\n",
        "# Runner\n",
        "analysis =  tune.run(\n",
        "              TRAINER,\n",
        "              stop  = stop_dict,\n",
        "              config= config_dict,\n",
        "              progress_reporter=JupyterNotebookReporter(overwrite=False),\n",
        "              verbose=2 #can be changed\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69r_Vg6rHQfE"
      },
      "source": [
        "Analyse the training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qw_7KpFHVDG"
      },
      "source": [
        "df = analysis.dataframe()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7BqN_E5oW_s"
      },
      "source": [
        "## Next Steps:\n",
        "Custom Gym environments and RLLIB\n",
        "\n",
        "https://medium.com/distributed-computing-with-ray/anatomy-of-a-custom-environment-for-rllib-327157f269e5\n",
        "\n",
        "https://developpaper.com/ray-and-rllib-for-fast-parallel-reinforcement-learning/\n",
        "\n",
        "https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c\n",
        "\n",
        "Example of custom environments and reward shaping\n",
        "\n",
        "https://colab.research.google.com/github/valin1/rllib-tutorial/blob/master/RLlib_Tutorial.ipynb\n",
        "\n",
        "https://colab.research.google.com/github/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6QNegJQobJ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}